{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question1_2_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-fVVPUgTLBI",
        "colab_type": "text"
      },
      "source": [
        "### Network Architecture A \n",
        "\n",
        "- Layer 1 - 300 neurons (ReLU activation function)\n",
        "- Layer 2 - SoftMax Layer (10 neurons)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW_aiNbQTQ32",
        "colab_type": "code",
        "outputId": "79dd4414-809b-49dc-d350-7b62a7ef2c2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Colab has two versions of TensorFlow installed: a 1.x version and a 2.x version. \n",
        "# Colab currently uses TF 1.x by default\n",
        "# To enable TF2 execute the following code\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfjwVfZBUIzw",
        "colab_type": "text"
      },
      "source": [
        "### Importing the Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRIegl_KTby3",
        "colab_type": "code",
        "outputId": "74247e69-1b58-4c60-e46f-d1584df2904f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Importing the libraries \n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skswo6BtUM4f",
        "colab_type": "text"
      },
      "source": [
        "### Creation of 3 Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvR6_Zp-UD2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -------------------------------------------CODE TO PREPARE THE DATASET---------------------------------------------------- \n",
        "\n",
        "def prepare_dataset(fashion_mnist):\n",
        "  # load the training and test data    \n",
        "  (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
        "  # reshape the feature data\n",
        "  tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
        "  te_x = te_x.reshape(te_x.shape[0], 784)\n",
        "  # noramlise feature data\n",
        "  tr_x = tr_x / 255.0\n",
        "  te_x = te_x / 255.0\n",
        "  # one hot encode the training labels and get the transpose\n",
        "  tr_y = np_utils.to_categorical(tr_y,10)\n",
        "  tr_y = tr_y.T\n",
        "  # one hot encode the test labels and get the transpose\n",
        "  te_y = np_utils.to_categorical(te_y,10)\n",
        "  te_y = te_y.T\n",
        "  return tr_x, tr_y, te_x, te_y\n",
        "\n",
        "# -----------------------------------CODE TO CALCULATE THE PROBABILITY OF EACH CLASS GIVEN THE TRAINING INSTANCE--------------------------------------\n",
        "\n",
        "def forward_pass(X_train, W1, b1, W2, b2):\n",
        "  \"\"\"\n",
        "  Return the predicted 10 class probabilities matrix for each of the training instances \n",
        "  \"\"\"\n",
        "  # Calculate the pre-activation outputs for each of the 300 neurons in the hidden layer for each of the training instance \n",
        "  # Will get 300 outputs for a single training instance in the form of (300*60000) matrix \n",
        "  # The size of the W1 is (300*784)\n",
        "  # The size of the training feature matrix is (784*60000)\n",
        "  # The size of the b1 is (300*1)\n",
        "  A1=  tf.matmul(W1, X_train) + b1\n",
        "  # Convert each element in A1 through relu activation function \n",
        "  H1= tf.math.maximum(A1, 0)\n",
        "  # Calculate the pre-activation outputs for each of the 10 neurons in the softmax layer for each of the training instance \n",
        "  # Will get 10 outputs for a single training instance in the form of (10*60000) matrix \n",
        "  # The size of the W2 is (10*300)\n",
        "  # The size of the H1 matrix is (300*60000)\n",
        "  # The size of the bias matrix is (10*1)\n",
        "  A2= tf.matmul(W2, H1) + b2\n",
        "  # Calculate a new matrix where each element is e to the power of pre-activation outputs \n",
        "  exponential_matrix= tf.math.exp(A2)\n",
        "  # Calculation of the final probabilities of each of the 10 classes for each instance in the training set \n",
        "  # Column wise sum calculation \n",
        "  column_sum= tf.reduce_sum(exponential_matrix, 0)\n",
        "  # Divide each element by the column sum so that each column is the probability of each class of a single instance \n",
        "  H2= exponential_matrix/column_sum \n",
        "  # Set the range so that the loss does not come out to be nan \n",
        "  H2= tf.clip_by_value(H2 ,1e-10, 1.0) \n",
        "  \n",
        "  return H2\n",
        "\n",
        "# -------------------------------- CODE TO CALCULATE THE LOSS FOR THE CURRENT SET OF TUNABLE PARAMETERS / WEIGHTS-------------------------------------\n",
        "\n",
        "def cross_entropy(y_train, y_pred_matrix):\n",
        "  \"\"\"\n",
        "  Return the loss value given the predicted probabilities matrix and the actual probabilities matrix\n",
        "  \"\"\"\n",
        "  # Compute the log of each element of the prediction matrix \n",
        "  log_matrix= tf.math.log(y_pred_matrix)\n",
        "  # Multiply each element of the actual labels matrix with the log matrix \n",
        "  product_matrix= y_train *log_matrix\n",
        "  # Take the negation of each element in the product matrix \n",
        "  negated_product_matrix= -1*(product_matrix)\n",
        "  # Compute the cross entropy loss for each of the training instances \n",
        "  # This will contain individual loss for the all training instances \n",
        "  # This operation will perform the column wise sum \n",
        "  single_loss_matrix= tf.reduce_sum(negated_product_matrix, 0)\n",
        "  # Compute the mean cross entropy loss \n",
        "  mean_loss= tf.reduce_mean(single_loss_matrix)\n",
        "\n",
        "  return mean_loss\n",
        "\n",
        "# ----------------------------------------- CALCULATION OF THE TRAINING AND TEST SET ACCURACY------------------------------------------------------\n",
        "\n",
        "def return_labels(matrix):\n",
        "  \"\"\"\n",
        "  Return the corrosponding class label for each vector of probability instance \n",
        "  \"\"\" \n",
        "  class_labels= tf.argmax(matrix) \n",
        "  \n",
        "  return class_labels\n",
        "\n",
        "def calculate_accuracy(feature_data, label_data, W1, b1, W2, b2):\n",
        "  \"\"\" \n",
        "  Return the accuracy value (applicable for both train and the test set) for the given set of weights and the biases \n",
        "  \"\"\"\n",
        "  # Calculate the matrix of predicted probabilities through calling of forward pass \n",
        "  predicted_matrix= forward_pass(feature_data, W1, b1, W2, b2)\n",
        "  # Get the class labels of the actual labels \n",
        "  actual_labels= return_labels(label_data)\n",
        "  # Get the class labels of the predicted probabilities \n",
        "  predicted_labels= return_labels(predicted_matrix)\n",
        "  # Get the correct prediction in the form of boolean array where 1 is correct prediction and 0 is the wrong prediction \n",
        "  correct_predictions= tf.cast(tf.equal(predicted_labels, actual_labels), tf.float32)\n",
        "  # Calculate the accuracy \n",
        "  accuracy= tf.reduce_mean(correct_predictions)\n",
        "\n",
        "  return accuracy \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmoM8duGURi_",
        "colab_type": "text"
      },
      "source": [
        "### Beginning of TensorFlow program "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJWUTJ_2UU8y",
        "colab_type": "code",
        "outputId": "d30b36eb-4d39-4d40-c02e-a7b62eba6ddf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Loading the fashion MNIST data-set \n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "# Prepare the dataset \n",
        "train_features, train_labels, test_features, test_labels= prepare_dataset(fashion_mnist)\n",
        "\n",
        "# Get the transpose of feature data \n",
        "train_features= train_features.T \n",
        "test_features= test_features.T\n",
        "\n",
        "# Print the shape of our 4 data structures \n",
        "print( \"Shape of training features \", train_features.shape)\n",
        "print (\"Shape of training labels \", train_labels.shape)\n",
        "print()\n",
        "print( \"Shape of test features \", test_features.shape)\n",
        "print (\"Shape of test labels \", test_labels.shape)\n",
        "print()\n",
        "print(\"The training process of our Softmax Neural Network begins.....\")\n",
        "print()\n",
        "\n",
        "X_train= tf.cast(train_features, tf.float32)\n",
        "y_train= tf.cast(train_labels, tf.float32)\n",
        "X_test= tf.cast(test_features, tf.float32)\n",
        "y_test= tf.cast(test_labels, tf.float32)\n",
        "\n",
        "# Set the Number of features\n",
        "num_features=  X_train.shape[0]\n",
        "# We now specify the size of hidden layer \n",
        "hidden_neurons= 300\n",
        "# We now specify the size of output layer \n",
        "output_neurons= 10 \n",
        "\n",
        "# Initialize the weight_matrix 1 and bias_matrix 1 \n",
        "# Each row of this matrix represents the 784 weights of a single neuron in the hidden layer \n",
        "W1= tf.Variable(tf.random.normal([hidden_neurons, num_features], mean=0.0, stddev=0.05))\n",
        "# It is a column vector where each row/element represents the bias value for a single neuron in the hidden layer \n",
        "b1= tf.Variable(tf.random.normal([hidden_neurons, 1], mean=0.0, stddev=0.05))\n",
        "\n",
        "# Initialize the weight_matrix 2 and bias_matrix 2 \n",
        "# Each row of this matrix represents the 300 weights of a single neuron in the softmax layer \n",
        "W2= tf.Variable(tf.random.normal([output_neurons, hidden_neurons], mean=0.0, stddev=0.05))\n",
        "# It is a column vector where each row/element represents the bias value for a single neuron in the softmax layer \n",
        "b2= tf.Variable(tf.random.normal([output_neurons, 1], mean=0.0, stddev=0.05))\n",
        "\n",
        "# Set the learning rate and the number of iterations \n",
        "learning_rate= 0.01 \n",
        "num_iterations= 1200\n",
        "\n",
        "# Adam optimizer to update the weights of the neural network \n",
        "adam_optimizer= tf.keras.optimizers.Adam()\n",
        "\n",
        "# Create the list to store the training accuracy and loss with each iteration \n",
        "training_loss= []\n",
        "training_acc= []\n",
        "# Create the list to store the test accuracy and loss with each iteration \n",
        "test_loss= []\n",
        "test_acc= []\n",
        "\n",
        "# Run the gradient descent to num_iterations number of times \n",
        "for iteration in range(num_iterations):\n",
        "\n",
        "  # Create an instance of GradientTape to monitor the forward pass and loss calculations\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Calculate the predicted probability matrix for the current weights and the biases for the training set\n",
        "    y_pred_matrix= forward_pass(X_train, W1, b1, W2, b2)\n",
        "    # Calculate the predicted probability matrix for the current weights and the biases for the test set\n",
        "    y_pred_test= forward_pass(X_test, W1, b1, W2, b2)\n",
        "    # Calculate the current training loss with the current predictions and the actual labels of the training set \n",
        "    current_loss_training= cross_entropy(y_train, y_pred_matrix)\n",
        "    # Calculate the current test loss with the current prediction and the actual labels of the test set \n",
        "    current_loss_test= cross_entropy(y_test, y_pred_test)\n",
        "  \n",
        "  # Calculate the gradients (partial derivates) of the loss with respect to each of the tunable weights \n",
        "  gradients= tape.gradient(current_loss_training, [W1, b1, W2, b2])\n",
        "\n",
        "  # Calculate the training accuracy with each iteration \n",
        "  training_accuracy= calculate_accuracy(X_train, y_train, W1, b1, W2, b2)\n",
        "\n",
        "  # Calculate the test accuracy with each each iteration \n",
        "  test_accuracy= calculate_accuracy(X_test, y_test, W1, b1, W2, b2)\n",
        "\n",
        "  # Print out the current iteration, current loss and current training accuracy \n",
        "  print(\"Iteration \",iteration, \": Loss = \",current_loss_training.numpy(),\" Acc: \", training_accuracy.numpy(),   'Val_loss = ',current_loss_test.numpy(),   'Val_acc = ', test_accuracy.numpy())\n",
        "\n",
        "  # Apply the Adam optimizer to update the weights and biases \n",
        "  adam_optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2]))\n",
        "\n",
        "  # Append the 4 values (train loss, train acc, test loss, test acc) with the each current iteration for the plotting \n",
        "  training_loss.append(current_loss_training.numpy())\n",
        "  training_acc.append(training_accuracy.numpy())\n",
        "  test_loss.append(current_loss_test.numpy())\n",
        "  test_acc.append(test_accuracy.numpy())\n",
        "\n",
        "# Calculate the test accuracy with the final updated weights and the biases through adam optimizer after running for certain number of iterations \n",
        "final_test_accuracy= calculate_accuracy(X_test, y_test, W1, b1, W2, b2)\n",
        "\n",
        "# Print the test accuracy \n",
        "print()\n",
        "print(\"The final test accuracy of the Fashion MNIST dataset is {}\".format(final_test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 1s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "Shape of training features  (784, 60000)\n",
            "Shape of training labels  (10, 60000)\n",
            "\n",
            "Shape of test features  (784, 10000)\n",
            "Shape of test labels  (10, 10000)\n",
            "\n",
            "The training process of our Softmax Neural Network begins.....\n",
            "\n",
            "Iteration  0 : Loss =  2.363158  Acc:  0.13583334 Val_loss =  2.3650005 Val_acc =  0.133\n",
            "Iteration  1 : Loss =  2.063843  Acc:  0.37495 Val_loss =  2.0672977 Val_acc =  0.3728\n",
            "Iteration  2 : Loss =  1.8467673  Acc:  0.47941667 Val_loss =  1.8520566 Val_acc =  0.4695\n",
            "Iteration  3 : Loss =  1.6607883  Acc:  0.57568336 Val_loss =  1.6674914 Val_acc =  0.5672\n",
            "Iteration  4 : Loss =  1.4934042  Acc:  0.63528335 Val_loss =  1.5013287 Val_acc =  0.6302\n",
            "Iteration  5 : Loss =  1.3512235  Acc:  0.65595 Val_loss =  1.3601416 Val_acc =  0.6513\n",
            "Iteration  6 : Loss =  1.2354159  Acc:  0.66715 Val_loss =  1.24509 Val_acc =  0.6589\n",
            "Iteration  7 : Loss =  1.1385405  Acc:  0.68043333 Val_loss =  1.148432 Val_acc =  0.6693\n",
            "Iteration  8 : Loss =  1.0571581  Acc:  0.6907 Val_loss =  1.0672078 Val_acc =  0.6794\n",
            "Iteration  9 : Loss =  0.9904832  Acc:  0.6971 Val_loss =  1.001226 Val_acc =  0.6862\n",
            "Iteration  10 : Loss =  0.9349691  Acc:  0.70311666 Val_loss =  0.9470703 Val_acc =  0.6916\n",
            "Iteration  11 : Loss =  0.88964206  Acc:  0.71023333 Val_loss =  0.90340793 Val_acc =  0.694\n",
            "Iteration  12 : Loss =  0.85343987  Acc:  0.71723336 Val_loss =  0.868684 Val_acc =  0.7038\n",
            "Iteration  13 : Loss =  0.82224154  Acc:  0.7220333 Val_loss =  0.83853036 Val_acc =  0.7095\n",
            "Iteration  14 : Loss =  0.79527265  Acc:  0.7262167 Val_loss =  0.8122245 Val_acc =  0.7121\n",
            "Iteration  15 : Loss =  0.7714744  Acc:  0.73365 Val_loss =  0.788852 Val_acc =  0.7186\n",
            "Iteration  16 : Loss =  0.7500837  Acc:  0.7428667 Val_loss =  0.7677381 Val_acc =  0.7282\n",
            "Iteration  17 : Loss =  0.7321592  Acc:  0.7499167 Val_loss =  0.7499188 Val_acc =  0.7375\n",
            "Iteration  18 : Loss =  0.7157134  Acc:  0.7550333 Val_loss =  0.733371 Val_acc =  0.7413\n",
            "Iteration  19 : Loss =  0.7008905  Acc:  0.75873333 Val_loss =  0.71851647 Val_acc =  0.7442\n",
            "Iteration  20 : Loss =  0.68698895  Acc:  0.7633833 Val_loss =  0.7050815 Val_acc =  0.7478\n",
            "Iteration  21 : Loss =  0.6738544  Acc:  0.76925 Val_loss =  0.69288546 Val_acc =  0.7542\n",
            "Iteration  22 : Loss =  0.66187096  Acc:  0.7751333 Val_loss =  0.6818196 Val_acc =  0.7587\n",
            "Iteration  23 : Loss =  0.650453  Acc:  0.78008336 Val_loss =  0.67092806 Val_acc =  0.7635\n",
            "Iteration  24 : Loss =  0.6402164  Acc:  0.78471667 Val_loss =  0.6611646 Val_acc =  0.7682\n",
            "Iteration  25 : Loss =  0.6301947  Acc:  0.79025 Val_loss =  0.6517917 Val_acc =  0.7734\n",
            "Iteration  26 : Loss =  0.62087804  Acc:  0.7946333 Val_loss =  0.6429273 Val_acc =  0.7775\n",
            "Iteration  27 : Loss =  0.61172706  Acc:  0.79646665 Val_loss =  0.6337415 Val_acc =  0.7805\n",
            "Iteration  28 : Loss =  0.6033376  Acc:  0.79823333 Val_loss =  0.62531316 Val_acc =  0.7847\n",
            "Iteration  29 : Loss =  0.59528685  Acc:  0.80086666 Val_loss =  0.6175672 Val_acc =  0.7869\n",
            "Iteration  30 : Loss =  0.5878172  Acc:  0.80448335 Val_loss =  0.61044365 Val_acc =  0.7908\n",
            "Iteration  31 : Loss =  0.5805631  Acc:  0.8064333 Val_loss =  0.60323614 Val_acc =  0.7946\n",
            "Iteration  32 : Loss =  0.5737043  Acc:  0.8089333 Val_loss =  0.59651 Val_acc =  0.7967\n",
            "Iteration  33 : Loss =  0.5670378  Acc:  0.81205 Val_loss =  0.59029275 Val_acc =  0.798\n",
            "Iteration  34 : Loss =  0.5608358  Acc:  0.8139667 Val_loss =  0.58453465 Val_acc =  0.8016\n",
            "Iteration  35 : Loss =  0.5548218  Acc:  0.81596667 Val_loss =  0.57870734 Val_acc =  0.8048\n",
            "Iteration  36 : Loss =  0.5492557  Acc:  0.8172 Val_loss =  0.5733594 Val_acc =  0.8049\n",
            "Iteration  37 : Loss =  0.5438404  Acc:  0.8196333 Val_loss =  0.56833524 Val_acc =  0.8075\n",
            "Iteration  38 : Loss =  0.5387661  Acc:  0.82096666 Val_loss =  0.56353104 Val_acc =  0.8087\n",
            "Iteration  39 : Loss =  0.53384244  Acc:  0.8225333 Val_loss =  0.55865103 Val_acc =  0.8091\n",
            "Iteration  40 : Loss =  0.5291543  Acc:  0.8239 Val_loss =  0.5541538 Val_acc =  0.8107\n",
            "Iteration  41 : Loss =  0.5247182  Acc:  0.8251 Val_loss =  0.5500897 Val_acc =  0.8128\n",
            "Iteration  42 : Loss =  0.5204298  Acc:  0.82591665 Val_loss =  0.54606855 Val_acc =  0.814\n",
            "Iteration  43 : Loss =  0.5164221  Acc:  0.8264833 Val_loss =  0.5423171 Val_acc =  0.8144\n",
            "Iteration  44 : Loss =  0.51248425  Acc:  0.82795 Val_loss =  0.53883487 Val_acc =  0.816\n",
            "Iteration  45 : Loss =  0.5086941  Acc:  0.82918334 Val_loss =  0.5354187 Val_acc =  0.8168\n",
            "Iteration  46 : Loss =  0.5050207  Acc:  0.83045 Val_loss =  0.53194904 Val_acc =  0.8187\n",
            "Iteration  47 : Loss =  0.5014721  Acc:  0.8319833 Val_loss =  0.52877814 Val_acc =  0.8187\n",
            "Iteration  48 : Loss =  0.4981037  Acc:  0.83323336 Val_loss =  0.525835 Val_acc =  0.8207\n",
            "Iteration  49 : Loss =  0.49480587  Acc:  0.83405 Val_loss =  0.5227513 Val_acc =  0.8211\n",
            "Iteration  50 : Loss =  0.4916391  Acc:  0.83475 Val_loss =  0.51988846 Val_acc =  0.822\n",
            "Iteration  51 : Loss =  0.48859394  Acc:  0.8357667 Val_loss =  0.5172069 Val_acc =  0.8224\n",
            "Iteration  52 : Loss =  0.48560527  Acc:  0.83665 Val_loss =  0.5143873 Val_acc =  0.823\n",
            "Iteration  53 : Loss =  0.48273835  Acc:  0.83735 Val_loss =  0.51174766 Val_acc =  0.8246\n",
            "Iteration  54 : Loss =  0.47996768  Acc:  0.83815 Val_loss =  0.5092804 Val_acc =  0.825\n",
            "Iteration  55 : Loss =  0.47725296  Acc:  0.839 Val_loss =  0.506757 Val_acc =  0.8251\n",
            "Iteration  56 : Loss =  0.4746202  Acc:  0.83958334 Val_loss =  0.5043866 Val_acc =  0.8255\n",
            "Iteration  57 : Loss =  0.47204733  Acc:  0.8405 Val_loss =  0.5021535 Val_acc =  0.8261\n",
            "Iteration  58 : Loss =  0.4695384  Acc:  0.8412333 Val_loss =  0.49989718 Val_acc =  0.8266\n",
            "Iteration  59 : Loss =  0.46710557  Acc:  0.84208333 Val_loss =  0.49770316 Val_acc =  0.8275\n",
            "Iteration  60 : Loss =  0.4647284  Acc:  0.8423667 Val_loss =  0.49559438 Val_acc =  0.8275\n",
            "Iteration  61 : Loss =  0.4624056  Acc:  0.84323335 Val_loss =  0.49346572 Val_acc =  0.8281\n",
            "Iteration  62 : Loss =  0.4601442  Acc:  0.8441167 Val_loss =  0.49139318 Val_acc =  0.8288\n",
            "Iteration  63 : Loss =  0.4579251  Acc:  0.8447667 Val_loss =  0.48944834 Val_acc =  0.8292\n",
            "Iteration  64 : Loss =  0.45574233  Acc:  0.84578335 Val_loss =  0.48743677 Val_acc =  0.8306\n",
            "Iteration  65 : Loss =  0.45361277  Acc:  0.8464 Val_loss =  0.48553896 Val_acc =  0.8309\n",
            "Iteration  66 : Loss =  0.45152858  Acc:  0.8470167 Val_loss =  0.4836874 Val_acc =  0.8317\n",
            "Iteration  67 : Loss =  0.4494828  Acc:  0.84793335 Val_loss =  0.48183632 Val_acc =  0.833\n",
            "Iteration  68 : Loss =  0.44746977  Acc:  0.84856665 Val_loss =  0.48003477 Val_acc =  0.8332\n",
            "Iteration  69 : Loss =  0.4454963  Acc:  0.84921664 Val_loss =  0.47830385 Val_acc =  0.8339\n",
            "Iteration  70 : Loss =  0.44356146  Acc:  0.85 Val_loss =  0.47654316 Val_acc =  0.8343\n",
            "Iteration  71 : Loss =  0.44166157  Acc:  0.85065 Val_loss =  0.47492418 Val_acc =  0.8347\n",
            "Iteration  72 : Loss =  0.4398058  Acc:  0.85105 Val_loss =  0.47321406 Val_acc =  0.8352\n",
            "Iteration  73 : Loss =  0.43801627  Acc:  0.8517333 Val_loss =  0.47171798 Val_acc =  0.8358\n",
            "Iteration  74 : Loss =  0.4363582  Acc:  0.8523167 Val_loss =  0.47011402 Val_acc =  0.8362\n",
            "Iteration  75 : Loss =  0.43498152  Acc:  0.85221666 Val_loss =  0.46920753 Val_acc =  0.837\n",
            "Iteration  76 : Loss =  0.43406475  Acc:  0.8527833 Val_loss =  0.46806988 Val_acc =  0.8359\n",
            "Iteration  77 : Loss =  0.43291026  Acc:  0.85263336 Val_loss =  0.4676763 Val_acc =  0.8363\n",
            "Iteration  78 : Loss =  0.4307149  Acc:  0.85393333 Val_loss =  0.4650742 Val_acc =  0.8368\n",
            "Iteration  79 : Loss =  0.42795762  Acc:  0.85473335 Val_loss =  0.46281385 Val_acc =  0.837\n",
            "Iteration  80 : Loss =  0.42684153  Acc:  0.8551667 Val_loss =  0.46198934 Val_acc =  0.8378\n",
            "Iteration  81 : Loss =  0.42620242  Acc:  0.85538334 Val_loss =  0.46116304 Val_acc =  0.838\n",
            "Iteration  82 : Loss =  0.42400208  Acc:  0.85595 Val_loss =  0.4595532 Val_acc =  0.8392\n",
            "Iteration  83 : Loss =  0.4219892  Acc:  0.8569 Val_loss =  0.45758682 Val_acc =  0.8388\n",
            "Iteration  84 : Loss =  0.4211821  Acc:  0.85678333 Val_loss =  0.45681885 Val_acc =  0.8393\n",
            "Iteration  85 : Loss =  0.41989473  Acc:  0.85733336 Val_loss =  0.45602232 Val_acc =  0.8402\n",
            "Iteration  86 : Loss =  0.41791752  Acc:  0.8582 Val_loss =  0.45400655 Val_acc =  0.8402\n",
            "Iteration  87 : Loss =  0.416589  Acc:  0.8588167 Val_loss =  0.45286572 Val_acc =  0.8405\n",
            "Iteration  88 : Loss =  0.41567487  Acc:  0.8591667 Val_loss =  0.45232245 Val_acc =  0.8418\n",
            "Iteration  89 : Loss =  0.41414002  Acc:  0.85941666 Val_loss =  0.45077324 Val_acc =  0.8409\n",
            "Iteration  90 : Loss =  0.4125024  Acc:  0.8602 Val_loss =  0.4494225 Val_acc =  0.8414\n",
            "Iteration  91 : Loss =  0.41148815  Acc:  0.86051667 Val_loss =  0.44867846 Val_acc =  0.843\n",
            "Iteration  92 : Loss =  0.4103652  Acc:  0.86075 Val_loss =  0.44760254 Val_acc =  0.8422\n",
            "Iteration  93 : Loss =  0.40877473  Acc:  0.86148334 Val_loss =  0.4463287 Val_acc =  0.8431\n",
            "Iteration  94 : Loss =  0.4074855  Acc:  0.86181664 Val_loss =  0.44520563 Val_acc =  0.8436\n",
            "Iteration  95 : Loss =  0.40648943  Acc:  0.8617667 Val_loss =  0.44432065 Val_acc =  0.8435\n",
            "Iteration  96 : Loss =  0.4051892  Acc:  0.8625 Val_loss =  0.44332725 Val_acc =  0.8453\n",
            "Iteration  97 : Loss =  0.40377644  Acc:  0.8624 Val_loss =  0.44202814 Val_acc =  0.8455\n",
            "Iteration  98 : Loss =  0.40264422  Acc:  0.8628333 Val_loss =  0.4410708 Val_acc =  0.845\n",
            "Iteration  99 : Loss =  0.4015837  Acc:  0.86378336 Val_loss =  0.44025245 Val_acc =  0.8465\n",
            "Iteration  100 : Loss =  0.40031785  Acc:  0.86361665 Val_loss =  0.4391006 Val_acc =  0.846\n",
            "Iteration  101 : Loss =  0.39902982  Acc:  0.86433333 Val_loss =  0.43804038 Val_acc =  0.8469\n",
            "Iteration  102 : Loss =  0.39793414  Acc:  0.8649167 Val_loss =  0.4371416 Val_acc =  0.8481\n",
            "Iteration  103 : Loss =  0.39685965  Acc:  0.8650333 Val_loss =  0.4362266 Val_acc =  0.8476\n",
            "Iteration  104 : Loss =  0.39564434  Acc:  0.8656667 Val_loss =  0.43522945 Val_acc =  0.8479\n",
            "Iteration  105 : Loss =  0.39443374  Acc:  0.8659833 Val_loss =  0.43417105 Val_acc =  0.8488\n",
            "Iteration  106 : Loss =  0.39334622  Acc:  0.86645 Val_loss =  0.43327647 Val_acc =  0.8494\n",
            "Iteration  107 : Loss =  0.39228916  Acc:  0.8670667 Val_loss =  0.4324067 Val_acc =  0.8493\n",
            "Iteration  108 : Loss =  0.39115605  Acc:  0.8675 Val_loss =  0.43141997 Val_acc =  0.8496\n",
            "Iteration  109 : Loss =  0.38999304  Acc:  0.8677667 Val_loss =  0.43044266 Val_acc =  0.851\n",
            "Iteration  110 : Loss =  0.38889688  Acc:  0.86825 Val_loss =  0.42950904 Val_acc =  0.8509\n",
            "Iteration  111 : Loss =  0.3878533  Acc:  0.86875 Val_loss =  0.42863175 Val_acc =  0.8511\n",
            "Iteration  112 : Loss =  0.3867823  Acc:  0.86915 Val_loss =  0.4277335 Val_acc =  0.8515\n",
            "Iteration  113 : Loss =  0.3856728  Acc:  0.86925 Val_loss =  0.4268069 Val_acc =  0.8523\n",
            "Iteration  114 : Loss =  0.38457513  Acc:  0.86973333 Val_loss =  0.42587802 Val_acc =  0.8524\n",
            "Iteration  115 : Loss =  0.38351786  Acc:  0.8702 Val_loss =  0.4249927 Val_acc =  0.8529\n",
            "Iteration  116 : Loss =  0.3824968  Acc:  0.8705 Val_loss =  0.4241701 Val_acc =  0.8534\n",
            "Iteration  117 : Loss =  0.38148275  Acc:  0.87086666 Val_loss =  0.42332196 Val_acc =  0.8535\n",
            "Iteration  118 : Loss =  0.38045463  Acc:  0.87118334 Val_loss =  0.42249256 Val_acc =  0.8541\n",
            "Iteration  119 : Loss =  0.37941808  Acc:  0.8714167 Val_loss =  0.42162636 Val_acc =  0.8538\n",
            "Iteration  120 : Loss =  0.37838352  Acc:  0.8719 Val_loss =  0.42082754 Val_acc =  0.8547\n",
            "Iteration  121 : Loss =  0.3773684  Acc:  0.872 Val_loss =  0.42001465 Val_acc =  0.8547\n",
            "Iteration  122 : Loss =  0.37637526  Acc:  0.87231666 Val_loss =  0.41922656 Val_acc =  0.8547\n",
            "Iteration  123 : Loss =  0.3754023  Acc:  0.87273335 Val_loss =  0.4184751 Val_acc =  0.8558\n",
            "Iteration  124 : Loss =  0.37445006  Acc:  0.87298334 Val_loss =  0.41773862 Val_acc =  0.8554\n",
            "Iteration  125 : Loss =  0.37352043  Acc:  0.873 Val_loss =  0.41700387 Val_acc =  0.8565\n",
            "Iteration  126 : Loss =  0.37263796  Acc:  0.8735167 Val_loss =  0.4163198 Val_acc =  0.8559\n",
            "Iteration  127 : Loss =  0.37183443  Acc:  0.8733 Val_loss =  0.41571942 Val_acc =  0.8569\n",
            "Iteration  128 : Loss =  0.37115103  Acc:  0.87378335 Val_loss =  0.4152523 Val_acc =  0.8576\n",
            "Iteration  129 : Loss =  0.3705337  Acc:  0.8733 Val_loss =  0.41481307 Val_acc =  0.8575\n",
            "Iteration  130 : Loss =  0.36995453  Acc:  0.87395 Val_loss =  0.4144544 Val_acc =  0.8572\n",
            "Iteration  131 : Loss =  0.36891264  Acc:  0.87401664 Val_loss =  0.41358793 Val_acc =  0.8581\n",
            "Iteration  132 : Loss =  0.36767122  Acc:  0.8750333 Val_loss =  0.41253647 Val_acc =  0.8581\n",
            "Iteration  133 : Loss =  0.36627483  Acc:  0.87495 Val_loss =  0.41126046 Val_acc =  0.8597\n",
            "Iteration  134 : Loss =  0.36527917  Acc:  0.8753167 Val_loss =  0.41049922 Val_acc =  0.8593\n",
            "Iteration  135 : Loss =  0.36464402  Acc:  0.87581664 Val_loss =  0.41003856 Val_acc =  0.8596\n",
            "Iteration  136 : Loss =  0.36402088  Acc:  0.87556666 Val_loss =  0.40961504 Val_acc =  0.8597\n",
            "Iteration  137 : Loss =  0.36317545  Acc:  0.8763833 Val_loss =  0.4089795 Val_acc =  0.8604\n",
            "Iteration  138 : Loss =  0.36208308  Acc:  0.87621665 Val_loss =  0.40803388 Val_acc =  0.8605\n",
            "Iteration  139 : Loss =  0.3611059  Acc:  0.877 Val_loss =  0.40729868 Val_acc =  0.8606\n",
            "Iteration  140 : Loss =  0.36033744  Acc:  0.87701666 Val_loss =  0.40666044 Val_acc =  0.8609\n",
            "Iteration  141 : Loss =  0.35963938  Acc:  0.87691665 Val_loss =  0.4062071 Val_acc =  0.8604\n",
            "Iteration  142 : Loss =  0.35882935  Acc:  0.87768334 Val_loss =  0.40557382 Val_acc =  0.8615\n",
            "Iteration  143 : Loss =  0.35787544  Acc:  0.8775667 Val_loss =  0.4047985 Val_acc =  0.8612\n",
            "Iteration  144 : Loss =  0.3569481  Acc:  0.8781833 Val_loss =  0.40407607 Val_acc =  0.861\n",
            "Iteration  145 : Loss =  0.3561687  Acc:  0.8786167 Val_loss =  0.40344074 Val_acc =  0.8627\n",
            "Iteration  146 : Loss =  0.35549748  Acc:  0.8785833 Val_loss =  0.40301505 Val_acc =  0.8619\n",
            "Iteration  147 : Loss =  0.35480332  Acc:  0.87855 Val_loss =  0.40245065 Val_acc =  0.8629\n",
            "Iteration  148 : Loss =  0.35397115  Acc:  0.87905 Val_loss =  0.4018471 Val_acc =  0.8618\n",
            "Iteration  149 : Loss =  0.35305703  Acc:  0.87945 Val_loss =  0.4010865 Val_acc =  0.8634\n",
            "Iteration  150 : Loss =  0.35217372  Acc:  0.8796667 Val_loss =  0.4004166 Val_acc =  0.8626\n",
            "Iteration  151 : Loss =  0.35139954  Acc:  0.8800667 Val_loss =  0.3998227 Val_acc =  0.8625\n",
            "Iteration  152 : Loss =  0.3507066  Acc:  0.87993336 Val_loss =  0.39932877 Val_acc =  0.8636\n",
            "Iteration  153 : Loss =  0.35001346  Acc:  0.88051665 Val_loss =  0.3988497 Val_acc =  0.8625\n",
            "Iteration  154 : Loss =  0.34926972  Acc:  0.88065 Val_loss =  0.39829203 Val_acc =  0.864\n",
            "Iteration  155 : Loss =  0.3484663  Acc:  0.8808333 Val_loss =  0.3977001 Val_acc =  0.8634\n",
            "Iteration  156 : Loss =  0.34765303  Acc:  0.88128334 Val_loss =  0.3970575 Val_acc =  0.8644\n",
            "Iteration  157 : Loss =  0.34687337  Acc:  0.8816 Val_loss =  0.3965075 Val_acc =  0.864\n",
            "Iteration  158 : Loss =  0.34614798  Acc:  0.88171667 Val_loss =  0.39593443 Val_acc =  0.8638\n",
            "Iteration  159 : Loss =  0.34547174  Acc:  0.88215 Val_loss =  0.3955045 Val_acc =  0.8656\n",
            "Iteration  160 : Loss =  0.34482273  Acc:  0.88201666 Val_loss =  0.39498886 Val_acc =  0.8646\n",
            "Iteration  161 : Loss =  0.34421986  Acc:  0.88261664 Val_loss =  0.39468247 Val_acc =  0.8662\n",
            "Iteration  162 : Loss =  0.343642  Acc:  0.8825667 Val_loss =  0.3941752 Val_acc =  0.865\n",
            "Iteration  163 : Loss =  0.3432172  Acc:  0.88283336 Val_loss =  0.394118 Val_acc =  0.8662\n",
            "Iteration  164 : Loss =  0.34282947  Acc:  0.88255 Val_loss =  0.39372924 Val_acc =  0.865\n",
            "Iteration  165 : Loss =  0.34279168  Acc:  0.8829833 Val_loss =  0.39415985 Val_acc =  0.8653\n",
            "Iteration  166 : Loss =  0.3424351  Acc:  0.88241667 Val_loss =  0.39367846 Val_acc =  0.8653\n",
            "Iteration  167 : Loss =  0.34189832  Acc:  0.88306665 Val_loss =  0.39365044 Val_acc =  0.865\n",
            "Iteration  168 : Loss =  0.340433  Acc:  0.88301665 Val_loss =  0.39206645 Val_acc =  0.8663\n",
            "Iteration  169 : Loss =  0.33878466  Acc:  0.88448334 Val_loss =  0.3907264 Val_acc =  0.8663\n",
            "Iteration  170 : Loss =  0.33783954  Acc:  0.8843167 Val_loss =  0.3899577 Val_acc =  0.8663\n",
            "Iteration  171 : Loss =  0.33765137  Acc:  0.88421667 Val_loss =  0.38986835 Val_acc =  0.8665\n",
            "Iteration  172 : Loss =  0.3376021  Acc:  0.8846167 Val_loss =  0.39026326 Val_acc =  0.8664\n",
            "Iteration  173 : Loss =  0.33672786  Acc:  0.8843833 Val_loss =  0.38932422 Val_acc =  0.8665\n",
            "Iteration  174 : Loss =  0.33545846  Acc:  0.88533336 Val_loss =  0.38840544 Val_acc =  0.8671\n",
            "Iteration  175 : Loss =  0.33446157  Acc:  0.8855 Val_loss =  0.3875288 Val_acc =  0.8667\n",
            "Iteration  176 : Loss =  0.33408496  Acc:  0.8854833 Val_loss =  0.38726363 Val_acc =  0.867\n",
            "Iteration  177 : Loss =  0.33391783  Acc:  0.88568336 Val_loss =  0.3874889 Val_acc =  0.8672\n",
            "Iteration  178 : Loss =  0.33322826  Acc:  0.88561666 Val_loss =  0.3867501 Val_acc =  0.8668\n",
            "Iteration  179 : Loss =  0.3322175  Acc:  0.8861333 Val_loss =  0.38610497 Val_acc =  0.8682\n",
            "Iteration  180 : Loss =  0.33127892  Acc:  0.88638335 Val_loss =  0.38523683 Val_acc =  0.8672\n",
            "Iteration  181 : Loss =  0.33075878  Acc:  0.88655 Val_loss =  0.38486946 Val_acc =  0.867\n",
            "Iteration  182 : Loss =  0.3304366  Acc:  0.88673335 Val_loss =  0.38488302 Val_acc =  0.8685\n",
            "Iteration  183 : Loss =  0.32987735  Acc:  0.88673335 Val_loss =  0.3843136 Val_acc =  0.8675\n",
            "Iteration  184 : Loss =  0.32908466  Acc:  0.8871 Val_loss =  0.38388065 Val_acc =  0.869\n",
            "Iteration  185 : Loss =  0.32824525  Acc:  0.8876 Val_loss =  0.38309157 Val_acc =  0.8677\n",
            "Iteration  186 : Loss =  0.32762718  Acc:  0.88771665 Val_loss =  0.38268343 Val_acc =  0.8687\n",
            "Iteration  187 : Loss =  0.3271721  Acc:  0.8875167 Val_loss =  0.3824823 Val_acc =  0.868\n",
            "Iteration  188 : Loss =  0.32666725  Acc:  0.88783336 Val_loss =  0.3820572 Val_acc =  0.8682\n",
            "Iteration  189 : Loss =  0.3259957  Acc:  0.8878833 Val_loss =  0.38171524 Val_acc =  0.8684\n",
            "Iteration  190 : Loss =  0.32523388  Acc:  0.8882167 Val_loss =  0.3810394 Val_acc =  0.8685\n",
            "Iteration  191 : Loss =  0.32454732  Acc:  0.88835 Val_loss =  0.3805845 Val_acc =  0.8686\n",
            "Iteration  192 : Loss =  0.32398984  Acc:  0.88855 Val_loss =  0.38024315 Val_acc =  0.8683\n",
            "Iteration  193 : Loss =  0.3234875  Acc:  0.88871664 Val_loss =  0.37986824 Val_acc =  0.8686\n",
            "Iteration  194 : Loss =  0.32294804  Acc:  0.88881665 Val_loss =  0.37961373 Val_acc =  0.8684\n",
            "Iteration  195 : Loss =  0.32233328  Acc:  0.88878334 Val_loss =  0.37907812 Val_acc =  0.8687\n",
            "Iteration  196 : Loss =  0.32167298  Acc:  0.8892 Val_loss =  0.3786901 Val_acc =  0.8687\n",
            "Iteration  197 : Loss =  0.32103378  Acc:  0.8890833 Val_loss =  0.37821886 Val_acc =  0.8693\n",
            "Iteration  198 : Loss =  0.3204616  Acc:  0.8897667 Val_loss =  0.37778986 Val_acc =  0.8689\n",
            "Iteration  199 : Loss =  0.31996152  Acc:  0.88958335 Val_loss =  0.37757626 Val_acc =  0.8697\n",
            "Iteration  200 : Loss =  0.31951097  Acc:  0.88981664 Val_loss =  0.37717983 Val_acc =  0.8685\n",
            "Iteration  201 : Loss =  0.31913036  Acc:  0.89005 Val_loss =  0.37717935 Val_acc =  0.8704\n",
            "Iteration  202 : Loss =  0.31880227  Acc:  0.8899 Val_loss =  0.3768014 Val_acc =  0.8691\n",
            "Iteration  203 : Loss =  0.31862324  Acc:  0.88983333 Val_loss =  0.37708744 Val_acc =  0.8704\n",
            "Iteration  204 : Loss =  0.31852436  Acc:  0.8898 Val_loss =  0.37690404 Val_acc =  0.8695\n",
            "Iteration  205 : Loss =  0.31840554  Acc:  0.88951665 Val_loss =  0.37723777 Val_acc =  0.8707\n",
            "Iteration  206 : Loss =  0.31775814  Acc:  0.88988334 Val_loss =  0.37653106 Val_acc =  0.8701\n",
            "Iteration  207 : Loss =  0.31648126  Acc:  0.89038336 Val_loss =  0.37559035 Val_acc =  0.8709\n",
            "Iteration  208 : Loss =  0.31501493  Acc:  0.89161664 Val_loss =  0.37416983 Val_acc =  0.8702\n",
            "Iteration  209 : Loss =  0.31423444  Acc:  0.8915833 Val_loss =  0.37352136 Val_acc =  0.8699\n",
            "Iteration  210 : Loss =  0.31419155  Acc:  0.89115 Val_loss =  0.37379906 Val_acc =  0.8718\n",
            "Iteration  211 : Loss =  0.3142043  Acc:  0.89125 Val_loss =  0.37382254 Val_acc =  0.8714\n",
            "Iteration  212 : Loss =  0.31365988  Acc:  0.89145 Val_loss =  0.37363735 Val_acc =  0.8715\n",
            "Iteration  213 : Loss =  0.3125548  Acc:  0.8921667 Val_loss =  0.3725567 Val_acc =  0.8709\n",
            "Iteration  214 : Loss =  0.31157637  Acc:  0.89231664 Val_loss =  0.37185135 Val_acc =  0.8718\n",
            "Iteration  215 : Loss =  0.31115597  Acc:  0.8922667 Val_loss =  0.37159383 Val_acc =  0.8717\n",
            "Iteration  216 : Loss =  0.31102654  Acc:  0.8925667 Val_loss =  0.37157923 Val_acc =  0.8714\n",
            "Iteration  217 : Loss =  0.31066468  Acc:  0.8921833 Val_loss =  0.37154835 Val_acc =  0.8722\n",
            "Iteration  218 : Loss =  0.30989757  Acc:  0.8929 Val_loss =  0.3707897 Val_acc =  0.8721\n",
            "Iteration  219 : Loss =  0.30905852  Acc:  0.8929667 Val_loss =  0.3702153 Val_acc =  0.872\n",
            "Iteration  220 : Loss =  0.30849555  Acc:  0.89315 Val_loss =  0.36977464 Val_acc =  0.8717\n",
            "Iteration  221 : Loss =  0.30819088  Acc:  0.8939833 Val_loss =  0.36967564 Val_acc =  0.8722\n",
            "Iteration  222 : Loss =  0.30786544  Acc:  0.89295 Val_loss =  0.36954328 Val_acc =  0.8722\n",
            "Iteration  223 : Loss =  0.30727595  Acc:  0.8940667 Val_loss =  0.3691091 Val_acc =  0.8721\n",
            "Iteration  224 : Loss =  0.30655885  Acc:  0.89376664 Val_loss =  0.3686089 Val_acc =  0.8724\n",
            "Iteration  225 : Loss =  0.3059303  Acc:  0.8940333 Val_loss =  0.36813885 Val_acc =  0.8728\n",
            "Iteration  226 : Loss =  0.30548906  Acc:  0.89423335 Val_loss =  0.3678274 Val_acc =  0.8722\n",
            "Iteration  227 : Loss =  0.3051275  Acc:  0.89383334 Val_loss =  0.36773002 Val_acc =  0.8728\n",
            "Iteration  228 : Loss =  0.3046882  Acc:  0.8946 Val_loss =  0.36739287 Val_acc =  0.8734\n",
            "Iteration  229 : Loss =  0.30412507  Acc:  0.8943167 Val_loss =  0.36706153 Val_acc =  0.8733\n",
            "Iteration  230 : Loss =  0.30350924  Acc:  0.8947833 Val_loss =  0.36658305 Val_acc =  0.8728\n",
            "Iteration  231 : Loss =  0.30295366  Acc:  0.8947833 Val_loss =  0.36622584 Val_acc =  0.8729\n",
            "Iteration  232 : Loss =  0.30249238  Acc:  0.8947833 Val_loss =  0.3659692 Val_acc =  0.8731\n",
            "Iteration  233 : Loss =  0.30208102  Acc:  0.89555 Val_loss =  0.36566654 Val_acc =  0.8731\n",
            "Iteration  234 : Loss =  0.30165273  Acc:  0.8952 Val_loss =  0.36553064 Val_acc =  0.8732\n",
            "Iteration  235 : Loss =  0.30117136  Acc:  0.89593333 Val_loss =  0.36509478 Val_acc =  0.8734\n",
            "Iteration  236 : Loss =  0.3006554  Acc:  0.8955333 Val_loss =  0.36488223 Val_acc =  0.8734\n",
            "Iteration  237 : Loss =  0.3001542  Acc:  0.8962333 Val_loss =  0.36441708 Val_acc =  0.8741\n",
            "Iteration  238 : Loss =  0.2997348  Acc:  0.89608335 Val_loss =  0.36433756 Val_acc =  0.8734\n",
            "Iteration  239 : Loss =  0.2994494  Acc:  0.8962 Val_loss =  0.36402708 Val_acc =  0.8731\n",
            "Iteration  240 : Loss =  0.29945508  Acc:  0.8965333 Val_loss =  0.36450526 Val_acc =  0.8743\n",
            "Iteration  241 : Loss =  0.29987666  Acc:  0.89535 Val_loss =  0.36473888 Val_acc =  0.8731\n",
            "Iteration  242 : Loss =  0.30117097  Acc:  0.8956 Val_loss =  0.36681932 Val_acc =  0.8757\n",
            "Iteration  243 : Loss =  0.3021882  Acc:  0.8940833 Val_loss =  0.36728802 Val_acc =  0.8723\n",
            "Iteration  244 : Loss =  0.30323398  Acc:  0.89461666 Val_loss =  0.36947846 Val_acc =  0.8747\n",
            "Iteration  245 : Loss =  0.29941627  Acc:  0.89561665 Val_loss =  0.36484873 Val_acc =  0.8722\n",
            "Iteration  246 : Loss =  0.29652727  Acc:  0.89685 Val_loss =  0.36257046 Val_acc =  0.8743\n",
            "Iteration  247 : Loss =  0.2962565  Acc:  0.8972 Val_loss =  0.36234728 Val_acc =  0.875\n",
            "Iteration  248 : Loss =  0.2975854  Acc:  0.8957667 Val_loss =  0.36364332 Val_acc =  0.8745\n",
            "Iteration  249 : Loss =  0.29755872  Acc:  0.8965667 Val_loss =  0.36438906 Val_acc =  0.8757\n",
            "Iteration  250 : Loss =  0.29514664  Acc:  0.8972667 Val_loss =  0.36149728 Val_acc =  0.8746\n",
            "Iteration  251 : Loss =  0.29426602  Acc:  0.89725 Val_loss =  0.36100802 Val_acc =  0.8744\n",
            "Iteration  252 : Loss =  0.29487327  Acc:  0.89778334 Val_loss =  0.36193588 Val_acc =  0.8757\n",
            "Iteration  253 : Loss =  0.29466152  Acc:  0.89705 Val_loss =  0.36155772 Val_acc =  0.8747\n",
            "Iteration  254 : Loss =  0.29324746  Acc:  0.8980833 Val_loss =  0.3606637 Val_acc =  0.8753\n",
            "Iteration  255 : Loss =  0.29246324  Acc:  0.89848334 Val_loss =  0.35979176 Val_acc =  0.8755\n",
            "Iteration  256 : Loss =  0.29285768  Acc:  0.8978 Val_loss =  0.3604063 Val_acc =  0.8737\n",
            "Iteration  257 : Loss =  0.29255062  Acc:  0.89848334 Val_loss =  0.3604447 Val_acc =  0.876\n",
            "Iteration  258 : Loss =  0.29132956  Acc:  0.8989 Val_loss =  0.35915872 Val_acc =  0.8745\n",
            "Iteration  259 : Loss =  0.29064262  Acc:  0.89916664 Val_loss =  0.35879102 Val_acc =  0.875\n",
            "Iteration  260 : Loss =  0.2908261  Acc:  0.89916664 Val_loss =  0.35913083 Val_acc =  0.8754\n",
            "Iteration  261 : Loss =  0.2906854  Acc:  0.89863336 Val_loss =  0.35911214 Val_acc =  0.8746\n",
            "Iteration  262 : Loss =  0.28965482  Acc:  0.89921665 Val_loss =  0.35832962 Val_acc =  0.8755\n",
            "Iteration  263 : Loss =  0.28895468  Acc:  0.8995 Val_loss =  0.35775778 Val_acc =  0.8749\n",
            "Iteration  264 : Loss =  0.2889662  Acc:  0.89916664 Val_loss =  0.35793296 Val_acc =  0.875\n",
            "Iteration  265 : Loss =  0.28878155  Acc:  0.8997 Val_loss =  0.3579684 Val_acc =  0.8761\n",
            "Iteration  266 : Loss =  0.28806973  Acc:  0.8997667 Val_loss =  0.35739622 Val_acc =  0.876\n",
            "Iteration  267 : Loss =  0.28738824  Acc:  0.90026665 Val_loss =  0.3568522 Val_acc =  0.8759\n",
            "Iteration  268 : Loss =  0.28718176  Acc:  0.9001333 Val_loss =  0.35688898 Val_acc =  0.8759\n",
            "Iteration  269 : Loss =  0.28701627  Acc:  0.9001167 Val_loss =  0.35680455 Val_acc =  0.8758\n",
            "Iteration  270 : Loss =  0.2864767  Acc:  0.9005167 Val_loss =  0.35649535 Val_acc =  0.8767\n",
            "Iteration  271 : Loss =  0.2858635  Acc:  0.90065 Val_loss =  0.35605487 Val_acc =  0.8758\n",
            "Iteration  272 : Loss =  0.28552592  Acc:  0.90063334 Val_loss =  0.35580343 Val_acc =  0.8756\n",
            "Iteration  273 : Loss =  0.28531682  Acc:  0.90071666 Val_loss =  0.35589424 Val_acc =  0.8766\n",
            "Iteration  274 : Loss =  0.28491312  Acc:  0.9008667 Val_loss =  0.3555295 Val_acc =  0.8755\n",
            "Iteration  275 : Loss =  0.28435394  Acc:  0.90113336 Val_loss =  0.35521394 Val_acc =  0.8767\n",
            "Iteration  276 : Loss =  0.28391895  Acc:  0.9012667 Val_loss =  0.35495168 Val_acc =  0.8766\n",
            "Iteration  277 : Loss =  0.28364998  Acc:  0.9012 Val_loss =  0.35478812 Val_acc =  0.8757\n",
            "Iteration  278 : Loss =  0.28334555  Acc:  0.9015833 Val_loss =  0.3547584 Val_acc =  0.8766\n",
            "Iteration  279 : Loss =  0.2828954  Acc:  0.9014 Val_loss =  0.35436141 Val_acc =  0.8761\n",
            "Iteration  280 : Loss =  0.28241926  Acc:  0.90176666 Val_loss =  0.3541487 Val_acc =  0.8769\n",
            "Iteration  281 : Loss =  0.28204158  Acc:  0.90176666 Val_loss =  0.3538971 Val_acc =  0.8773\n",
            "Iteration  282 : Loss =  0.28173554  Acc:  0.9019333 Val_loss =  0.35377124 Val_acc =  0.8767\n",
            "Iteration  283 : Loss =  0.2813828  Acc:  0.90211666 Val_loss =  0.35364792 Val_acc =  0.8776\n",
            "Iteration  284 : Loss =  0.28095716  Acc:  0.902 Val_loss =  0.3533552 Val_acc =  0.8767\n",
            "Iteration  285 : Loss =  0.28052706  Acc:  0.90241665 Val_loss =  0.35314104 Val_acc =  0.8776\n",
            "Iteration  286 : Loss =  0.28015596  Acc:  0.90246665 Val_loss =  0.35293385 Val_acc =  0.8775\n",
            "Iteration  287 : Loss =  0.27982748  Acc:  0.90243334 Val_loss =  0.3527995 Val_acc =  0.8773\n",
            "Iteration  288 : Loss =  0.27948025  Acc:  0.90285 Val_loss =  0.35262528 Val_acc =  0.8784\n",
            "Iteration  289 : Loss =  0.2790897  Acc:  0.9027 Val_loss =  0.35241282 Val_acc =  0.8773\n",
            "Iteration  290 : Loss =  0.2786806  Acc:  0.903 Val_loss =  0.3521609 Val_acc =  0.8782\n",
            "Iteration  291 : Loss =  0.2782944  Acc:  0.90291667 Val_loss =  0.35198665 Val_acc =  0.8782\n",
            "Iteration  292 : Loss =  0.27794012  Acc:  0.9030333 Val_loss =  0.35175896 Val_acc =  0.8785\n",
            "Iteration  293 : Loss =  0.27759662  Acc:  0.9033333 Val_loss =  0.35167867 Val_acc =  0.8785\n",
            "Iteration  294 : Loss =  0.27723658  Acc:  0.9033333 Val_loss =  0.35141954 Val_acc =  0.8785\n",
            "Iteration  295 : Loss =  0.27685747  Acc:  0.9036 Val_loss =  0.35130498 Val_acc =  0.8787\n",
            "Iteration  296 : Loss =  0.2764726  Acc:  0.90365 Val_loss =  0.35101745 Val_acc =  0.8783\n",
            "Iteration  297 : Loss =  0.27610233  Acc:  0.9038 Val_loss =  0.35090324 Val_acc =  0.8782\n",
            "Iteration  298 : Loss =  0.2757522  Acc:  0.90391666 Val_loss =  0.35065234 Val_acc =  0.8783\n",
            "Iteration  299 : Loss =  0.27541828  Acc:  0.90421665 Val_loss =  0.35058963 Val_acc =  0.8783\n",
            "Iteration  300 : Loss =  0.27509537  Acc:  0.90426666 Val_loss =  0.35033023 Val_acc =  0.8786\n",
            "Iteration  301 : Loss =  0.274789  Acc:  0.90431666 Val_loss =  0.35035524 Val_acc =  0.8782\n",
            "Iteration  302 : Loss =  0.274522  Acc:  0.90433335 Val_loss =  0.35006958 Val_acc =  0.8789\n",
            "Iteration  303 : Loss =  0.27433866  Acc:  0.90425 Val_loss =  0.3503722 Val_acc =  0.8781\n",
            "Iteration  304 : Loss =  0.27430335  Acc:  0.9041833 Val_loss =  0.3501245 Val_acc =  0.8788\n",
            "Iteration  305 : Loss =  0.27453893  Acc:  0.9039 Val_loss =  0.35110238 Val_acc =  0.8778\n",
            "Iteration  306 : Loss =  0.2750138  Acc:  0.9037333 Val_loss =  0.35105464 Val_acc =  0.8778\n",
            "Iteration  307 : Loss =  0.27589163  Acc:  0.9030667 Val_loss =  0.3530648 Val_acc =  0.8768\n",
            "Iteration  308 : Loss =  0.27613676  Acc:  0.90326667 Val_loss =  0.35244393 Val_acc =  0.877\n",
            "Iteration  309 : Loss =  0.27574334  Acc:  0.90281665 Val_loss =  0.3533284 Val_acc =  0.877\n",
            "Iteration  310 : Loss =  0.27356368  Acc:  0.90426666 Val_loss =  0.3502871 Val_acc =  0.8778\n",
            "Iteration  311 : Loss =  0.27155656  Acc:  0.9051167 Val_loss =  0.34897974 Val_acc =  0.8792\n",
            "Iteration  312 : Loss =  0.27089238  Acc:  0.9051 Val_loss =  0.3483901 Val_acc =  0.8794\n",
            "Iteration  313 : Loss =  0.27152136  Acc:  0.90508336 Val_loss =  0.34887594 Val_acc =  0.8782\n",
            "Iteration  314 : Loss =  0.27219433  Acc:  0.9048 Val_loss =  0.35046604 Val_acc =  0.8775\n",
            "Iteration  315 : Loss =  0.27154213  Acc:  0.9047667 Val_loss =  0.34918922 Val_acc =  0.8782\n",
            "Iteration  316 : Loss =  0.2702019  Acc:  0.90565 Val_loss =  0.34860903 Val_acc =  0.8789\n",
            "Iteration  317 : Loss =  0.26919228  Acc:  0.9058 Val_loss =  0.3474485 Val_acc =  0.8791\n",
            "Iteration  318 : Loss =  0.2691799  Acc:  0.90585 Val_loss =  0.34749496 Val_acc =  0.8792\n",
            "Iteration  319 : Loss =  0.26954228  Acc:  0.90611666 Val_loss =  0.34849194 Val_acc =  0.878\n",
            "Iteration  320 : Loss =  0.26932493  Acc:  0.90531665 Val_loss =  0.34784132 Val_acc =  0.8784\n",
            "Iteration  321 : Loss =  0.2684965  Acc:  0.9062333 Val_loss =  0.34770074 Val_acc =  0.8784\n",
            "Iteration  322 : Loss =  0.2676285  Acc:  0.90625 Val_loss =  0.34665114 Val_acc =  0.8789\n",
            "Iteration  323 : Loss =  0.2673272  Acc:  0.9063333 Val_loss =  0.34653738 Val_acc =  0.8795\n",
            "Iteration  324 : Loss =  0.26740465  Acc:  0.9065 Val_loss =  0.34706694 Val_acc =  0.879\n",
            "Iteration  325 : Loss =  0.26728985  Acc:  0.90643334 Val_loss =  0.3467204 Val_acc =  0.8793\n",
            "Iteration  326 : Loss =  0.26675397  Acc:  0.90676665 Val_loss =  0.34677935 Val_acc =  0.8791\n",
            "Iteration  327 : Loss =  0.26605713  Acc:  0.90655 Val_loss =  0.34593496 Val_acc =  0.8789\n",
            "Iteration  328 : Loss =  0.26561296  Acc:  0.90695 Val_loss =  0.34575364 Val_acc =  0.8796\n",
            "Iteration  329 : Loss =  0.26548082  Acc:  0.9070167 Val_loss =  0.3459234 Val_acc =  0.8793\n",
            "Iteration  330 : Loss =  0.2653851  Acc:  0.90688336 Val_loss =  0.34572116 Val_acc =  0.8792\n",
            "Iteration  331 : Loss =  0.26507333  Acc:  0.9073 Val_loss =  0.34591693 Val_acc =  0.8794\n",
            "Iteration  332 : Loss =  0.2645559  Acc:  0.90718335 Val_loss =  0.34526837 Val_acc =  0.8789\n",
            "Iteration  333 : Loss =  0.2640584  Acc:  0.90755 Val_loss =  0.34509537 Val_acc =  0.8791\n",
            "Iteration  334 : Loss =  0.26373684  Acc:  0.9073833 Val_loss =  0.34496444 Val_acc =  0.8798\n",
            "Iteration  335 : Loss =  0.26354864  Acc:  0.9073333 Val_loss =  0.34476084 Val_acc =  0.8795\n",
            "Iteration  336 : Loss =  0.26334444  Acc:  0.90785 Val_loss =  0.34500346 Val_acc =  0.8801\n",
            "Iteration  337 : Loss =  0.26300845  Acc:  0.90776664 Val_loss =  0.34452158 Val_acc =  0.8794\n",
            "Iteration  338 : Loss =  0.2625835  Acc:  0.9080667 Val_loss =  0.34453106 Val_acc =  0.8792\n",
            "Iteration  339 : Loss =  0.26216462  Acc:  0.90815 Val_loss =  0.34410918 Val_acc =  0.8799\n",
            "Iteration  340 : Loss =  0.26183337  Acc:  0.90835 Val_loss =  0.34399113 Val_acc =  0.88\n",
            "Iteration  341 : Loss =  0.26157963  Acc:  0.9084833 Val_loss =  0.34399232 Val_acc =  0.8794\n",
            "Iteration  342 : Loss =  0.26133785  Acc:  0.90826666 Val_loss =  0.34375113 Val_acc =  0.8794\n",
            "Iteration  343 : Loss =  0.2610517  Acc:  0.90851665 Val_loss =  0.3438449 Val_acc =  0.8796\n",
            "Iteration  344 : Loss =  0.2607092  Acc:  0.90853333 Val_loss =  0.34347036 Val_acc =  0.8798\n",
            "Iteration  345 : Loss =  0.26034632  Acc:  0.90886664 Val_loss =  0.34343588 Val_acc =  0.8796\n",
            "Iteration  346 : Loss =  0.26000497  Acc:  0.90893334 Val_loss =  0.34320527 Val_acc =  0.8802\n",
            "Iteration  347 : Loss =  0.25971565  Acc:  0.9091167 Val_loss =  0.3430659 Val_acc =  0.88\n",
            "Iteration  348 : Loss =  0.25948626  Acc:  0.90896666 Val_loss =  0.34313414 Val_acc =  0.8798\n",
            "Iteration  349 : Loss =  0.25931287  Acc:  0.90933335 Val_loss =  0.34293082 Val_acc =  0.8792\n",
            "Iteration  350 : Loss =  0.2592441  Acc:  0.90896666 Val_loss =  0.34332573 Val_acc =  0.8812\n",
            "Iteration  351 : Loss =  0.2592923  Acc:  0.909 Val_loss =  0.34322602 Val_acc =  0.8794\n",
            "Iteration  352 : Loss =  0.25968787  Acc:  0.90858334 Val_loss =  0.34419176 Val_acc =  0.8806\n",
            "Iteration  353 : Loss =  0.26038796  Acc:  0.90853333 Val_loss =  0.3446815 Val_acc =  0.8772\n",
            "Iteration  354 : Loss =  0.26145083  Acc:  0.9080833 Val_loss =  0.3464013 Val_acc =  0.8787\n",
            "Iteration  355 : Loss =  0.2621902  Acc:  0.90748334 Val_loss =  0.34691182 Val_acc =  0.876\n",
            "Iteration  356 : Loss =  0.26122016  Acc:  0.90815 Val_loss =  0.3464999 Val_acc =  0.8785\n",
            "Iteration  357 : Loss =  0.2588578  Acc:  0.9087333 Val_loss =  0.34395126 Val_acc =  0.878\n",
            "Iteration  358 : Loss =  0.25662136  Acc:  0.9098167 Val_loss =  0.34198412 Val_acc =  0.8803\n",
            "Iteration  359 : Loss =  0.25638017  Acc:  0.90991664 Val_loss =  0.34190238 Val_acc =  0.8804\n",
            "Iteration  360 : Loss =  0.2575267  Acc:  0.9091167 Val_loss =  0.34322897 Val_acc =  0.8789\n",
            "Iteration  361 : Loss =  0.2579455  Acc:  0.9094667 Val_loss =  0.34402657 Val_acc =  0.8799\n",
            "Iteration  362 : Loss =  0.256909  Acc:  0.9094333 Val_loss =  0.3428631 Val_acc =  0.8782\n",
            "Iteration  363 : Loss =  0.2553767  Acc:  0.91038334 Val_loss =  0.34171602 Val_acc =  0.8807\n",
            "Iteration  364 : Loss =  0.25485468  Acc:  0.91075 Val_loss =  0.34110552 Val_acc =  0.8804\n",
            "Iteration  365 : Loss =  0.25531223  Acc:  0.9102333 Val_loss =  0.3419665 Val_acc =  0.8792\n",
            "Iteration  366 : Loss =  0.2553997  Acc:  0.9102 Val_loss =  0.3421793 Val_acc =  0.8811\n",
            "Iteration  367 : Loss =  0.25474295  Acc:  0.9103 Val_loss =  0.3416758 Val_acc =  0.88\n",
            "Iteration  368 : Loss =  0.2537808  Acc:  0.9109167 Val_loss =  0.341007 Val_acc =  0.8807\n",
            "Iteration  369 : Loss =  0.25343695  Acc:  0.9112167 Val_loss =  0.34060484 Val_acc =  0.8805\n",
            "Iteration  370 : Loss =  0.25363228  Acc:  0.91118336 Val_loss =  0.34126958 Val_acc =  0.8799\n",
            "Iteration  371 : Loss =  0.25352314  Acc:  0.91153336 Val_loss =  0.3410624 Val_acc =  0.8807\n",
            "Iteration  372 : Loss =  0.2529227  Acc:  0.91125 Val_loss =  0.34081954 Val_acc =  0.8797\n",
            "Iteration  373 : Loss =  0.25217208  Acc:  0.9115833 Val_loss =  0.34014726 Val_acc =  0.8812\n",
            "Iteration  374 : Loss =  0.25187492  Acc:  0.9116667 Val_loss =  0.33999577 Val_acc =  0.8808\n",
            "Iteration  375 : Loss =  0.2519378  Acc:  0.9117 Val_loss =  0.34040123 Val_acc =  0.8803\n",
            "Iteration  376 : Loss =  0.25183645  Acc:  0.91205 Val_loss =  0.34027126 Val_acc =  0.8817\n",
            "Iteration  377 : Loss =  0.2514096  Acc:  0.91195 Val_loss =  0.3402631 Val_acc =  0.8803\n",
            "Iteration  378 : Loss =  0.25083363  Acc:  0.91223335 Val_loss =  0.33963493 Val_acc =  0.8811\n",
            "Iteration  379 : Loss =  0.2504764  Acc:  0.91235 Val_loss =  0.33963633 Val_acc =  0.8816\n",
            "Iteration  380 : Loss =  0.2503559  Acc:  0.9123333 Val_loss =  0.3396352 Val_acc =  0.8808\n",
            "Iteration  381 : Loss =  0.25021386  Acc:  0.91261667 Val_loss =  0.3397144 Val_acc =  0.8826\n",
            "Iteration  382 : Loss =  0.24989219  Acc:  0.91223335 Val_loss =  0.33958325 Val_acc =  0.8815\n",
            "Iteration  383 : Loss =  0.24942727  Acc:  0.91285 Val_loss =  0.339288 Val_acc =  0.8819\n",
            "Iteration  384 : Loss =  0.24903703  Acc:  0.91291666 Val_loss =  0.33908296 Val_acc =  0.8817\n",
            "Iteration  385 : Loss =  0.24880244  Acc:  0.913 Val_loss =  0.33905014 Val_acc =  0.8811\n",
            "Iteration  386 : Loss =  0.2486403  Acc:  0.91315 Val_loss =  0.33903912 Val_acc =  0.8819\n",
            "Iteration  387 : Loss =  0.2484302  Acc:  0.91286665 Val_loss =  0.33904618 Val_acc =  0.8812\n",
            "Iteration  388 : Loss =  0.24810398  Acc:  0.9133667 Val_loss =  0.33889145 Val_acc =  0.8821\n",
            "Iteration  389 : Loss =  0.24773328  Acc:  0.91316664 Val_loss =  0.338719 Val_acc =  0.8813\n",
            "Iteration  390 : Loss =  0.2473975  Acc:  0.91335 Val_loss =  0.33863842 Val_acc =  0.8821\n",
            "Iteration  391 : Loss =  0.2471366  Acc:  0.91363335 Val_loss =  0.33845228 Val_acc =  0.881\n",
            "Iteration  392 : Loss =  0.24693772  Acc:  0.9132 Val_loss =  0.338623 Val_acc =  0.8817\n",
            "Iteration  393 : Loss =  0.24676068  Acc:  0.9136 Val_loss =  0.3384176 Val_acc =  0.8814\n",
            "Iteration  394 : Loss =  0.2465686  Acc:  0.9134333 Val_loss =  0.33868933 Val_acc =  0.8817\n",
            "Iteration  395 : Loss =  0.24634519  Acc:  0.91363335 Val_loss =  0.33827257 Val_acc =  0.8816\n",
            "Iteration  396 : Loss =  0.2461255  Acc:  0.9134833 Val_loss =  0.33865508 Val_acc =  0.8815\n",
            "Iteration  397 : Loss =  0.2459738  Acc:  0.91386664 Val_loss =  0.3382323 Val_acc =  0.8807\n",
            "Iteration  398 : Loss =  0.24601468  Acc:  0.9133667 Val_loss =  0.33904424 Val_acc =  0.8815\n",
            "Iteration  399 : Loss =  0.24619524  Acc:  0.9138 Val_loss =  0.33877826 Val_acc =  0.8799\n",
            "Iteration  400 : Loss =  0.24682103  Acc:  0.91291666 Val_loss =  0.3404562 Val_acc =  0.8795\n",
            "Iteration  401 : Loss =  0.2471728  Acc:  0.9130333 Val_loss =  0.34001684 Val_acc =  0.8797\n",
            "Iteration  402 : Loss =  0.24804322  Acc:  0.9119167 Val_loss =  0.34224567 Val_acc =  0.8793\n",
            "Iteration  403 : Loss =  0.24731585  Acc:  0.91298336 Val_loss =  0.3404654 Val_acc =  0.8799\n",
            "Iteration  404 : Loss =  0.24652022  Acc:  0.9127833 Val_loss =  0.34097824 Val_acc =  0.8798\n",
            "Iteration  405 : Loss =  0.24464919  Acc:  0.9141667 Val_loss =  0.33826235 Val_acc =  0.8803\n",
            "Iteration  406 : Loss =  0.24334192  Acc:  0.91438335 Val_loss =  0.33769223 Val_acc =  0.8819\n",
            "Iteration  407 : Loss =  0.24296686  Acc:  0.9149 Val_loss =  0.33736685 Val_acc =  0.8826\n",
            "Iteration  408 : Loss =  0.24333411  Acc:  0.91468334 Val_loss =  0.33767256 Val_acc =  0.8811\n",
            "Iteration  409 : Loss =  0.24392763  Acc:  0.91431665 Val_loss =  0.33913794 Val_acc =  0.8817\n",
            "Iteration  410 : Loss =  0.24370374  Acc:  0.9143 Val_loss =  0.33829686 Val_acc =  0.8806\n",
            "Iteration  411 : Loss =  0.24311915  Acc:  0.9145333 Val_loss =  0.33869728 Val_acc =  0.8818\n",
            "Iteration  412 : Loss =  0.24209972  Acc:  0.91505 Val_loss =  0.33716986 Val_acc =  0.8812\n",
            "Iteration  413 : Loss =  0.2413994  Acc:  0.9151 Val_loss =  0.3370399 Val_acc =  0.8819\n",
            "Iteration  414 : Loss =  0.24118815  Acc:  0.91538334 Val_loss =  0.33691984 Val_acc =  0.8835\n",
            "Iteration  415 : Loss =  0.24129531  Acc:  0.91538334 Val_loss =  0.33701164 Val_acc =  0.8812\n",
            "Iteration  416 : Loss =  0.2414342  Acc:  0.9152667 Val_loss =  0.33778384 Val_acc =  0.883\n",
            "Iteration  417 : Loss =  0.24112077  Acc:  0.91531664 Val_loss =  0.33709845 Val_acc =  0.8812\n",
            "Iteration  418 : Loss =  0.2406234  Acc:  0.91555 Val_loss =  0.3373138 Val_acc =  0.883\n",
            "Iteration  419 : Loss =  0.2399716  Acc:  0.91575 Val_loss =  0.33639747 Val_acc =  0.8817\n",
            "Iteration  420 : Loss =  0.239518  Acc:  0.91583335 Val_loss =  0.33636037 Val_acc =  0.8824\n",
            "Iteration  421 : Loss =  0.23932569  Acc:  0.91613334 Val_loss =  0.33627984 Val_acc =  0.8834\n",
            "Iteration  422 : Loss =  0.23929134  Acc:  0.91566664 Val_loss =  0.33632773 Val_acc =  0.8812\n",
            "Iteration  423 : Loss =  0.23926783  Acc:  0.9164 Val_loss =  0.33675224 Val_acc =  0.8833\n",
            "Iteration  424 : Loss =  0.23903818  Acc:  0.9162167 Val_loss =  0.33639473 Val_acc =  0.8812\n",
            "Iteration  425 : Loss =  0.23869978  Acc:  0.9166 Val_loss =  0.33657277 Val_acc =  0.8835\n",
            "Iteration  426 : Loss =  0.23821653  Acc:  0.91608334 Val_loss =  0.3360044 Val_acc =  0.8816\n",
            "Iteration  427 : Loss =  0.23778057  Acc:  0.9163667 Val_loss =  0.33588213 Val_acc =  0.8831\n",
            "Iteration  428 : Loss =  0.23744626  Acc:  0.9166333 Val_loss =  0.33568844 Val_acc =  0.8829\n",
            "Iteration  429 : Loss =  0.23722626  Acc:  0.91681665 Val_loss =  0.33554798 Val_acc =  0.8827\n",
            "Iteration  430 : Loss =  0.23707767  Acc:  0.9166333 Val_loss =  0.3357795 Val_acc =  0.8827\n",
            "Iteration  431 : Loss =  0.23692825  Acc:  0.917 Val_loss =  0.33552733 Val_acc =  0.8814\n",
            "Iteration  432 : Loss =  0.23675573  Acc:  0.91693336 Val_loss =  0.33588952 Val_acc =  0.8827\n",
            "Iteration  433 : Loss =  0.2364942  Acc:  0.9169833 Val_loss =  0.3354466 Val_acc =  0.8813\n",
            "Iteration  434 : Loss =  0.23620011  Acc:  0.91693336 Val_loss =  0.3356718 Val_acc =  0.8825\n",
            "Iteration  435 : Loss =  0.23585549  Acc:  0.9173333 Val_loss =  0.33521745 Val_acc =  0.8814\n",
            "Iteration  436 : Loss =  0.23552299  Acc:  0.91723335 Val_loss =  0.33529457 Val_acc =  0.883\n",
            "Iteration  437 : Loss =  0.23520957  Acc:  0.91745 Val_loss =  0.33500788 Val_acc =  0.8825\n",
            "Iteration  438 : Loss =  0.23493314  Acc:  0.91746664 Val_loss =  0.33499572 Val_acc =  0.8828\n",
            "Iteration  439 : Loss =  0.23468818  Acc:  0.91756666 Val_loss =  0.33493018 Val_acc =  0.883\n",
            "Iteration  440 : Loss =  0.23446764  Acc:  0.9177333 Val_loss =  0.3348455 Val_acc =  0.8825\n",
            "Iteration  441 : Loss =  0.23426522  Acc:  0.91763335 Val_loss =  0.33492973 Val_acc =  0.8829\n",
            "Iteration  442 : Loss =  0.23407172  Acc:  0.9180167 Val_loss =  0.33478877 Val_acc =  0.8817\n",
            "Iteration  443 : Loss =  0.23389961  Acc:  0.91775 Val_loss =  0.3349662 Val_acc =  0.8832\n",
            "Iteration  444 : Loss =  0.2337391  Acc:  0.9180833 Val_loss =  0.33481508 Val_acc =  0.8807\n",
            "Iteration  445 : Loss =  0.23363653  Acc:  0.9181333 Val_loss =  0.33505118 Val_acc =  0.8833\n",
            "Iteration  446 : Loss =  0.23361276  Acc:  0.91803336 Val_loss =  0.335101 Val_acc =  0.8805\n",
            "Iteration  447 : Loss =  0.23375393  Acc:  0.91815 Val_loss =  0.33547845 Val_acc =  0.8829\n",
            "Iteration  448 : Loss =  0.2342445  Acc:  0.91756666 Val_loss =  0.3362196 Val_acc =  0.8803\n",
            "Iteration  449 : Loss =  0.23495983  Acc:  0.9177833 Val_loss =  0.33695087 Val_acc =  0.8815\n",
            "Iteration  450 : Loss =  0.236407  Acc:  0.91651666 Val_loss =  0.33899215 Val_acc =  0.8782\n",
            "Iteration  451 : Loss =  0.23654327  Acc:  0.9170667 Val_loss =  0.33882234 Val_acc =  0.8815\n",
            "Iteration  452 : Loss =  0.23632954  Acc:  0.91656667 Val_loss =  0.33940357 Val_acc =  0.8788\n",
            "Iteration  453 : Loss =  0.23363777  Acc:  0.9185333 Val_loss =  0.33614227 Val_acc =  0.8814\n",
            "Iteration  454 : Loss =  0.23169638  Acc:  0.91856664 Val_loss =  0.33489737 Val_acc =  0.8825\n",
            "Iteration  455 : Loss =  0.23168112  Acc:  0.91858333 Val_loss =  0.33468568 Val_acc =  0.8805\n",
            "Iteration  456 : Loss =  0.2329768  Acc:  0.91826665 Val_loss =  0.33643565 Val_acc =  0.8827\n",
            "Iteration  457 : Loss =  0.23369941  Acc:  0.9177167 Val_loss =  0.33730674 Val_acc =  0.8787\n",
            "Iteration  458 : Loss =  0.23229021  Acc:  0.9185333 Val_loss =  0.33603668 Val_acc =  0.8829\n",
            "Iteration  459 : Loss =  0.23047815  Acc:  0.9192167 Val_loss =  0.33433205 Val_acc =  0.881\n",
            "Iteration  460 : Loss =  0.22974528  Acc:  0.9191667 Val_loss =  0.3338879 Val_acc =  0.8823\n",
            "Iteration  461 : Loss =  0.2302973  Acc:  0.9195667 Val_loss =  0.33445883 Val_acc =  0.8823\n",
            "Iteration  462 : Loss =  0.23090756  Acc:  0.9189 Val_loss =  0.33549404 Val_acc =  0.8803\n",
            "Iteration  463 : Loss =  0.2302283  Acc:  0.9194833 Val_loss =  0.33482763 Val_acc =  0.8826\n",
            "Iteration  464 : Loss =  0.22917545  Acc:  0.9196333 Val_loss =  0.33391285 Val_acc =  0.8804\n",
            "Iteration  465 : Loss =  0.22872396  Acc:  0.9194 Val_loss =  0.3338564 Val_acc =  0.8823\n",
            "Iteration  466 : Loss =  0.22904326  Acc:  0.91985 Val_loss =  0.33401918 Val_acc =  0.8817\n",
            "Iteration  467 : Loss =  0.22933273  Acc:  0.91975 Val_loss =  0.33498463 Val_acc =  0.8818\n",
            "Iteration  468 : Loss =  0.22882412  Acc:  0.9201 Val_loss =  0.33412713 Val_acc =  0.8817\n",
            "Iteration  469 : Loss =  0.22801016  Acc:  0.9199 Val_loss =  0.33388686 Val_acc =  0.882\n",
            "Iteration  470 : Loss =  0.22750819  Acc:  0.9202833 Val_loss =  0.33325568 Val_acc =  0.882\n",
            "Iteration  471 : Loss =  0.2275334  Acc:  0.9202 Val_loss =  0.3336011 Val_acc =  0.883\n",
            "Iteration  472 : Loss =  0.22763948  Acc:  0.92025 Val_loss =  0.33388263 Val_acc =  0.8808\n",
            "Iteration  473 : Loss =  0.22731738  Acc:  0.92048335 Val_loss =  0.33368096 Val_acc =  0.8828\n",
            "Iteration  474 : Loss =  0.22674581  Acc:  0.92041665 Val_loss =  0.3333043 Val_acc =  0.8813\n",
            "Iteration  475 : Loss =  0.22628139  Acc:  0.92075 Val_loss =  0.33306152 Val_acc =  0.8826\n",
            "Iteration  476 : Loss =  0.22615017  Acc:  0.9209167 Val_loss =  0.3330006 Val_acc =  0.8827\n",
            "Iteration  477 : Loss =  0.2261589  Acc:  0.92085 Val_loss =  0.33338803 Val_acc =  0.882\n",
            "Iteration  478 : Loss =  0.2259788  Acc:  0.92115 Val_loss =  0.3331772 Val_acc =  0.8822\n",
            "Iteration  479 : Loss =  0.22560444  Acc:  0.92095 Val_loss =  0.33315822 Val_acc =  0.8816\n",
            "Iteration  480 : Loss =  0.22517908  Acc:  0.9212 Val_loss =  0.33275646 Val_acc =  0.8829\n",
            "Iteration  481 : Loss =  0.22490488  Acc:  0.92123336 Val_loss =  0.3327397 Val_acc =  0.8825\n",
            "Iteration  482 : Loss =  0.22478405  Acc:  0.9210167 Val_loss =  0.33283013 Val_acc =  0.8819\n",
            "Iteration  483 : Loss =  0.22467624  Acc:  0.92156667 Val_loss =  0.3328478 Val_acc =  0.8828\n",
            "Iteration  484 : Loss =  0.22447722  Acc:  0.92118335 Val_loss =  0.33290416 Val_acc =  0.8819\n",
            "Iteration  485 : Loss =  0.22416031  Acc:  0.92183334 Val_loss =  0.33270985 Val_acc =  0.883\n",
            "Iteration  486 : Loss =  0.22384007  Acc:  0.92145 Val_loss =  0.33253902 Val_acc =  0.882\n",
            "Iteration  487 : Loss =  0.22359745  Acc:  0.92176664 Val_loss =  0.33257827 Val_acc =  0.8826\n",
            "Iteration  488 : Loss =  0.22345228  Acc:  0.92183334 Val_loss =  0.33243823 Val_acc =  0.883\n",
            "Iteration  489 : Loss =  0.2233654  Acc:  0.92181665 Val_loss =  0.33281407 Val_acc =  0.8827\n",
            "Iteration  490 : Loss =  0.22330323  Acc:  0.92188334 Val_loss =  0.33256194 Val_acc =  0.8831\n",
            "Iteration  491 : Loss =  0.22330305  Acc:  0.92183334 Val_loss =  0.33323175 Val_acc =  0.8823\n",
            "Iteration  492 : Loss =  0.22346523  Acc:  0.92215 Val_loss =  0.33299437 Val_acc =  0.8826\n",
            "Iteration  493 : Loss =  0.22408542  Acc:  0.92121667 Val_loss =  0.33458793 Val_acc =  0.8823\n",
            "Iteration  494 : Loss =  0.22501631  Acc:  0.92046666 Val_loss =  0.33480683 Val_acc =  0.8831\n",
            "Iteration  495 : Loss =  0.22722915  Acc:  0.91895 Val_loss =  0.33845675 Val_acc =  0.881\n",
            "Iteration  496 : Loss =  0.22778828  Acc:  0.91926664 Val_loss =  0.3378596 Val_acc =  0.8812\n",
            "Iteration  497 : Loss =  0.22907887  Acc:  0.918 Val_loss =  0.34079662 Val_acc =  0.8811\n",
            "Iteration  498 : Loss =  0.22533864  Acc:  0.9202167 Val_loss =  0.3357793 Val_acc =  0.8821\n",
            "Iteration  499 : Loss =  0.22228226  Acc:  0.9220833 Val_loss =  0.33363858 Val_acc =  0.8821\n",
            "Iteration  500 : Loss =  0.22082706  Acc:  0.9227333 Val_loss =  0.33196098 Val_acc =  0.8835\n",
            "Iteration  501 : Loss =  0.2218261  Acc:  0.9220667 Val_loss =  0.33294213 Val_acc =  0.8831\n",
            "Iteration  502 : Loss =  0.22374384  Acc:  0.92043334 Val_loss =  0.3359373 Val_acc =  0.8826\n",
            "Iteration  503 : Loss =  0.22324267  Acc:  0.92116666 Val_loss =  0.33458763 Val_acc =  0.8834\n",
            "Iteration  504 : Loss =  0.22180553  Acc:  0.9219 Val_loss =  0.33425328 Val_acc =  0.8829\n",
            "Iteration  505 : Loss =  0.21999899  Acc:  0.92296666 Val_loss =  0.3319985 Val_acc =  0.8837\n",
            "Iteration  506 : Loss =  0.21973173  Acc:  0.92315 Val_loss =  0.33201122 Val_acc =  0.8832\n",
            "Iteration  507 : Loss =  0.22063944  Acc:  0.9224833 Val_loss =  0.3335457 Val_acc =  0.8834\n",
            "Iteration  508 : Loss =  0.22103177  Acc:  0.9220167 Val_loss =  0.33348256 Val_acc =  0.8834\n",
            "Iteration  509 : Loss =  0.22067048  Acc:  0.9225 Val_loss =  0.33405852 Val_acc =  0.8829\n",
            "Iteration  510 : Loss =  0.21931009  Acc:  0.9235 Val_loss =  0.33220375 Val_acc =  0.8839\n",
            "Iteration  511 : Loss =  0.21851537  Acc:  0.9235 Val_loss =  0.33187187 Val_acc =  0.883\n",
            "Iteration  512 : Loss =  0.21862164  Acc:  0.9238167 Val_loss =  0.33222413 Val_acc =  0.8844\n",
            "Iteration  513 : Loss =  0.2190249  Acc:  0.9231833 Val_loss =  0.33243826 Val_acc =  0.883\n",
            "Iteration  514 : Loss =  0.21911615  Acc:  0.92321664 Val_loss =  0.33324662 Val_acc =  0.8831\n",
            "Iteration  515 : Loss =  0.2183612  Acc:  0.9234833 Val_loss =  0.33208716 Val_acc =  0.8835\n",
            "Iteration  516 : Loss =  0.21760064  Acc:  0.92405 Val_loss =  0.33182997 Val_acc =  0.8829\n",
            "Iteration  517 : Loss =  0.21726689  Acc:  0.9242 Val_loss =  0.33156034 Val_acc =  0.8838\n",
            "Iteration  518 : Loss =  0.21737722  Acc:  0.92368335 Val_loss =  0.33171034 Val_acc =  0.8837\n",
            "Iteration  519 : Loss =  0.2175391  Acc:  0.92395 Val_loss =  0.3324094 Val_acc =  0.8834\n",
            "Iteration  520 : Loss =  0.2172717  Acc:  0.92373335 Val_loss =  0.3318996 Val_acc =  0.8842\n",
            "Iteration  521 : Loss =  0.21677737  Acc:  0.92438334 Val_loss =  0.33192924 Val_acc =  0.8832\n",
            "Iteration  522 : Loss =  0.2162695  Acc:  0.9241 Val_loss =  0.3313709 Val_acc =  0.884\n",
            "Iteration  523 : Loss =  0.21602963  Acc:  0.92436665 Val_loss =  0.33133197 Val_acc =  0.8839\n",
            "Iteration  524 : Loss =  0.21601489  Acc:  0.92475 Val_loss =  0.33165517 Val_acc =  0.8827\n",
            "Iteration  525 : Loss =  0.21598905  Acc:  0.9244 Val_loss =  0.3315266 Val_acc =  0.8839\n",
            "Iteration  526 : Loss =  0.21582787  Acc:  0.92476666 Val_loss =  0.3318572 Val_acc =  0.8833\n",
            "Iteration  527 : Loss =  0.21545747  Acc:  0.92446667 Val_loss =  0.33136168 Val_acc =  0.8837\n",
            "Iteration  528 : Loss =  0.2150792  Acc:  0.92511666 Val_loss =  0.33133286 Val_acc =  0.8832\n",
            "Iteration  529 : Loss =  0.2147967  Acc:  0.9249833 Val_loss =  0.33117938 Val_acc =  0.8839\n",
            "Iteration  530 : Loss =  0.21464437  Acc:  0.9248667 Val_loss =  0.33111572 Val_acc =  0.8837\n",
            "Iteration  531 : Loss =  0.21455336  Acc:  0.92525 Val_loss =  0.33141488 Val_acc =  0.8831\n",
            "Iteration  532 : Loss =  0.21441758  Acc:  0.9248833 Val_loss =  0.33117566 Val_acc =  0.8835\n",
            "Iteration  533 : Loss =  0.21421072  Acc:  0.92545 Val_loss =  0.3314258 Val_acc =  0.8829\n",
            "Iteration  534 : Loss =  0.2139228  Acc:  0.9249333 Val_loss =  0.33104748 Val_acc =  0.8836\n",
            "Iteration  535 : Loss =  0.21363077  Acc:  0.92553335 Val_loss =  0.33111715 Val_acc =  0.8835\n",
            "Iteration  536 : Loss =  0.21337569  Acc:  0.92545 Val_loss =  0.33094272 Val_acc =  0.884\n",
            "Iteration  537 : Loss =  0.21317796  Acc:  0.9255 Val_loss =  0.33091858 Val_acc =  0.884\n",
            "Iteration  538 : Loss =  0.21302097  Acc:  0.92575 Val_loss =  0.3310366 Val_acc =  0.8838\n",
            "Iteration  539 : Loss =  0.21287099  Acc:  0.9254 Val_loss =  0.3308895 Val_acc =  0.8834\n",
            "Iteration  540 : Loss =  0.21270677  Acc:  0.92586666 Val_loss =  0.33111525 Val_acc =  0.8836\n",
            "Iteration  541 : Loss =  0.2125041  Acc:  0.9256 Val_loss =  0.33085784 Val_acc =  0.8837\n",
            "Iteration  542 : Loss =  0.21228041  Acc:  0.92606664 Val_loss =  0.3310405 Val_acc =  0.8835\n",
            "Iteration  543 : Loss =  0.2120383  Acc:  0.9259167 Val_loss =  0.3307772 Val_acc =  0.8837\n",
            "Iteration  544 : Loss =  0.21180153  Acc:  0.92611665 Val_loss =  0.33088186 Val_acc =  0.8837\n",
            "Iteration  545 : Loss =  0.21157931  Acc:  0.92625 Val_loss =  0.33072415 Val_acc =  0.8838\n",
            "Iteration  546 : Loss =  0.21137731  Acc:  0.9262667 Val_loss =  0.33078215 Val_acc =  0.884\n",
            "Iteration  547 : Loss =  0.2111982  Acc:  0.92648333 Val_loss =  0.3307458 Val_acc =  0.8839\n",
            "Iteration  548 : Loss =  0.21104439  Acc:  0.9263 Val_loss =  0.3307849 Val_acc =  0.8837\n",
            "Iteration  549 : Loss =  0.21091777  Acc:  0.9264 Val_loss =  0.33084115 Val_acc =  0.8844\n",
            "Iteration  550 : Loss =  0.21083838  Acc:  0.92656666 Val_loss =  0.33095542 Val_acc =  0.8833\n",
            "Iteration  551 : Loss =  0.21081175  Acc:  0.92681664 Val_loss =  0.33106738 Val_acc =  0.8843\n",
            "Iteration  552 : Loss =  0.2109181  Acc:  0.92648333 Val_loss =  0.33144918 Val_acc =  0.8827\n",
            "Iteration  553 : Loss =  0.211104  Acc:  0.9266667 Val_loss =  0.3316552 Val_acc =  0.8838\n",
            "Iteration  554 : Loss =  0.21161328  Acc:  0.9265 Val_loss =  0.33263198 Val_acc =  0.8817\n",
            "Iteration  555 : Loss =  0.21200234  Acc:  0.92685 Val_loss =  0.3328586 Val_acc =  0.8837\n",
            "Iteration  556 : Loss =  0.21278907  Acc:  0.9257 Val_loss =  0.3343674 Val_acc =  0.8815\n",
            "Iteration  557 : Loss =  0.21250609  Acc:  0.92655 Val_loss =  0.3336869 Val_acc =  0.8834\n",
            "Iteration  558 : Loss =  0.2120235  Acc:  0.92583334 Val_loss =  0.3339577 Val_acc =  0.8824\n",
            "Iteration  559 : Loss =  0.2103789  Acc:  0.92723334 Val_loss =  0.33188057 Val_acc =  0.8838\n",
            "Iteration  560 : Loss =  0.20906006  Acc:  0.9273833 Val_loss =  0.3311273 Val_acc =  0.8839\n",
            "Iteration  561 : Loss =  0.20857336  Acc:  0.92765 Val_loss =  0.3305335 Val_acc =  0.8834\n",
            "Iteration  562 : Loss =  0.20895368  Acc:  0.9274167 Val_loss =  0.3311917 Val_acc =  0.8848\n",
            "Iteration  563 : Loss =  0.20966266  Acc:  0.9271 Val_loss =  0.33221117 Val_acc =  0.8824\n",
            "Iteration  564 : Loss =  0.20977734  Acc:  0.92745 Val_loss =  0.33227113 Val_acc =  0.8839\n",
            "Iteration  565 : Loss =  0.20945379  Acc:  0.92716664 Val_loss =  0.33240193 Val_acc =  0.882\n",
            "Iteration  566 : Loss =  0.20834802  Acc:  0.92756665 Val_loss =  0.33122087 Val_acc =  0.8854\n",
            "Iteration  567 : Loss =  0.20744869  Acc:  0.92785 Val_loss =  0.3305475 Val_acc =  0.8835\n",
            "Iteration  568 : Loss =  0.20709977  Acc:  0.92808336 Val_loss =  0.3304223 Val_acc =  0.8841\n",
            "Iteration  569 : Loss =  0.20726845  Acc:  0.9282 Val_loss =  0.33060807 Val_acc =  0.8849\n",
            "Iteration  570 : Loss =  0.20755555  Acc:  0.9281167 Val_loss =  0.331362 Val_acc =  0.8828\n",
            "Iteration  571 : Loss =  0.20744035  Acc:  0.9281 Val_loss =  0.33111015 Val_acc =  0.8849\n",
            "Iteration  572 : Loss =  0.20704024  Acc:  0.9280667 Val_loss =  0.3311725 Val_acc =  0.883\n",
            "Iteration  573 : Loss =  0.20639846  Acc:  0.9284667 Val_loss =  0.33045575 Val_acc =  0.885\n",
            "Iteration  574 : Loss =  0.20593366  Acc:  0.9284833 Val_loss =  0.33022505 Val_acc =  0.8841\n",
            "Iteration  575 : Loss =  0.20576395  Acc:  0.9285167 Val_loss =  0.33030882 Val_acc =  0.8842\n",
            "Iteration  576 : Loss =  0.20580335  Acc:  0.9287 Val_loss =  0.33032957 Val_acc =  0.8848\n",
            "Iteration  577 : Loss =  0.20585518  Acc:  0.92845 Val_loss =  0.330883 Val_acc =  0.8839\n",
            "Iteration  578 : Loss =  0.20571041  Acc:  0.9288167 Val_loss =  0.3305708 Val_acc =  0.885\n",
            "Iteration  579 : Loss =  0.20540789  Acc:  0.92855 Val_loss =  0.33080235 Val_acc =  0.8844\n",
            "Iteration  580 : Loss =  0.20500785  Acc:  0.929 Val_loss =  0.33023354 Val_acc =  0.8846\n",
            "Iteration  581 : Loss =  0.20467992  Acc:  0.929 Val_loss =  0.33035034 Val_acc =  0.8846\n",
            "Iteration  582 : Loss =  0.20450303  Acc:  0.92925 Val_loss =  0.3301486 Val_acc =  0.8837\n",
            "Iteration  583 : Loss =  0.20447758  Acc:  0.92896664 Val_loss =  0.3304415 Val_acc =  0.8847\n",
            "Iteration  584 : Loss =  0.20452124  Acc:  0.92931664 Val_loss =  0.33051074 Val_acc =  0.8836\n",
            "Iteration  585 : Loss =  0.20457642  Acc:  0.9292167 Val_loss =  0.3308988 Val_acc =  0.8849\n",
            "Iteration  586 : Loss =  0.2045928  Acc:  0.9288167 Val_loss =  0.33087105 Val_acc =  0.8841\n",
            "Iteration  587 : Loss =  0.20470016  Acc:  0.92845 Val_loss =  0.33152005 Val_acc =  0.8841\n",
            "Iteration  588 : Loss =  0.20476584  Acc:  0.9288333 Val_loss =  0.33127847 Val_acc =  0.8843\n",
            "Iteration  589 : Loss =  0.20527035  Acc:  0.92785 Val_loss =  0.33267814 Val_acc =  0.8838\n",
            "Iteration  590 : Loss =  0.20553708  Acc:  0.92815 Val_loss =  0.3322431 Val_acc =  0.8843\n",
            "Iteration  591 : Loss =  0.20634656  Acc:  0.927 Val_loss =  0.33435005 Val_acc =  0.8829\n",
            "Iteration  592 : Loss =  0.20616488  Acc:  0.9275333 Val_loss =  0.3331372 Val_acc =  0.8838\n",
            "Iteration  593 : Loss =  0.20595177  Acc:  0.9274167 Val_loss =  0.33431518 Val_acc =  0.8829\n",
            "Iteration  594 : Loss =  0.2044767  Acc:  0.92871666 Val_loss =  0.3318332 Val_acc =  0.8845\n",
            "Iteration  595 : Loss =  0.20303544  Acc:  0.9289167 Val_loss =  0.33135548 Val_acc =  0.8838\n",
            "Iteration  596 : Loss =  0.20192137  Acc:  0.93013334 Val_loss =  0.3299002 Val_acc =  0.8843\n",
            "Iteration  597 : Loss =  0.20163164  Acc:  0.93008333 Val_loss =  0.32986322 Val_acc =  0.8838\n",
            "Iteration  598 : Loss =  0.20200963  Acc:  0.9296 Val_loss =  0.33083093 Val_acc =  0.8851\n",
            "Iteration  599 : Loss =  0.20254707  Acc:  0.9295667 Val_loss =  0.33093607 Val_acc =  0.885\n",
            "Iteration  600 : Loss =  0.20296037  Acc:  0.9288 Val_loss =  0.33243498 Val_acc =  0.8836\n",
            "Iteration  601 : Loss =  0.20267451  Acc:  0.9292667 Val_loss =  0.33143008 Val_acc =  0.885\n",
            "Iteration  602 : Loss =  0.20217021  Acc:  0.9291667 Val_loss =  0.33190453 Val_acc =  0.8838\n",
            "Iteration  603 : Loss =  0.20127848  Acc:  0.93008333 Val_loss =  0.3304978 Val_acc =  0.8847\n",
            "Iteration  604 : Loss =  0.20065436  Acc:  0.9304 Val_loss =  0.33036014 Val_acc =  0.8848\n",
            "Iteration  605 : Loss =  0.2003305  Acc:  0.93058336 Val_loss =  0.33010992 Val_acc =  0.8835\n",
            "Iteration  606 : Loss =  0.2002775  Acc:  0.93081665 Val_loss =  0.3300212 Val_acc =  0.8849\n",
            "Iteration  607 : Loss =  0.20032631  Acc:  0.93058336 Val_loss =  0.33068463 Val_acc =  0.8839\n",
            "Iteration  608 : Loss =  0.20027503  Acc:  0.93081665 Val_loss =  0.33027586 Val_acc =  0.8848\n",
            "Iteration  609 : Loss =  0.20005822  Acc:  0.9303833 Val_loss =  0.33080947 Val_acc =  0.8839\n",
            "Iteration  610 : Loss =  0.19969311  Acc:  0.9310333 Val_loss =  0.33008483 Val_acc =  0.8847\n",
            "Iteration  611 : Loss =  0.19935398  Acc:  0.9310333 Val_loss =  0.33029762 Val_acc =  0.8849\n",
            "Iteration  612 : Loss =  0.19909194  Acc:  0.93115 Val_loss =  0.32995206 Val_acc =  0.8841\n",
            "Iteration  613 : Loss =  0.19897638  Acc:  0.9313667 Val_loss =  0.33005673 Val_acc =  0.8846\n",
            "Iteration  614 : Loss =  0.19896972  Acc:  0.93126667 Val_loss =  0.33031976 Val_acc =  0.8838\n",
            "Iteration  615 : Loss =  0.19892243  Acc:  0.9315 Val_loss =  0.33023456 Val_acc =  0.8854\n",
            "Iteration  616 : Loss =  0.19884573  Acc:  0.93121666 Val_loss =  0.3306287 Val_acc =  0.8841\n",
            "Iteration  617 : Loss =  0.1985685  Acc:  0.9317167 Val_loss =  0.33021432 Val_acc =  0.8855\n",
            "Iteration  618 : Loss =  0.19824626  Acc:  0.9316 Val_loss =  0.33031937 Val_acc =  0.8839\n",
            "Iteration  619 : Loss =  0.1978542  Acc:  0.93193334 Val_loss =  0.32989776 Val_acc =  0.8853\n",
            "Iteration  620 : Loss =  0.19752675  Acc:  0.93198335 Val_loss =  0.32983595 Val_acc =  0.8844\n",
            "Iteration  621 : Loss =  0.19727896  Acc:  0.9321167 Val_loss =  0.32978672 Val_acc =  0.8847\n",
            "Iteration  622 : Loss =  0.19711511  Acc:  0.93226665 Val_loss =  0.3296357 Val_acc =  0.8847\n",
            "Iteration  623 : Loss =  0.19700848  Acc:  0.932 Val_loss =  0.3299497 Val_acc =  0.8848\n",
            "Iteration  624 : Loss =  0.19691178  Acc:  0.9321833 Val_loss =  0.3296886 Val_acc =  0.8847\n",
            "Iteration  625 : Loss =  0.1968059  Acc:  0.93195 Val_loss =  0.33014706 Val_acc =  0.8846\n",
            "Iteration  626 : Loss =  0.19665992  Acc:  0.9323 Val_loss =  0.3297614 Val_acc =  0.8852\n",
            "Iteration  627 : Loss =  0.19649218  Acc:  0.9321333 Val_loss =  0.33021295 Val_acc =  0.8847\n",
            "Iteration  628 : Loss =  0.196286  Acc:  0.9324167 Val_loss =  0.32973656 Val_acc =  0.8852\n",
            "Iteration  629 : Loss =  0.19609265  Acc:  0.9321667 Val_loss =  0.3301195 Val_acc =  0.8848\n",
            "Iteration  630 : Loss =  0.19590586  Acc:  0.9326 Val_loss =  0.3297312 Val_acc =  0.885\n",
            "Iteration  631 : Loss =  0.1958015  Acc:  0.93245 Val_loss =  0.3301386 Val_acc =  0.8854\n",
            "Iteration  632 : Loss =  0.19575976  Acc:  0.93236667 Val_loss =  0.3300002 Val_acc =  0.8852\n",
            "Iteration  633 : Loss =  0.19590189  Acc:  0.93228334 Val_loss =  0.330596 Val_acc =  0.8848\n",
            "Iteration  634 : Loss =  0.19620882  Acc:  0.9324167 Val_loss =  0.33084634 Val_acc =  0.8844\n",
            "Iteration  635 : Loss =  0.19680005  Acc:  0.93181664 Val_loss =  0.33186266 Val_acc =  0.8849\n",
            "Iteration  636 : Loss =  0.19760934  Acc:  0.93161666 Val_loss =  0.33260122 Val_acc =  0.8843\n",
            "Iteration  637 : Loss =  0.19845119  Acc:  0.93081665 Val_loss =  0.33395186 Val_acc =  0.8834\n",
            "Iteration  638 : Loss =  0.19881925  Acc:  0.93075 Val_loss =  0.33413497 Val_acc =  0.8842\n",
            "Iteration  639 : Loss =  0.19834994  Acc:  0.93056667 Val_loss =  0.33424547 Val_acc =  0.8833\n",
            "Iteration  640 : Loss =  0.19663249  Acc:  0.93193334 Val_loss =  0.3321557 Val_acc =  0.8846\n",
            "Iteration  641 : Loss =  0.19491778  Acc:  0.9324333 Val_loss =  0.3310086 Val_acc =  0.8845\n",
            "Iteration  642 : Loss =  0.193811  Acc:  0.9332 Val_loss =  0.32961214 Val_acc =  0.885\n",
            "Iteration  643 : Loss =  0.19385417  Acc:  0.93361664 Val_loss =  0.33018866 Val_acc =  0.8838\n",
            "Iteration  644 : Loss =  0.19449268  Acc:  0.93315 Val_loss =  0.3307944 Val_acc =  0.886\n",
            "Iteration  645 : Loss =  0.19522318  Acc:  0.93271667 Val_loss =  0.33187145 Val_acc =  0.8844\n",
            "Iteration  646 : Loss =  0.1950853  Acc:  0.93275 Val_loss =  0.3319114 Val_acc =  0.8843\n",
            "Iteration  647 : Loss =  0.19443545  Acc:  0.93301666 Val_loss =  0.3312645 Val_acc =  0.8849\n",
            "Iteration  648 : Loss =  0.19351287  Acc:  0.9331 Val_loss =  0.3307895 Val_acc =  0.8847\n",
            "Iteration  649 : Loss =  0.19283299  Acc:  0.93378335 Val_loss =  0.32986203 Val_acc =  0.8852\n",
            "Iteration  650 : Loss =  0.192766  Acc:  0.9334 Val_loss =  0.33051807 Val_acc =  0.8844\n",
            "Iteration  651 : Loss =  0.19299795  Acc:  0.93378335 Val_loss =  0.33035743 Val_acc =  0.8855\n",
            "Iteration  652 : Loss =  0.1931641  Acc:  0.93365 Val_loss =  0.3312831 Val_acc =  0.8846\n",
            "Iteration  653 : Loss =  0.19277868  Acc:  0.9339333 Val_loss =  0.3305007 Val_acc =  0.8852\n",
            "Iteration  654 : Loss =  0.19215585  Acc:  0.93441665 Val_loss =  0.33042446 Val_acc =  0.8843\n",
            "Iteration  655 : Loss =  0.19144113  Acc:  0.93446666 Val_loss =  0.32960132 Val_acc =  0.8858\n",
            "Iteration  656 : Loss =  0.1910474  Acc:  0.93446666 Val_loss =  0.32941177 Val_acc =  0.8858\n",
            "Iteration  657 : Loss =  0.191021  Acc:  0.9346333 Val_loss =  0.32968783 Val_acc =  0.8847\n",
            "Iteration  658 : Loss =  0.19116992  Acc:  0.93476665 Val_loss =  0.32974398 Val_acc =  0.8856\n",
            "Iteration  659 : Loss =  0.19126241  Acc:  0.9345833 Val_loss =  0.33042416 Val_acc =  0.8845\n",
            "Iteration  660 : Loss =  0.1910861  Acc:  0.93471664 Val_loss =  0.3299771 Val_acc =  0.8856\n",
            "Iteration  661 : Loss =  0.19074516  Acc:  0.93438333 Val_loss =  0.33026817 Val_acc =  0.8847\n",
            "Iteration  662 : Loss =  0.19033156  Acc:  0.9346833 Val_loss =  0.329606 Val_acc =  0.8858\n",
            "Iteration  663 : Loss =  0.19002637  Acc:  0.93488336 Val_loss =  0.3298005 Val_acc =  0.8853\n",
            "Iteration  664 : Loss =  0.18987937  Acc:  0.9349833 Val_loss =  0.32954526 Val_acc =  0.8855\n",
            "Iteration  665 : Loss =  0.1898705  Acc:  0.93515 Val_loss =  0.32988307 Val_acc =  0.8853\n",
            "Iteration  666 : Loss =  0.18987595  Acc:  0.935 Val_loss =  0.32990688 Val_acc =  0.8855\n",
            "Iteration  667 : Loss =  0.1898225  Acc:  0.9349667 Val_loss =  0.33017403 Val_acc =  0.8847\n",
            "Iteration  668 : Loss =  0.189663  Acc:  0.93521667 Val_loss =  0.3300136 Val_acc =  0.8857\n",
            "Iteration  669 : Loss =  0.18945642  Acc:  0.93518335 Val_loss =  0.33018947 Val_acc =  0.8851\n",
            "Iteration  670 : Loss =  0.18921845  Acc:  0.9353 Val_loss =  0.32981488 Val_acc =  0.8856\n",
            "Iteration  671 : Loss =  0.18910424  Acc:  0.93521667 Val_loss =  0.3302893 Val_acc =  0.8848\n",
            "Iteration  672 : Loss =  0.18903463  Acc:  0.93516666 Val_loss =  0.32987168 Val_acc =  0.8859\n",
            "Iteration  673 : Loss =  0.18912491  Acc:  0.9349167 Val_loss =  0.33079097 Val_acc =  0.8847\n",
            "Iteration  674 : Loss =  0.18920891  Acc:  0.9352 Val_loss =  0.33029038 Val_acc =  0.8853\n",
            "Iteration  675 : Loss =  0.18937123  Acc:  0.93471664 Val_loss =  0.33149227 Val_acc =  0.884\n",
            "Iteration  676 : Loss =  0.18938005  Acc:  0.93523335 Val_loss =  0.33074403 Val_acc =  0.8859\n",
            "Iteration  677 : Loss =  0.18939655  Acc:  0.93476665 Val_loss =  0.3319061 Val_acc =  0.8837\n",
            "Iteration  678 : Loss =  0.18911554  Acc:  0.9352667 Val_loss =  0.3308063 Val_acc =  0.8854\n",
            "Iteration  679 : Loss =  0.18888193  Acc:  0.93471664 Val_loss =  0.33163914 Val_acc =  0.884\n",
            "Iteration  680 : Loss =  0.18835111  Acc:  0.9352 Val_loss =  0.33042967 Val_acc =  0.8861\n",
            "Iteration  681 : Loss =  0.18799391  Acc:  0.9354 Val_loss =  0.3308989 Val_acc =  0.8843\n",
            "Iteration  682 : Loss =  0.18751591  Acc:  0.93588334 Val_loss =  0.3301003 Val_acc =  0.8847\n",
            "Iteration  683 : Loss =  0.18719521  Acc:  0.93616664 Val_loss =  0.33024845 Val_acc =  0.8857\n",
            "Iteration  684 : Loss =  0.18690188  Acc:  0.93605 Val_loss =  0.33001763 Val_acc =  0.886\n",
            "Iteration  685 : Loss =  0.18660776  Acc:  0.93663335 Val_loss =  0.32984945 Val_acc =  0.8854\n",
            "Iteration  686 : Loss =  0.18634729  Acc:  0.93655 Val_loss =  0.3299363 Val_acc =  0.8858\n",
            "Iteration  687 : Loss =  0.18606935  Acc:  0.93686664 Val_loss =  0.32957146 Val_acc =  0.8855\n",
            "Iteration  688 : Loss =  0.18584265  Acc:  0.93665 Val_loss =  0.32982764 Val_acc =  0.885\n",
            "Iteration  689 : Loss =  0.18567728  Acc:  0.93665 Val_loss =  0.32946476 Val_acc =  0.8864\n",
            "Iteration  690 : Loss =  0.1855986  Acc:  0.9364167 Val_loss =  0.3299044 Val_acc =  0.8854\n",
            "Iteration  691 : Loss =  0.18557751  Acc:  0.93661666 Val_loss =  0.329657 Val_acc =  0.8862\n",
            "Iteration  692 : Loss =  0.1856741  Acc:  0.9362 Val_loss =  0.3303169 Val_acc =  0.8854\n",
            "Iteration  693 : Loss =  0.18575886  Acc:  0.9364667 Val_loss =  0.33015355 Val_acc =  0.8855\n",
            "Iteration  694 : Loss =  0.18597451  Acc:  0.9361167 Val_loss =  0.33096823 Val_acc =  0.8843\n",
            "Iteration  695 : Loss =  0.18600969  Acc:  0.9361333 Val_loss =  0.33071744 Val_acc =  0.8854\n",
            "Iteration  696 : Loss =  0.1861168  Acc:  0.93615 Val_loss =  0.33146873 Val_acc =  0.8837\n",
            "Iteration  697 : Loss =  0.18583757  Acc:  0.9360833 Val_loss =  0.33084965 Val_acc =  0.8857\n",
            "Iteration  698 : Loss =  0.18558623  Acc:  0.93635 Val_loss =  0.33126178 Val_acc =  0.8842\n",
            "Iteration  699 : Loss =  0.18497853  Acc:  0.93668336 Val_loss =  0.33029586 Val_acc =  0.8854\n",
            "Iteration  700 : Loss =  0.18447419  Acc:  0.9367 Val_loss =  0.330425 Val_acc =  0.8853\n",
            "Iteration  701 : Loss =  0.18392105  Acc:  0.9371 Val_loss =  0.32957748 Val_acc =  0.8862\n",
            "Iteration  702 : Loss =  0.18353818  Acc:  0.9374 Val_loss =  0.3297604 Val_acc =  0.8857\n",
            "Iteration  703 : Loss =  0.18327828  Acc:  0.93763334 Val_loss =  0.32932281 Val_acc =  0.8863\n",
            "Iteration  704 : Loss =  0.18314293  Acc:  0.938 Val_loss =  0.32964256 Val_acc =  0.8862\n",
            "Iteration  705 : Loss =  0.18307956  Acc:  0.93795 Val_loss =  0.3295392 Val_acc =  0.8856\n",
            "Iteration  706 : Loss =  0.1830718  Acc:  0.9378 Val_loss =  0.3298558 Val_acc =  0.8862\n",
            "Iteration  707 : Loss =  0.18302387  Acc:  0.93768334 Val_loss =  0.3298794 Val_acc =  0.8859\n",
            "Iteration  708 : Loss =  0.18299392  Acc:  0.9379 Val_loss =  0.33006307 Val_acc =  0.8862\n",
            "Iteration  709 : Loss =  0.18288359  Acc:  0.93778336 Val_loss =  0.33014274 Val_acc =  0.8862\n",
            "Iteration  710 : Loss =  0.1827613  Acc:  0.9378833 Val_loss =  0.3300973 Val_acc =  0.8851\n",
            "Iteration  711 : Loss =  0.18259268  Acc:  0.93785 Val_loss =  0.33025393 Val_acc =  0.8861\n",
            "Iteration  712 : Loss =  0.18238162  Acc:  0.93796664 Val_loss =  0.32994685 Val_acc =  0.8855\n",
            "Iteration  713 : Loss =  0.18220769  Acc:  0.93756664 Val_loss =  0.33026847 Val_acc =  0.8851\n",
            "Iteration  714 : Loss =  0.181987  Acc:  0.93808335 Val_loss =  0.32979813 Val_acc =  0.8865\n",
            "Iteration  715 : Loss =  0.18189593  Acc:  0.93756664 Val_loss =  0.33036634 Val_acc =  0.8849\n",
            "Iteration  716 : Loss =  0.18180384  Acc:  0.9378833 Val_loss =  0.3298549 Val_acc =  0.8866\n",
            "Iteration  717 : Loss =  0.18187976  Acc:  0.9375333 Val_loss =  0.3307838 Val_acc =  0.884\n",
            "Iteration  718 : Loss =  0.18196546  Acc:  0.9382 Val_loss =  0.3302548 Val_acc =  0.8864\n",
            "Iteration  719 : Loss =  0.18219121  Acc:  0.9375833 Val_loss =  0.33158603 Val_acc =  0.8843\n",
            "Iteration  720 : Loss =  0.18235372  Acc:  0.93798333 Val_loss =  0.33093593 Val_acc =  0.8867\n",
            "Iteration  721 : Loss =  0.18251339  Acc:  0.9375 Val_loss =  0.33234364 Val_acc =  0.8834\n",
            "Iteration  722 : Loss =  0.18239598  Acc:  0.93805 Val_loss =  0.33129323 Val_acc =  0.8864\n",
            "Iteration  723 : Loss =  0.18204053  Acc:  0.9377667 Val_loss =  0.33215457 Val_acc =  0.8833\n",
            "Iteration  724 : Loss =  0.1813335  Acc:  0.9382167 Val_loss =  0.3305938 Val_acc =  0.8871\n",
            "Iteration  725 : Loss =  0.18051432  Acc:  0.93831664 Val_loss =  0.33072636 Val_acc =  0.885\n",
            "Iteration  726 : Loss =  0.17975084  Acc:  0.93895 Val_loss =  0.3295058 Val_acc =  0.8868\n",
            "Iteration  727 : Loss =  0.17925426  Acc:  0.9393167 Val_loss =  0.3295005 Val_acc =  0.8866\n",
            "Iteration  728 : Loss =  0.17906283  Acc:  0.93953335 Val_loss =  0.32942778 Val_acc =  0.8863\n",
            "Iteration  729 : Loss =  0.17910606  Acc:  0.93953335 Val_loss =  0.32947505 Val_acc =  0.887\n",
            "Iteration  730 : Loss =  0.17927788  Acc:  0.93935 Val_loss =  0.33023375 Val_acc =  0.8851\n",
            "Iteration  731 : Loss =  0.17943655  Acc:  0.9392 Val_loss =  0.33003107 Val_acc =  0.8873\n",
            "Iteration  732 : Loss =  0.17950945  Acc:  0.93915 Val_loss =  0.33092377 Val_acc =  0.8849\n",
            "Iteration  733 : Loss =  0.17936945  Acc:  0.9390333 Val_loss =  0.33023363 Val_acc =  0.887\n",
            "Iteration  734 : Loss =  0.17904586  Acc:  0.9392667 Val_loss =  0.33072403 Val_acc =  0.8855\n",
            "Iteration  735 : Loss =  0.17857604  Acc:  0.9395667 Val_loss =  0.32976943 Val_acc =  0.887\n",
            "Iteration  736 : Loss =  0.1780993  Acc:  0.93958336 Val_loss =  0.32993796 Val_acc =  0.8855\n",
            "Iteration  737 : Loss =  0.17771591  Acc:  0.93976665 Val_loss =  0.32935694 Val_acc =  0.8871\n",
            "Iteration  738 : Loss =  0.17748259  Acc:  0.9400333 Val_loss =  0.32950628 Val_acc =  0.887\n",
            "Iteration  739 : Loss =  0.17739496  Acc:  0.94011664 Val_loss =  0.3295217 Val_acc =  0.8873\n",
            "Iteration  740 : Loss =  0.17742887  Acc:  0.9401 Val_loss =  0.32966563 Val_acc =  0.8866\n",
            "Iteration  741 : Loss =  0.17757697  Acc:  0.94015 Val_loss =  0.33011016 Val_acc =  0.8865\n",
            "Iteration  742 : Loss =  0.17773357  Acc:  0.9399667 Val_loss =  0.33025557 Val_acc =  0.886\n",
            "Iteration  743 : Loss =  0.17803632  Acc:  0.9398 Val_loss =  0.33090514 Val_acc =  0.8856\n",
            "Iteration  744 : Loss =  0.17824538  Acc:  0.9399 Val_loss =  0.3312022 Val_acc =  0.8858\n",
            "Iteration  745 : Loss =  0.17871983  Acc:  0.93911666 Val_loss =  0.3317832 Val_acc =  0.8853\n",
            "Iteration  746 : Loss =  0.17924275  Acc:  0.9389333 Val_loss =  0.33274695 Val_acc =  0.8842\n",
            "Iteration  747 : Loss =  0.17978682  Acc:  0.93806666 Val_loss =  0.33296824 Val_acc =  0.8859\n",
            "Iteration  748 : Loss =  0.18079668  Acc:  0.93745 Val_loss =  0.33494902 Val_acc =  0.8844\n",
            "Iteration  749 : Loss =  0.18032055  Acc:  0.93731666 Val_loss =  0.33363995 Val_acc =  0.887\n",
            "Iteration  750 : Loss =  0.18036836  Acc:  0.9372 Val_loss =  0.33497152 Val_acc =  0.8845\n",
            "Iteration  751 : Loss =  0.17841884  Acc:  0.9389 Val_loss =  0.33191898 Val_acc =  0.8872\n",
            "Iteration  752 : Loss =  0.17697808  Acc:  0.93941665 Val_loss =  0.33157662 Val_acc =  0.8838\n",
            "Iteration  753 : Loss =  0.1759026  Acc:  0.9403333 Val_loss =  0.3298438 Val_acc =  0.8866\n",
            "Iteration  754 : Loss =  0.17574802  Acc:  0.9405 Val_loss =  0.33025274 Val_acc =  0.8871\n",
            "Iteration  755 : Loss =  0.17620441  Acc:  0.94026667 Val_loss =  0.33098277 Val_acc =  0.8846\n",
            "Iteration  756 : Loss =  0.17682953  Acc:  0.93981665 Val_loss =  0.33140022 Val_acc =  0.8858\n",
            "Iteration  757 : Loss =  0.17747417  Acc:  0.93915 Val_loss =  0.33300558 Val_acc =  0.8844\n",
            "Iteration  758 : Loss =  0.17692517  Acc:  0.93941665 Val_loss =  0.33173817 Val_acc =  0.887\n",
            "Iteration  759 : Loss =  0.1763231  Acc:  0.9395 Val_loss =  0.3321568 Val_acc =  0.8844\n",
            "Iteration  760 : Loss =  0.17509682  Acc:  0.9407167 Val_loss =  0.33024693 Val_acc =  0.8866\n",
            "Iteration  761 : Loss =  0.17429297  Acc:  0.9412 Val_loss =  0.33013502 Val_acc =  0.8856\n",
            "Iteration  762 : Loss =  0.17401041  Acc:  0.94156665 Val_loss =  0.3296796 Val_acc =  0.887\n",
            "Iteration  763 : Loss =  0.17420502  Acc:  0.9410833 Val_loss =  0.33007494 Val_acc =  0.887\n",
            "Iteration  764 : Loss =  0.1745598  Acc:  0.94083333 Val_loss =  0.33085895 Val_acc =  0.8846\n",
            "Iteration  765 : Loss =  0.17462164  Acc:  0.9407 Val_loss =  0.33062285 Val_acc =  0.8855\n",
            "Iteration  766 : Loss =  0.1744958  Acc:  0.9407 Val_loss =  0.33122417 Val_acc =  0.8847\n",
            "Iteration  767 : Loss =  0.17384958  Acc:  0.94135 Val_loss =  0.33014464 Val_acc =  0.8867\n",
            "Iteration  768 : Loss =  0.17325296  Acc:  0.9414333 Val_loss =  0.33017772 Val_acc =  0.8856\n",
            "Iteration  769 : Loss =  0.17278372  Acc:  0.94191664 Val_loss =  0.3294772 Val_acc =  0.8873\n",
            "Iteration  770 : Loss =  0.1725987  Acc:  0.94208336 Val_loss =  0.32965463 Val_acc =  0.8873\n",
            "Iteration  771 : Loss =  0.17263411  Acc:  0.9421667 Val_loss =  0.3298373 Val_acc =  0.8864\n",
            "Iteration  772 : Loss =  0.17275423  Acc:  0.94168335 Val_loss =  0.33000216 Val_acc =  0.8868\n",
            "Iteration  773 : Loss =  0.17283438  Acc:  0.9417 Val_loss =  0.3305492 Val_acc =  0.885\n",
            "Iteration  774 : Loss =  0.17269635  Acc:  0.94163334 Val_loss =  0.33021986 Val_acc =  0.887\n",
            "Iteration  775 : Loss =  0.17245553  Acc:  0.94191664 Val_loss =  0.33045942 Val_acc =  0.8852\n",
            "Iteration  776 : Loss =  0.17203923  Acc:  0.9418333 Val_loss =  0.32982793 Val_acc =  0.887\n",
            "Iteration  777 : Loss =  0.1716491  Acc:  0.94255 Val_loss =  0.32983398 Val_acc =  0.8865\n",
            "Iteration  778 : Loss =  0.17132016  Acc:  0.94245 Val_loss =  0.32946593 Val_acc =  0.8875\n",
            "Iteration  779 : Loss =  0.1711083  Acc:  0.9428 Val_loss =  0.32954428 Val_acc =  0.8874\n",
            "Iteration  780 : Loss =  0.17099923  Acc:  0.94311666 Val_loss =  0.32959127 Val_acc =  0.8877\n",
            "Iteration  781 : Loss =  0.17096063  Acc:  0.94266665 Val_loss =  0.3296498 Val_acc =  0.8872\n",
            "Iteration  782 : Loss =  0.17095514  Acc:  0.94271666 Val_loss =  0.3299343 Val_acc =  0.8863\n",
            "Iteration  783 : Loss =  0.17092676  Acc:  0.9425667 Val_loss =  0.329872 Val_acc =  0.8869\n",
            "Iteration  784 : Loss =  0.17087668  Acc:  0.94278336 Val_loss =  0.33019572 Val_acc =  0.8861\n",
            "Iteration  785 : Loss =  0.170754  Acc:  0.9427 Val_loss =  0.33003202 Val_acc =  0.8868\n",
            "Iteration  786 : Loss =  0.17060104  Acc:  0.94303334 Val_loss =  0.33022153 Val_acc =  0.8861\n",
            "Iteration  787 : Loss =  0.17039974  Acc:  0.9428333 Val_loss =  0.33000433 Val_acc =  0.8869\n",
            "Iteration  788 : Loss =  0.170185  Acc:  0.9431667 Val_loss =  0.33006063 Val_acc =  0.8865\n",
            "Iteration  789 : Loss =  0.1699694  Acc:  0.94308335 Val_loss =  0.32993093 Val_acc =  0.8872\n",
            "Iteration  790 : Loss =  0.16975427  Acc:  0.94338334 Val_loss =  0.3298815 Val_acc =  0.8871\n",
            "Iteration  791 : Loss =  0.16957076  Acc:  0.94325 Val_loss =  0.32989314 Val_acc =  0.8873\n",
            "Iteration  792 : Loss =  0.16939609  Acc:  0.94345 Val_loss =  0.32974443 Val_acc =  0.8874\n",
            "Iteration  793 : Loss =  0.16926354  Acc:  0.94335 Val_loss =  0.32997525 Val_acc =  0.8875\n",
            "Iteration  794 : Loss =  0.16914928  Acc:  0.9436167 Val_loss =  0.32979146 Val_acc =  0.8876\n",
            "Iteration  795 : Loss =  0.1690762  Acc:  0.9435167 Val_loss =  0.330191 Val_acc =  0.887\n",
            "Iteration  796 : Loss =  0.16902985  Acc:  0.9438 Val_loss =  0.32993555 Val_acc =  0.8874\n",
            "Iteration  797 : Loss =  0.16901915  Acc:  0.94336665 Val_loss =  0.33055946 Val_acc =  0.8861\n",
            "Iteration  798 : Loss =  0.16903389  Acc:  0.94386667 Val_loss =  0.33019292 Val_acc =  0.8877\n",
            "Iteration  799 : Loss =  0.1690895  Acc:  0.94343334 Val_loss =  0.33106205 Val_acc =  0.886\n",
            "Iteration  800 : Loss =  0.16914688  Acc:  0.94381666 Val_loss =  0.33055624 Val_acc =  0.8878\n",
            "Iteration  801 : Loss =  0.16922402  Acc:  0.94315 Val_loss =  0.33158135 Val_acc =  0.8857\n",
            "Iteration  802 : Loss =  0.1692459  Acc:  0.9438 Val_loss =  0.3309243 Val_acc =  0.8869\n",
            "Iteration  803 : Loss =  0.16922009  Acc:  0.9428667 Val_loss =  0.33194652 Val_acc =  0.8848\n",
            "Iteration  804 : Loss =  0.16902603  Acc:  0.9436167 Val_loss =  0.33104685 Val_acc =  0.8871\n",
            "Iteration  805 : Loss =  0.16872892  Acc:  0.9432667 Val_loss =  0.33178824 Val_acc =  0.8852\n",
            "Iteration  806 : Loss =  0.16825742  Acc:  0.9439333 Val_loss =  0.3306459 Val_acc =  0.8872\n",
            "Iteration  807 : Loss =  0.16776355  Acc:  0.9435667 Val_loss =  0.33096752 Val_acc =  0.8855\n",
            "Iteration  808 : Loss =  0.16728255  Acc:  0.94423336 Val_loss =  0.3300418 Val_acc =  0.8867\n",
            "Iteration  809 : Loss =  0.16696848  Acc:  0.9444 Val_loss =  0.3303166 Val_acc =  0.8864\n",
            "Iteration  810 : Loss =  0.16681293  Acc:  0.9445 Val_loss =  0.33008724 Val_acc =  0.8877\n",
            "Iteration  811 : Loss =  0.16686726  Acc:  0.94455 Val_loss =  0.33044395 Val_acc =  0.887\n",
            "Iteration  812 : Loss =  0.1671294  Acc:  0.9443667 Val_loss =  0.33086815 Val_acc =  0.8868\n",
            "Iteration  813 : Loss =  0.16747107  Acc:  0.94425 Val_loss =  0.33127442 Val_acc =  0.8867\n",
            "Iteration  814 : Loss =  0.16807568  Acc:  0.9438 Val_loss =  0.33220503 Val_acc =  0.8859\n",
            "Iteration  815 : Loss =  0.16835362  Acc:  0.9438 Val_loss =  0.3324849 Val_acc =  0.885\n",
            "Iteration  816 : Loss =  0.1688183  Acc:  0.94301665 Val_loss =  0.33323005 Val_acc =  0.8852\n",
            "Iteration  817 : Loss =  0.16847384  Acc:  0.9435 Val_loss =  0.33301264 Val_acc =  0.885\n",
            "Iteration  818 : Loss =  0.16800866  Acc:  0.94318336 Val_loss =  0.3325181 Val_acc =  0.8854\n",
            "Iteration  819 : Loss =  0.16731104  Acc:  0.9438 Val_loss =  0.33229184 Val_acc =  0.885\n",
            "Iteration  820 : Loss =  0.1665856  Acc:  0.9440167 Val_loss =  0.33114892 Val_acc =  0.8872\n",
            "Iteration  821 : Loss =  0.1664568  Acc:  0.9438 Val_loss =  0.33192125 Val_acc =  0.8854\n",
            "Iteration  822 : Loss =  0.16644116  Acc:  0.9443667 Val_loss =  0.33124313 Val_acc =  0.8871\n",
            "Iteration  823 : Loss =  0.16657986  Acc:  0.9439667 Val_loss =  0.3324974 Val_acc =  0.8849\n",
            "Iteration  824 : Loss =  0.16644141  Acc:  0.9446833 Val_loss =  0.3315702 Val_acc =  0.8875\n",
            "Iteration  825 : Loss =  0.16599183  Acc:  0.94443333 Val_loss =  0.33206216 Val_acc =  0.8863\n",
            "Iteration  826 : Loss =  0.16518489  Acc:  0.9454833 Val_loss =  0.33072358 Val_acc =  0.8876\n",
            "Iteration  827 : Loss =  0.16445062  Acc:  0.9454167 Val_loss =  0.33049497 Val_acc =  0.8874\n",
            "Iteration  828 : Loss =  0.16400875  Acc:  0.94598335 Val_loss =  0.33016106 Val_acc =  0.8884\n",
            "Iteration  829 : Loss =  0.16402005  Acc:  0.94545 Val_loss =  0.33015025 Val_acc =  0.8876\n",
            "Iteration  830 : Loss =  0.16437389  Acc:  0.9450167 Val_loss =  0.33119833 Val_acc =  0.8858\n",
            "Iteration  831 : Loss =  0.16476241  Acc:  0.94516665 Val_loss =  0.33104202 Val_acc =  0.8872\n",
            "Iteration  832 : Loss =  0.16504143  Acc:  0.94451666 Val_loss =  0.33232734 Val_acc =  0.8851\n",
            "Iteration  833 : Loss =  0.16494225  Acc:  0.94495 Val_loss =  0.33143622 Val_acc =  0.8875\n",
            "Iteration  834 : Loss =  0.1646594  Acc:  0.94453335 Val_loss =  0.3321962 Val_acc =  0.8855\n",
            "Iteration  835 : Loss =  0.16408548  Acc:  0.9454167 Val_loss =  0.33092916 Val_acc =  0.8877\n",
            "Iteration  836 : Loss =  0.16370024  Acc:  0.94535 Val_loss =  0.3313208 Val_acc =  0.8848\n",
            "Iteration  837 : Loss =  0.16341057  Acc:  0.94535 Val_loss =  0.3307499 Val_acc =  0.8876\n",
            "Iteration  838 : Loss =  0.16339684  Acc:  0.94586664 Val_loss =  0.33114251 Val_acc =  0.8858\n",
            "Iteration  839 : Loss =  0.16350114  Acc:  0.94563335 Val_loss =  0.33137524 Val_acc =  0.8867\n",
            "Iteration  840 : Loss =  0.16343708  Acc:  0.94601667 Val_loss =  0.3313827 Val_acc =  0.8864\n",
            "Iteration  841 : Loss =  0.1633029  Acc:  0.9457833 Val_loss =  0.33154878 Val_acc =  0.8868\n",
            "Iteration  842 : Loss =  0.16279982  Acc:  0.94633335 Val_loss =  0.33098847 Val_acc =  0.8874\n",
            "Iteration  843 : Loss =  0.16228954  Acc:  0.94628334 Val_loss =  0.3307462 Val_acc =  0.8875\n",
            "Iteration  844 : Loss =  0.16180117  Acc:  0.9469333 Val_loss =  0.33039728 Val_acc =  0.8884\n",
            "Iteration  845 : Loss =  0.16151504  Acc:  0.94668335 Val_loss =  0.33017296 Val_acc =  0.8879\n",
            "Iteration  846 : Loss =  0.16142751  Acc:  0.94673336 Val_loss =  0.33046347 Val_acc =  0.888\n",
            "Iteration  847 : Loss =  0.16144733  Acc:  0.9467667 Val_loss =  0.33031976 Val_acc =  0.8879\n",
            "Iteration  848 : Loss =  0.16146958  Acc:  0.9465333 Val_loss =  0.3308457 Val_acc =  0.8874\n",
            "Iteration  849 : Loss =  0.1613969  Acc:  0.94661665 Val_loss =  0.33053002 Val_acc =  0.8876\n",
            "Iteration  850 : Loss =  0.16123946  Acc:  0.94661665 Val_loss =  0.33090904 Val_acc =  0.8873\n",
            "Iteration  851 : Loss =  0.16097371  Acc:  0.94705 Val_loss =  0.33045542 Val_acc =  0.888\n",
            "Iteration  852 : Loss =  0.1606826  Acc:  0.94673336 Val_loss =  0.33060738 Val_acc =  0.8883\n",
            "Iteration  853 : Loss =  0.16039996  Acc:  0.9472 Val_loss =  0.330303 Val_acc =  0.8886\n",
            "Iteration  854 : Loss =  0.16018529  Acc:  0.9471833 Val_loss =  0.33032268 Val_acc =  0.8885\n",
            "Iteration  855 : Loss =  0.16005164  Acc:  0.9472 Val_loss =  0.33039162 Val_acc =  0.8885\n",
            "Iteration  856 : Loss =  0.15999043  Acc:  0.9472833 Val_loss =  0.33032763 Val_acc =  0.8879\n",
            "Iteration  857 : Loss =  0.15998407  Acc:  0.94701666 Val_loss =  0.33078712 Val_acc =  0.8874\n",
            "Iteration  858 : Loss =  0.16000858  Acc:  0.9471667 Val_loss =  0.3305766 Val_acc =  0.8878\n",
            "Iteration  859 : Loss =  0.16009733  Acc:  0.9467833 Val_loss =  0.33132362 Val_acc =  0.8866\n",
            "Iteration  860 : Loss =  0.16023955  Acc:  0.94706666 Val_loss =  0.3310069 Val_acc =  0.8879\n",
            "Iteration  861 : Loss =  0.16061474  Acc:  0.94621664 Val_loss =  0.3322826 Val_acc =  0.8858\n",
            "Iteration  862 : Loss =  0.16106637  Acc:  0.9464667 Val_loss =  0.33211273 Val_acc =  0.8874\n",
            "Iteration  863 : Loss =  0.16224137  Acc:  0.94533336 Val_loss =  0.33442867 Val_acc =  0.8851\n",
            "Iteration  864 : Loss =  0.1630491  Acc:  0.94451666 Val_loss =  0.33445027 Val_acc =  0.8869\n",
            "Iteration  865 : Loss =  0.16509248  Acc:  0.94355 Val_loss =  0.33775228 Val_acc =  0.8837\n",
            "Iteration  866 : Loss =  0.1649029  Acc:  0.9432 Val_loss =  0.33668175 Val_acc =  0.8855\n",
            "Iteration  867 : Loss =  0.1650138  Acc:  0.9432833 Val_loss =  0.33769825 Val_acc =  0.8848\n",
            "Iteration  868 : Loss =  0.1624668  Acc:  0.94523335 Val_loss =  0.33452585 Val_acc =  0.8859\n",
            "Iteration  869 : Loss =  0.15993902  Acc:  0.94685 Val_loss =  0.33224815 Val_acc =  0.8861\n",
            "Iteration  870 : Loss =  0.15853243  Acc:  0.9475333 Val_loss =  0.33109158 Val_acc =  0.8877\n",
            "Iteration  871 : Loss =  0.15879352  Acc:  0.9475833 Val_loss =  0.33120152 Val_acc =  0.8877\n",
            "Iteration  872 : Loss =  0.16028577  Acc:  0.9462 Val_loss =  0.33377337 Val_acc =  0.8855\n",
            "Iteration  873 : Loss =  0.16115452  Acc:  0.94533336 Val_loss =  0.33397785 Val_acc =  0.887\n",
            "Iteration  874 : Loss =  0.16173998  Acc:  0.9453833 Val_loss =  0.33560383 Val_acc =  0.8845\n",
            "Iteration  875 : Loss =  0.16018906  Acc:  0.9462 Val_loss =  0.3333727 Val_acc =  0.8863\n",
            "Iteration  876 : Loss =  0.15856588  Acc:  0.9475333 Val_loss =  0.33216876 Val_acc =  0.8853\n",
            "Iteration  877 : Loss =  0.15739636  Acc:  0.9479167 Val_loss =  0.3310666 Val_acc =  0.8886\n",
            "Iteration  878 : Loss =  0.15726666  Acc:  0.94813335 Val_loss =  0.33094013 Val_acc =  0.888\n",
            "Iteration  879 : Loss =  0.15793785  Acc:  0.9472167 Val_loss =  0.33238417 Val_acc =  0.8851\n",
            "Iteration  880 : Loss =  0.158482  Acc:  0.94695 Val_loss =  0.33251175 Val_acc =  0.8877\n",
            "Iteration  881 : Loss =  0.15874992  Acc:  0.9469 Val_loss =  0.3336073 Val_acc =  0.8853\n",
            "Iteration  882 : Loss =  0.15786883  Acc:  0.94705 Val_loss =  0.33214867 Val_acc =  0.888\n",
            "Iteration  883 : Loss =  0.15693465  Acc:  0.9478 Val_loss =  0.33167723 Val_acc =  0.8858\n",
            "Iteration  884 : Loss =  0.15623264  Acc:  0.9485833 Val_loss =  0.33092135 Val_acc =  0.8885\n",
            "Iteration  885 : Loss =  0.156122  Acc:  0.94846666 Val_loss =  0.33088848 Val_acc =  0.8883\n",
            "Iteration  886 : Loss =  0.15644549  Acc:  0.94816667 Val_loss =  0.3318022 Val_acc =  0.8863\n",
            "Iteration  887 : Loss =  0.15673667  Acc:  0.94801664 Val_loss =  0.33174154 Val_acc =  0.8879\n",
            "Iteration  888 : Loss =  0.15685406  Acc:  0.9475833 Val_loss =  0.3326495 Val_acc =  0.8854\n",
            "Iteration  889 : Loss =  0.15640274  Acc:  0.94815 Val_loss =  0.33171192 Val_acc =  0.8882\n",
            "Iteration  890 : Loss =  0.15586042  Acc:  0.9482833 Val_loss =  0.33177394 Val_acc =  0.8864\n",
            "Iteration  891 : Loss =  0.15531676  Acc:  0.9486833 Val_loss =  0.331048 Val_acc =  0.888\n",
            "Iteration  892 : Loss =  0.15503663  Acc:  0.94886667 Val_loss =  0.33105412 Val_acc =  0.8888\n",
            "Iteration  893 : Loss =  0.15501519  Acc:  0.9489833 Val_loss =  0.33128756 Val_acc =  0.8875\n",
            "Iteration  894 : Loss =  0.15511468  Acc:  0.9487333 Val_loss =  0.33127183 Val_acc =  0.8879\n",
            "Iteration  895 : Loss =  0.1552128  Acc:  0.9485667 Val_loss =  0.33192036 Val_acc =  0.8858\n",
            "Iteration  896 : Loss =  0.1551226  Acc:  0.94886667 Val_loss =  0.33149558 Val_acc =  0.8886\n",
            "Iteration  897 : Loss =  0.15493064  Acc:  0.94875 Val_loss =  0.33191016 Val_acc =  0.8858\n",
            "Iteration  898 : Loss =  0.15459391  Acc:  0.9489667 Val_loss =  0.33129844 Val_acc =  0.8885\n",
            "Iteration  899 : Loss =  0.15428701  Acc:  0.94915 Val_loss =  0.33148444 Val_acc =  0.887\n",
            "Iteration  900 : Loss =  0.1540421  Acc:  0.9491 Val_loss =  0.33113334 Val_acc =  0.8889\n",
            "Iteration  901 : Loss =  0.15388967  Acc:  0.94913334 Val_loss =  0.33123773 Val_acc =  0.888\n",
            "Iteration  902 : Loss =  0.15382467  Acc:  0.9493167 Val_loss =  0.3312872 Val_acc =  0.8885\n",
            "Iteration  903 : Loss =  0.15381262  Acc:  0.94928336 Val_loss =  0.33137608 Val_acc =  0.888\n",
            "Iteration  904 : Loss =  0.15380353  Acc:  0.949 Val_loss =  0.33166376 Val_acc =  0.8877\n",
            "Iteration  905 : Loss =  0.15376806  Acc:  0.94906664 Val_loss =  0.33165094 Val_acc =  0.8887\n",
            "Iteration  906 : Loss =  0.15370643  Acc:  0.94915 Val_loss =  0.33189645 Val_acc =  0.8871\n",
            "Iteration  907 : Loss =  0.15360785  Acc:  0.94918334 Val_loss =  0.33182 Val_acc =  0.8881\n",
            "Iteration  908 : Loss =  0.15348011  Acc:  0.94948334 Val_loss =  0.33190453 Val_acc =  0.887\n",
            "Iteration  909 : Loss =  0.1533794  Acc:  0.94953334 Val_loss =  0.33195347 Val_acc =  0.8881\n",
            "Iteration  910 : Loss =  0.15325715  Acc:  0.94965 Val_loss =  0.33187678 Val_acc =  0.8868\n",
            "Iteration  911 : Loss =  0.1532283  Acc:  0.9494 Val_loss =  0.33220187 Val_acc =  0.8875\n",
            "Iteration  912 : Loss =  0.15320209  Acc:  0.94965 Val_loss =  0.33203316 Val_acc =  0.8878\n",
            "Iteration  913 : Loss =  0.15331003  Acc:  0.9493833 Val_loss =  0.33271924 Val_acc =  0.8866\n",
            "Iteration  914 : Loss =  0.15340446  Acc:  0.94981664 Val_loss =  0.33246005 Val_acc =  0.8885\n",
            "Iteration  915 : Loss =  0.1535893  Acc:  0.94888335 Val_loss =  0.3334541 Val_acc =  0.8866\n",
            "Iteration  916 : Loss =  0.15367025  Acc:  0.94965 Val_loss =  0.33295146 Val_acc =  0.8884\n",
            "Iteration  917 : Loss =  0.1537085  Acc:  0.94893336 Val_loss =  0.3339686 Val_acc =  0.8871\n",
            "Iteration  918 : Loss =  0.15351316  Acc:  0.9497 Val_loss =  0.333074 Val_acc =  0.8886\n",
            "Iteration  919 : Loss =  0.15316986  Acc:  0.9491 Val_loss =  0.33373597 Val_acc =  0.8867\n",
            "Iteration  920 : Loss =  0.15262184  Acc:  0.9500667 Val_loss =  0.33252177 Val_acc =  0.8886\n",
            "Iteration  921 : Loss =  0.15204245  Acc:  0.9496667 Val_loss =  0.33274993 Val_acc =  0.8874\n",
            "Iteration  922 : Loss =  0.1515126  Acc:  0.95053333 Val_loss =  0.33180243 Val_acc =  0.8878\n",
            "Iteration  923 : Loss =  0.15114503  Acc:  0.9501167 Val_loss =  0.33194023 Val_acc =  0.8873\n",
            "Iteration  924 : Loss =  0.15095618  Acc:  0.9504667 Val_loss =  0.33172455 Val_acc =  0.8879\n",
            "Iteration  925 : Loss =  0.15092354  Acc:  0.95058334 Val_loss =  0.33186293 Val_acc =  0.8876\n",
            "Iteration  926 : Loss =  0.15101069  Acc:  0.9504667 Val_loss =  0.33223426 Val_acc =  0.8887\n",
            "Iteration  927 : Loss =  0.1511207  Acc:  0.95066667 Val_loss =  0.33228424 Val_acc =  0.8874\n",
            "Iteration  928 : Loss =  0.15129124  Acc:  0.95025 Val_loss =  0.33296844 Val_acc =  0.8872\n",
            "Iteration  929 : Loss =  0.15133561  Acc:  0.9504833 Val_loss =  0.3327685 Val_acc =  0.8881\n",
            "Iteration  930 : Loss =  0.15138666  Acc:  0.9501167 Val_loss =  0.3333373 Val_acc =  0.8874\n",
            "Iteration  931 : Loss =  0.15118143  Acc:  0.9504333 Val_loss =  0.33288053 Val_acc =  0.888\n",
            "Iteration  932 : Loss =  0.15097377  Acc:  0.95051664 Val_loss =  0.33315918 Val_acc =  0.8874\n",
            "Iteration  933 : Loss =  0.15056692  Acc:  0.9509 Val_loss =  0.33265424 Val_acc =  0.8875\n",
            "Iteration  934 : Loss =  0.15021595  Acc:  0.9507167 Val_loss =  0.33259672 Val_acc =  0.8876\n",
            "Iteration  935 : Loss =  0.14985299  Acc:  0.9508333 Val_loss =  0.33233383 Val_acc =  0.8869\n",
            "Iteration  936 : Loss =  0.14961828  Acc:  0.95105 Val_loss =  0.3321733 Val_acc =  0.8884\n",
            "Iteration  937 : Loss =  0.14951204  Acc:  0.95091665 Val_loss =  0.33247232 Val_acc =  0.8872\n",
            "Iteration  938 : Loss =  0.14952742  Acc:  0.95103335 Val_loss =  0.33230796 Val_acc =  0.889\n",
            "Iteration  939 : Loss =  0.14971127  Acc:  0.9504833 Val_loss =  0.33311856 Val_acc =  0.8862\n",
            "Iteration  940 : Loss =  0.14993127  Acc:  0.95093334 Val_loss =  0.3328591 Val_acc =  0.8886\n",
            "Iteration  941 : Loss =  0.15031688  Acc:  0.9504167 Val_loss =  0.3341609 Val_acc =  0.8865\n",
            "Iteration  942 : Loss =  0.15053454  Acc:  0.95075 Val_loss =  0.33367315 Val_acc =  0.8885\n",
            "Iteration  943 : Loss =  0.15090701  Acc:  0.9497833 Val_loss =  0.33523306 Val_acc =  0.8871\n",
            "Iteration  944 : Loss =  0.15080959  Acc:  0.9504667 Val_loss =  0.33419192 Val_acc =  0.8884\n",
            "Iteration  945 : Loss =  0.1508838  Acc:  0.9496833 Val_loss =  0.33552897 Val_acc =  0.8864\n",
            "Iteration  946 : Loss =  0.15027815  Acc:  0.95066667 Val_loss =  0.33396286 Val_acc =  0.8879\n",
            "Iteration  947 : Loss =  0.14989844  Acc:  0.94991666 Val_loss =  0.3346463 Val_acc =  0.8858\n",
            "Iteration  948 : Loss =  0.14918335  Acc:  0.95103335 Val_loss =  0.33333924 Val_acc =  0.8889\n",
            "Iteration  949 : Loss =  0.1487817  Acc:  0.9511667 Val_loss =  0.33353248 Val_acc =  0.8858\n",
            "Iteration  950 : Loss =  0.14857724  Acc:  0.9514 Val_loss =  0.33335733 Val_acc =  0.8878\n",
            "Iteration  951 : Loss =  0.14853679  Acc:  0.95163333 Val_loss =  0.33336598 Val_acc =  0.8878\n",
            "Iteration  952 : Loss =  0.14873523  Acc:  0.95091665 Val_loss =  0.33414522 Val_acc =  0.8876\n",
            "Iteration  953 : Loss =  0.1487646  Acc:  0.95175 Val_loss =  0.3338171 Val_acc =  0.8881\n",
            "Iteration  954 : Loss =  0.14877175  Acc:  0.95053333 Val_loss =  0.3346534 Val_acc =  0.8871\n",
            "Iteration  955 : Loss =  0.1484508  Acc:  0.9518 Val_loss =  0.33378482 Val_acc =  0.8882\n",
            "Iteration  956 : Loss =  0.14801608  Acc:  0.951 Val_loss =  0.33409607 Val_acc =  0.8871\n",
            "Iteration  957 : Loss =  0.14745437  Acc:  0.95243335 Val_loss =  0.33311713 Val_acc =  0.8882\n",
            "Iteration  958 : Loss =  0.14695632  Acc:  0.9518833 Val_loss =  0.3331579 Val_acc =  0.8878\n",
            "Iteration  959 : Loss =  0.14661928  Acc:  0.9522667 Val_loss =  0.33267003 Val_acc =  0.8888\n",
            "Iteration  960 : Loss =  0.14650065  Acc:  0.9524 Val_loss =  0.3328755 Val_acc =  0.8874\n",
            "Iteration  961 : Loss =  0.14658213  Acc:  0.9521 Val_loss =  0.33307743 Val_acc =  0.8885\n",
            "Iteration  962 : Loss =  0.1467796  Acc:  0.95236665 Val_loss =  0.33345914 Val_acc =  0.8877\n",
            "Iteration  963 : Loss =  0.14709434  Acc:  0.9518 Val_loss =  0.33404177 Val_acc =  0.8871\n",
            "Iteration  964 : Loss =  0.14725226  Acc:  0.95198333 Val_loss =  0.33425504 Val_acc =  0.8875\n",
            "Iteration  965 : Loss =  0.1474808  Acc:  0.9515333 Val_loss =  0.33472392 Val_acc =  0.8871\n",
            "Iteration  966 : Loss =  0.14733358  Acc:  0.95185 Val_loss =  0.33462298 Val_acc =  0.8876\n",
            "Iteration  967 : Loss =  0.14723851  Acc:  0.9519333 Val_loss =  0.33463264 Val_acc =  0.8875\n",
            "Iteration  968 : Loss =  0.14691897  Acc:  0.9518 Val_loss =  0.3345729 Val_acc =  0.8869\n",
            "Iteration  969 : Loss =  0.1466619  Acc:  0.9519333 Val_loss =  0.33417028 Val_acc =  0.8882\n",
            "Iteration  970 : Loss =  0.1465958  Acc:  0.9514167 Val_loss =  0.3347603 Val_acc =  0.8853\n",
            "Iteration  971 : Loss =  0.14648591  Acc:  0.95208335 Val_loss =  0.33416387 Val_acc =  0.8882\n",
            "Iteration  972 : Loss =  0.14666778  Acc:  0.95145 Val_loss =  0.3353363 Val_acc =  0.8868\n",
            "Iteration  973 : Loss =  0.14655305  Acc:  0.95208335 Val_loss =  0.33443317 Val_acc =  0.8882\n",
            "Iteration  974 : Loss =  0.1464901  Acc:  0.9515667 Val_loss =  0.33550298 Val_acc =  0.8874\n",
            "Iteration  975 : Loss =  0.14604065  Acc:  0.95278335 Val_loss =  0.3342225 Val_acc =  0.8882\n",
            "Iteration  976 : Loss =  0.14553332  Acc:  0.95208335 Val_loss =  0.33471656 Val_acc =  0.888\n",
            "Iteration  977 : Loss =  0.14490882  Acc:  0.9533167 Val_loss =  0.33352974 Val_acc =  0.8879\n",
            "Iteration  978 : Loss =  0.14439917  Acc:  0.9529667 Val_loss =  0.33361104 Val_acc =  0.8874\n",
            "Iteration  979 : Loss =  0.14405328  Acc:  0.9533167 Val_loss =  0.333167 Val_acc =  0.8884\n",
            "Iteration  980 : Loss =  0.14389247  Acc:  0.9536 Val_loss =  0.33319327 Val_acc =  0.8881\n",
            "Iteration  981 : Loss =  0.14387272  Acc:  0.9531 Val_loss =  0.3335426 Val_acc =  0.8876\n",
            "Iteration  982 : Loss =  0.14393866  Acc:  0.9536167 Val_loss =  0.33346003 Val_acc =  0.8878\n",
            "Iteration  983 : Loss =  0.1440459  Acc:  0.9529167 Val_loss =  0.33420396 Val_acc =  0.8877\n",
            "Iteration  984 : Loss =  0.14412497  Acc:  0.9536167 Val_loss =  0.33387843 Val_acc =  0.8879\n",
            "Iteration  985 : Loss =  0.14419971  Acc:  0.9527 Val_loss =  0.3347315 Val_acc =  0.8875\n",
            "Iteration  986 : Loss =  0.14416091  Acc:  0.95351666 Val_loss =  0.33416668 Val_acc =  0.8887\n",
            "Iteration  987 : Loss =  0.14414817  Acc:  0.9525833 Val_loss =  0.33500418 Val_acc =  0.8873\n",
            "Iteration  988 : Loss =  0.14401016  Acc:  0.9534 Val_loss =  0.33430663 Val_acc =  0.888\n",
            "Iteration  989 : Loss =  0.14401507  Acc:  0.95246667 Val_loss =  0.3351355 Val_acc =  0.8863\n",
            "Iteration  990 : Loss =  0.14393851  Acc:  0.95311666 Val_loss =  0.33453652 Val_acc =  0.8878\n",
            "Iteration  991 : Loss =  0.144142  Acc:  0.9526167 Val_loss =  0.33551288 Val_acc =  0.8864\n",
            "Iteration  992 : Loss =  0.14432223  Acc:  0.95285 Val_loss =  0.33532655 Val_acc =  0.8872\n",
            "Iteration  993 : Loss =  0.14470899  Acc:  0.9526333 Val_loss =  0.33634588 Val_acc =  0.8864\n",
            "Iteration  994 : Loss =  0.1450982  Acc:  0.95215 Val_loss =  0.3365672 Val_acc =  0.8865\n",
            "Iteration  995 : Loss =  0.14517924  Acc:  0.9524 Val_loss =  0.33704787 Val_acc =  0.8871\n",
            "Iteration  996 : Loss =  0.14513247  Acc:  0.95245 Val_loss =  0.33698636 Val_acc =  0.887\n",
            "Iteration  997 : Loss =  0.14429404  Acc:  0.9529 Val_loss =  0.3363397 Val_acc =  0.8881\n",
            "Iteration  998 : Loss =  0.14337322  Acc:  0.95336664 Val_loss =  0.33547965 Val_acc =  0.888\n",
            "Iteration  999 : Loss =  0.1422818  Acc:  0.9544167 Val_loss =  0.33450943 Val_acc =  0.8874\n",
            "Iteration  1000 : Loss =  0.14161439  Acc:  0.95435 Val_loss =  0.33399504 Val_acc =  0.8885\n",
            "Iteration  1001 : Loss =  0.14144671  Acc:  0.95446664 Val_loss =  0.33398047 Val_acc =  0.8881\n",
            "Iteration  1002 : Loss =  0.14168377  Acc:  0.95461667 Val_loss =  0.3344428 Val_acc =  0.8868\n",
            "Iteration  1003 : Loss =  0.14215517  Acc:  0.9539 Val_loss =  0.33506444 Val_acc =  0.8883\n",
            "Iteration  1004 : Loss =  0.14260645  Acc:  0.95385 Val_loss =  0.33583155 Val_acc =  0.888\n",
            "Iteration  1005 : Loss =  0.14301242  Acc:  0.95325 Val_loss =  0.33625534 Val_acc =  0.8875\n",
            "Iteration  1006 : Loss =  0.14300807  Acc:  0.95341665 Val_loss =  0.33664975 Val_acc =  0.8877\n",
            "Iteration  1007 : Loss =  0.14274785  Acc:  0.9535667 Val_loss =  0.33618447 Val_acc =  0.8871\n",
            "Iteration  1008 : Loss =  0.14215447  Acc:  0.95365 Val_loss =  0.3361241 Val_acc =  0.8866\n",
            "Iteration  1009 : Loss =  0.14145061  Acc:  0.95426667 Val_loss =  0.3350385 Val_acc =  0.8879\n",
            "Iteration  1010 : Loss =  0.14099294  Acc:  0.95411664 Val_loss =  0.33523437 Val_acc =  0.8879\n",
            "Iteration  1011 : Loss =  0.14078234  Acc:  0.9547167 Val_loss =  0.33461547 Val_acc =  0.888\n",
            "Iteration  1012 : Loss =  0.1408683  Acc:  0.95426667 Val_loss =  0.33537352 Val_acc =  0.8879\n",
            "Iteration  1013 : Loss =  0.14104551  Acc:  0.9545 Val_loss =  0.33519706 Val_acc =  0.888\n",
            "Iteration  1014 : Loss =  0.14119719  Acc:  0.9540833 Val_loss =  0.336029 Val_acc =  0.8884\n",
            "Iteration  1015 : Loss =  0.14107876  Acc:  0.95488334 Val_loss =  0.3356099 Val_acc =  0.8879\n",
            "Iteration  1016 : Loss =  0.14080697  Acc:  0.9544 Val_loss =  0.33592105 Val_acc =  0.8881\n",
            "Iteration  1017 : Loss =  0.14027375  Acc:  0.95531666 Val_loss =  0.33516786 Val_acc =  0.8881\n",
            "Iteration  1018 : Loss =  0.13976543  Acc:  0.9551 Val_loss =  0.33508915 Val_acc =  0.8886\n",
            "Iteration  1019 : Loss =  0.13937382  Acc:  0.95556664 Val_loss =  0.33469042 Val_acc =  0.8876\n",
            "Iteration  1020 : Loss =  0.13919131  Acc:  0.95558333 Val_loss =  0.33470196 Val_acc =  0.8876\n",
            "Iteration  1021 : Loss =  0.13918112  Acc:  0.9553667 Val_loss =  0.33492202 Val_acc =  0.8882\n",
            "Iteration  1022 : Loss =  0.13926522  Acc:  0.9557667 Val_loss =  0.33493224 Val_acc =  0.8867\n",
            "Iteration  1023 : Loss =  0.13939005  Acc:  0.9551 Val_loss =  0.3355211 Val_acc =  0.8888\n",
            "Iteration  1024 : Loss =  0.13944577  Acc:  0.9555333 Val_loss =  0.33533415 Val_acc =  0.8875\n",
            "Iteration  1025 : Loss =  0.13943824  Acc:  0.95491666 Val_loss =  0.3359837 Val_acc =  0.8886\n",
            "Iteration  1026 : Loss =  0.13929914  Acc:  0.9554667 Val_loss =  0.33547434 Val_acc =  0.888\n",
            "Iteration  1027 : Loss =  0.13911659  Acc:  0.95505 Val_loss =  0.33599848 Val_acc =  0.8877\n",
            "Iteration  1028 : Loss =  0.13893014  Acc:  0.95566666 Val_loss =  0.33535284 Val_acc =  0.8876\n",
            "Iteration  1029 : Loss =  0.1388259  Acc:  0.9548333 Val_loss =  0.33599418 Val_acc =  0.8876\n",
            "Iteration  1030 : Loss =  0.13883464  Acc:  0.95545 Val_loss =  0.33551475 Val_acc =  0.8881\n",
            "Iteration  1031 : Loss =  0.13908893  Acc:  0.95463336 Val_loss =  0.33663175 Val_acc =  0.8875\n",
            "Iteration  1032 : Loss =  0.1394126  Acc:  0.9554833 Val_loss =  0.33638224 Val_acc =  0.8882\n",
            "Iteration  1033 : Loss =  0.140163  Acc:  0.9536833 Val_loss =  0.33817318 Val_acc =  0.8868\n",
            "Iteration  1034 : Loss =  0.14060518  Acc:  0.9544 Val_loss =  0.33786452 Val_acc =  0.8878\n",
            "Iteration  1035 : Loss =  0.14150241  Acc:  0.95308334 Val_loss =  0.3399867 Val_acc =  0.8864\n",
            "Iteration  1036 : Loss =  0.14124723  Acc:  0.95426667 Val_loss =  0.338775 Val_acc =  0.8878\n",
            "Iteration  1037 : Loss =  0.14119552  Acc:  0.95306665 Val_loss =  0.33991605 Val_acc =  0.8862\n",
            "Iteration  1038 : Loss =  0.13974853  Acc:  0.95491666 Val_loss =  0.3375172 Val_acc =  0.8877\n",
            "Iteration  1039 : Loss =  0.13860218  Acc:  0.95445 Val_loss =  0.33727756 Val_acc =  0.8873\n",
            "Iteration  1040 : Loss =  0.13746351  Acc:  0.95615 Val_loss =  0.33560142 Val_acc =  0.8883\n",
            "Iteration  1041 : Loss =  0.13697046  Acc:  0.9558667 Val_loss =  0.33564577 Val_acc =  0.8878\n",
            "Iteration  1042 : Loss =  0.13703275  Acc:  0.95636666 Val_loss =  0.33578697 Val_acc =  0.8875\n",
            "Iteration  1043 : Loss =  0.13745663  Acc:  0.9558667 Val_loss =  0.3363187 Val_acc =  0.8879\n",
            "Iteration  1044 : Loss =  0.13797617  Acc:  0.9552 Val_loss =  0.3374062 Val_acc =  0.8875\n",
            "Iteration  1045 : Loss =  0.1383272  Acc:  0.95561665 Val_loss =  0.33742863 Val_acc =  0.888\n",
            "Iteration  1046 : Loss =  0.13855217  Acc:  0.9547833 Val_loss =  0.33846176 Val_acc =  0.8867\n",
            "Iteration  1047 : Loss =  0.1381773  Acc:  0.95571667 Val_loss =  0.33747846 Val_acc =  0.8875\n",
            "Iteration  1048 : Loss =  0.13780665  Acc:  0.9546667 Val_loss =  0.33796218 Val_acc =  0.8874\n",
            "Iteration  1049 : Loss =  0.13705985  Acc:  0.9565167 Val_loss =  0.33656913 Val_acc =  0.8877\n",
            "Iteration  1050 : Loss =  0.13654795  Acc:  0.9554833 Val_loss =  0.3368379 Val_acc =  0.8883\n",
            "Iteration  1051 : Loss =  0.1361919  Acc:  0.9569167 Val_loss =  0.336055 Val_acc =  0.887\n",
            "Iteration  1052 : Loss =  0.1361058  Acc:  0.9564667 Val_loss =  0.3365541 Val_acc =  0.888\n",
            "Iteration  1053 : Loss =  0.13617754  Acc:  0.9566 Val_loss =  0.3364813 Val_acc =  0.8879\n",
            "Iteration  1054 : Loss =  0.13634351  Acc:  0.95625 Val_loss =  0.3370124 Val_acc =  0.8879\n",
            "Iteration  1055 : Loss =  0.13634457  Acc:  0.95655 Val_loss =  0.3371055 Val_acc =  0.8878\n",
            "Iteration  1056 : Loss =  0.13629547  Acc:  0.9563 Val_loss =  0.3371711 Val_acc =  0.8878\n",
            "Iteration  1057 : Loss =  0.13599402  Acc:  0.9564833 Val_loss =  0.33710796 Val_acc =  0.8878\n",
            "Iteration  1058 : Loss =  0.13563345  Acc:  0.95676666 Val_loss =  0.33672547 Val_acc =  0.8876\n",
            "Iteration  1059 : Loss =  0.13524836  Acc:  0.9568167 Val_loss =  0.33672425 Val_acc =  0.8876\n",
            "Iteration  1060 : Loss =  0.13492146  Acc:  0.95725 Val_loss =  0.3362887 Val_acc =  0.8879\n",
            "Iteration  1061 : Loss =  0.13472596  Acc:  0.9571667 Val_loss =  0.3364924 Val_acc =  0.8875\n",
            "Iteration  1062 : Loss =  0.13464735  Acc:  0.9572333 Val_loss =  0.3362576 Val_acc =  0.8876\n",
            "Iteration  1063 : Loss =  0.13467123  Acc:  0.95706666 Val_loss =  0.33672982 Val_acc =  0.8875\n",
            "Iteration  1064 : Loss =  0.13474588  Acc:  0.95735 Val_loss =  0.33663526 Val_acc =  0.8871\n",
            "Iteration  1065 : Loss =  0.1348522  Acc:  0.9569333 Val_loss =  0.3372871 Val_acc =  0.8879\n",
            "Iteration  1066 : Loss =  0.1349011  Acc:  0.95738333 Val_loss =  0.33710292 Val_acc =  0.8875\n",
            "Iteration  1067 : Loss =  0.13495809  Acc:  0.9566333 Val_loss =  0.33778444 Val_acc =  0.8881\n",
            "Iteration  1068 : Loss =  0.13489108  Acc:  0.9572833 Val_loss =  0.33735487 Val_acc =  0.8873\n",
            "Iteration  1069 : Loss =  0.13481441  Acc:  0.95666665 Val_loss =  0.3379605 Val_acc =  0.8881\n",
            "Iteration  1070 : Loss =  0.13461725  Acc:  0.9575167 Val_loss =  0.33732623 Val_acc =  0.8878\n",
            "Iteration  1071 : Loss =  0.13441896  Acc:  0.9568167 Val_loss =  0.33788013 Val_acc =  0.8876\n",
            "Iteration  1072 : Loss =  0.13421088  Acc:  0.9577 Val_loss =  0.3372019 Val_acc =  0.8878\n",
            "Iteration  1073 : Loss =  0.13406353  Acc:  0.95676666 Val_loss =  0.33785298 Val_acc =  0.8879\n",
            "Iteration  1074 : Loss =  0.13398339  Acc:  0.9578 Val_loss =  0.33722544 Val_acc =  0.8878\n",
            "Iteration  1075 : Loss =  0.13405211  Acc:  0.9565 Val_loss =  0.33815494 Val_acc =  0.8874\n",
            "Iteration  1076 : Loss =  0.13412464  Acc:  0.95765 Val_loss =  0.33760253 Val_acc =  0.8884\n",
            "Iteration  1077 : Loss =  0.13440937  Acc:  0.9563 Val_loss =  0.33887914 Val_acc =  0.8875\n",
            "Iteration  1078 : Loss =  0.1344813  Acc:  0.9574 Val_loss =  0.3382476 Val_acc =  0.8879\n",
            "Iteration  1079 : Loss =  0.13475437  Acc:  0.9562333 Val_loss =  0.3396155 Val_acc =  0.8873\n",
            "Iteration  1080 : Loss =  0.13457082  Acc:  0.9573333 Val_loss =  0.3386712 Val_acc =  0.8869\n",
            "Iteration  1081 : Loss =  0.13450013  Acc:  0.95626664 Val_loss =  0.33961225 Val_acc =  0.8872\n",
            "Iteration  1082 : Loss =  0.13393368  Acc:  0.9576333 Val_loss =  0.33832526 Val_acc =  0.8875\n",
            "Iteration  1083 : Loss =  0.13346647  Acc:  0.95678335 Val_loss =  0.33870664 Val_acc =  0.8877\n",
            "Iteration  1084 : Loss =  0.13281994  Acc:  0.9583333 Val_loss =  0.33759296 Val_acc =  0.888\n",
            "Iteration  1085 : Loss =  0.1323328  Acc:  0.95781666 Val_loss =  0.33770302 Val_acc =  0.888\n",
            "Iteration  1086 : Loss =  0.13196513  Acc:  0.95855 Val_loss =  0.33716637 Val_acc =  0.8875\n",
            "Iteration  1087 : Loss =  0.13176677  Acc:  0.9583833 Val_loss =  0.337264 Val_acc =  0.8872\n",
            "Iteration  1088 : Loss =  0.13169363  Acc:  0.9586833 Val_loss =  0.3373469 Val_acc =  0.8881\n",
            "Iteration  1089 : Loss =  0.13171662  Acc:  0.9583333 Val_loss =  0.33744764 Val_acc =  0.8876\n",
            "Iteration  1090 : Loss =  0.13180462  Acc:  0.9583 Val_loss =  0.33791956 Val_acc =  0.8881\n",
            "Iteration  1091 : Loss =  0.13193233  Acc:  0.9586167 Val_loss =  0.33788812 Val_acc =  0.8878\n",
            "Iteration  1092 : Loss =  0.13209303  Acc:  0.95775 Val_loss =  0.33857113 Val_acc =  0.888\n",
            "Iteration  1093 : Loss =  0.13221076  Acc:  0.9586833 Val_loss =  0.33839595 Val_acc =  0.888\n",
            "Iteration  1094 : Loss =  0.1323578  Acc:  0.9575667 Val_loss =  0.33919498 Val_acc =  0.8879\n",
            "Iteration  1095 : Loss =  0.1323438  Acc:  0.95853335 Val_loss =  0.3387614 Val_acc =  0.8878\n",
            "Iteration  1096 : Loss =  0.1323295  Acc:  0.95745 Val_loss =  0.33949474 Val_acc =  0.8874\n",
            "Iteration  1097 : Loss =  0.13206899  Acc:  0.9586667 Val_loss =  0.33876142 Val_acc =  0.8877\n",
            "Iteration  1098 : Loss =  0.13183402  Acc:  0.95755 Val_loss =  0.33926585 Val_acc =  0.8879\n",
            "Iteration  1099 : Loss =  0.13141312  Acc:  0.959 Val_loss =  0.3383972 Val_acc =  0.8882\n",
            "Iteration  1100 : Loss =  0.13107336  Acc:  0.9582833 Val_loss =  0.33871028 Val_acc =  0.8875\n",
            "Iteration  1101 : Loss =  0.13071519  Acc:  0.9590333 Val_loss =  0.33802924 Val_acc =  0.8883\n",
            "Iteration  1102 : Loss =  0.13048573  Acc:  0.9587167 Val_loss =  0.3384058 Val_acc =  0.8879\n",
            "Iteration  1103 : Loss =  0.13035396  Acc:  0.95928335 Val_loss =  0.33804643 Val_acc =  0.888\n",
            "Iteration  1104 : Loss =  0.1303547  Acc:  0.9589 Val_loss =  0.3385958 Val_acc =  0.887\n",
            "Iteration  1105 : Loss =  0.13050988  Acc:  0.9591333 Val_loss =  0.3384851 Val_acc =  0.8882\n",
            "Iteration  1106 : Loss =  0.1308642  Acc:  0.9583333 Val_loss =  0.3395102 Val_acc =  0.888\n",
            "Iteration  1107 : Loss =  0.13135587  Acc:  0.95886666 Val_loss =  0.33962736 Val_acc =  0.8877\n",
            "Iteration  1108 : Loss =  0.13214412  Acc:  0.9572833 Val_loss =  0.3413831 Val_acc =  0.8879\n",
            "Iteration  1109 : Loss =  0.13278934  Acc:  0.9576167 Val_loss =  0.34135196 Val_acc =  0.8878\n",
            "Iteration  1110 : Loss =  0.13355006  Acc:  0.9565667 Val_loss =  0.3433486 Val_acc =  0.8872\n",
            "Iteration  1111 : Loss =  0.13344812  Acc:  0.95743334 Val_loss =  0.34222797 Val_acc =  0.8874\n",
            "Iteration  1112 : Loss =  0.13290556  Acc:  0.9566 Val_loss =  0.34295648 Val_acc =  0.8877\n",
            "Iteration  1113 : Loss =  0.13161469  Acc:  0.9586167 Val_loss =  0.34060296 Val_acc =  0.8882\n",
            "Iteration  1114 : Loss =  0.1304155  Acc:  0.958 Val_loss =  0.34046277 Val_acc =  0.8874\n",
            "Iteration  1115 : Loss =  0.12982877  Acc:  0.95951664 Val_loss =  0.33912867 Val_acc =  0.8884\n",
            "Iteration  1116 : Loss =  0.13014175  Acc:  0.9583833 Val_loss =  0.34024373 Val_acc =  0.8875\n",
            "Iteration  1117 : Loss =  0.1308795  Acc:  0.9589667 Val_loss =  0.3407454 Val_acc =  0.8873\n",
            "Iteration  1118 : Loss =  0.13139248  Acc:  0.95776665 Val_loss =  0.34176964 Val_acc =  0.8882\n",
            "Iteration  1119 : Loss =  0.1316354  Acc:  0.9583167 Val_loss =  0.34203333 Val_acc =  0.8875\n",
            "Iteration  1120 : Loss =  0.13059951  Acc:  0.9583833 Val_loss =  0.3411829 Val_acc =  0.8883\n",
            "Iteration  1121 : Loss =  0.12942395  Acc:  0.95966667 Val_loss =  0.34003198 Val_acc =  0.8879\n",
            "Iteration  1122 : Loss =  0.1283436  Acc:  0.9597833 Val_loss =  0.3391056 Val_acc =  0.8876\n",
            "Iteration  1123 : Loss =  0.12800719  Acc:  0.9601 Val_loss =  0.3388364 Val_acc =  0.8877\n",
            "Iteration  1124 : Loss =  0.12834455  Acc:  0.95996666 Val_loss =  0.3394024 Val_acc =  0.8872\n",
            "Iteration  1125 : Loss =  0.12888196  Acc:  0.9594333 Val_loss =  0.34002537 Val_acc =  0.8878\n",
            "Iteration  1126 : Loss =  0.12932795  Acc:  0.95958334 Val_loss =  0.3406532 Val_acc =  0.8882\n",
            "Iteration  1127 : Loss =  0.1291503  Acc:  0.9591 Val_loss =  0.34069848 Val_acc =  0.8877\n",
            "Iteration  1128 : Loss =  0.12872751  Acc:  0.96026665 Val_loss =  0.34019503 Val_acc =  0.8874\n",
            "Iteration  1129 : Loss =  0.12811185  Acc:  0.9594667 Val_loss =  0.34001604 Val_acc =  0.8876\n",
            "Iteration  1130 : Loss =  0.12770632  Acc:  0.9605 Val_loss =  0.33926407 Val_acc =  0.8882\n",
            "Iteration  1131 : Loss =  0.12769173  Acc:  0.9593667 Val_loss =  0.34005982 Val_acc =  0.8874\n",
            "Iteration  1132 : Loss =  0.12792291  Acc:  0.96036667 Val_loss =  0.33982018 Val_acc =  0.888\n",
            "Iteration  1133 : Loss =  0.12822175  Acc:  0.9590167 Val_loss =  0.3410708 Val_acc =  0.8872\n",
            "Iteration  1134 : Loss =  0.12834008  Acc:  0.95993334 Val_loss =  0.34051427 Val_acc =  0.8875\n",
            "Iteration  1135 : Loss =  0.12825105  Acc:  0.95893335 Val_loss =  0.34125286 Val_acc =  0.8875\n",
            "Iteration  1136 : Loss =  0.1279166  Acc:  0.96023333 Val_loss =  0.3402978 Val_acc =  0.888\n",
            "Iteration  1137 : Loss =  0.12755726  Acc:  0.9594833 Val_loss =  0.34076616 Val_acc =  0.8875\n",
            "Iteration  1138 : Loss =  0.1272541  Acc:  0.96045 Val_loss =  0.3399748 Val_acc =  0.8882\n",
            "Iteration  1139 : Loss =  0.12721545  Acc:  0.95965 Val_loss =  0.34075284 Val_acc =  0.8878\n",
            "Iteration  1140 : Loss =  0.12727231  Acc:  0.96028334 Val_loss =  0.34035504 Val_acc =  0.8877\n",
            "Iteration  1141 : Loss =  0.12748627  Acc:  0.9594333 Val_loss =  0.34139466 Val_acc =  0.8874\n",
            "Iteration  1142 : Loss =  0.12753195  Acc:  0.96071666 Val_loss =  0.34093916 Val_acc =  0.8876\n",
            "Iteration  1143 : Loss =  0.12745662  Acc:  0.9594667 Val_loss =  0.34167016 Val_acc =  0.887\n",
            "Iteration  1144 : Loss =  0.12711787  Acc:  0.96085 Val_loss =  0.34076867 Val_acc =  0.8874\n",
            "Iteration  1145 : Loss =  0.12675889  Acc:  0.9597667 Val_loss =  0.34118894 Val_acc =  0.8873\n",
            "Iteration  1146 : Loss =  0.12634256  Acc:  0.96096665 Val_loss =  0.340245 Val_acc =  0.8881\n",
            "Iteration  1147 : Loss =  0.12611762  Acc:  0.96023333 Val_loss =  0.34076837 Val_acc =  0.8879\n",
            "Iteration  1148 : Loss =  0.12598936  Acc:  0.96108335 Val_loss =  0.34021327 Val_acc =  0.8877\n",
            "Iteration  1149 : Loss =  0.12596834  Acc:  0.96028334 Val_loss =  0.34086177 Val_acc =  0.8874\n",
            "Iteration  1150 : Loss =  0.12593411  Acc:  0.96113336 Val_loss =  0.34049594 Val_acc =  0.8877\n",
            "Iteration  1151 : Loss =  0.12585299  Acc:  0.9605333 Val_loss =  0.34108895 Val_acc =  0.8877\n",
            "Iteration  1152 : Loss =  0.12567669  Acc:  0.9611833 Val_loss =  0.3405308 Val_acc =  0.888\n",
            "Iteration  1153 : Loss =  0.1254516  Acc:  0.96078336 Val_loss =  0.3409915 Val_acc =  0.8874\n",
            "Iteration  1154 : Loss =  0.12520596  Acc:  0.96143335 Val_loss =  0.34029078 Val_acc =  0.8881\n",
            "Iteration  1155 : Loss =  0.124991715  Acc:  0.9609333 Val_loss =  0.34079266 Val_acc =  0.8872\n",
            "Iteration  1156 : Loss =  0.12483213  Acc:  0.9617 Val_loss =  0.34026548 Val_acc =  0.8882\n",
            "Iteration  1157 : Loss =  0.12475037  Acc:  0.96113336 Val_loss =  0.34082398 Val_acc =  0.8877\n",
            "Iteration  1158 : Loss =  0.12472379  Acc:  0.9615833 Val_loss =  0.3405341 Val_acc =  0.8875\n",
            "Iteration  1159 : Loss =  0.12476901  Acc:  0.96108335 Val_loss =  0.3410493 Val_acc =  0.888\n",
            "Iteration  1160 : Loss =  0.12482968  Acc:  0.96171665 Val_loss =  0.34093586 Val_acc =  0.8876\n",
            "Iteration  1161 : Loss =  0.12493149  Acc:  0.9608833 Val_loss =  0.3415152 Val_acc =  0.8876\n",
            "Iteration  1162 : Loss =  0.12501425  Acc:  0.96165 Val_loss =  0.34138393 Val_acc =  0.888\n",
            "Iteration  1163 : Loss =  0.12519483  Acc:  0.96023333 Val_loss =  0.34225997 Val_acc =  0.8873\n",
            "Iteration  1164 : Loss =  0.12530965  Acc:  0.9611 Val_loss =  0.3418582 Val_acc =  0.8879\n",
            "Iteration  1165 : Loss =  0.12567504  Acc:  0.9597333 Val_loss =  0.34329504 Val_acc =  0.8869\n",
            "Iteration  1166 : Loss =  0.12581602  Acc:  0.96063334 Val_loss =  0.34249708 Val_acc =  0.8885\n",
            "Iteration  1167 : Loss =  0.12635098  Acc:  0.95895 Val_loss =  0.3445027 Val_acc =  0.8872\n",
            "Iteration  1168 : Loss =  0.12632602  Acc:  0.9602 Val_loss =  0.3432202 Val_acc =  0.8889\n",
            "Iteration  1169 : Loss =  0.12661767  Acc:  0.9587167 Val_loss =  0.34515202 Val_acc =  0.887\n",
            "Iteration  1170 : Loss =  0.1260843  Acc:  0.96035 Val_loss =  0.34327957 Val_acc =  0.889\n",
            "Iteration  1171 : Loss =  0.1256355  Acc:  0.9593167 Val_loss =  0.34424615 Val_acc =  0.8874\n",
            "Iteration  1172 : Loss =  0.12474201  Acc:  0.96133333 Val_loss =  0.342308 Val_acc =  0.8886\n",
            "Iteration  1173 : Loss =  0.12394535  Acc:  0.9608667 Val_loss =  0.34245807 Val_acc =  0.8878\n",
            "Iteration  1174 : Loss =  0.12329826  Acc:  0.962 Val_loss =  0.34139106 Val_acc =  0.8879\n",
            "Iteration  1175 : Loss =  0.12293599  Acc:  0.9618833 Val_loss =  0.34145322 Val_acc =  0.8872\n",
            "Iteration  1176 : Loss =  0.12286023  Acc:  0.9618 Val_loss =  0.3416021 Val_acc =  0.8878\n",
            "Iteration  1177 : Loss =  0.12303032  Acc:  0.9623333 Val_loss =  0.3416354 Val_acc =  0.8876\n",
            "Iteration  1178 : Loss =  0.123407036  Acc:  0.96105 Val_loss =  0.3427704 Val_acc =  0.8875\n",
            "Iteration  1179 : Loss =  0.1237541  Acc:  0.96178335 Val_loss =  0.34252885 Val_acc =  0.8884\n",
            "Iteration  1180 : Loss =  0.124256395  Acc:  0.96001667 Val_loss =  0.3441846 Val_acc =  0.8869\n",
            "Iteration  1181 : Loss =  0.12429353  Acc:  0.9613 Val_loss =  0.34322906 Val_acc =  0.8891\n",
            "Iteration  1182 : Loss =  0.124376416  Acc:  0.95986664 Val_loss =  0.34462428 Val_acc =  0.8872\n",
            "Iteration  1183 : Loss =  0.123795986  Acc:  0.9616 Val_loss =  0.34292275 Val_acc =  0.8886\n",
            "Iteration  1184 : Loss =  0.123242915  Acc:  0.9605167 Val_loss =  0.34356475 Val_acc =  0.8873\n",
            "Iteration  1185 : Loss =  0.122481115  Acc:  0.96255 Val_loss =  0.34202906 Val_acc =  0.888\n",
            "Iteration  1186 : Loss =  0.12191065  Acc:  0.96183336 Val_loss =  0.34223032 Val_acc =  0.8877\n",
            "Iteration  1187 : Loss =  0.12156644  Acc:  0.9625667 Val_loss =  0.34170562 Val_acc =  0.8871\n",
            "Iteration  1188 : Loss =  0.12147135  Acc:  0.9626333 Val_loss =  0.34179208 Val_acc =  0.8873\n",
            "Iteration  1189 : Loss =  0.12156032  Acc:  0.96218336 Val_loss =  0.34233135 Val_acc =  0.8882\n",
            "Iteration  1190 : Loss =  0.121752456  Acc:  0.96276665 Val_loss =  0.34217593 Val_acc =  0.8882\n",
            "Iteration  1191 : Loss =  0.12201634  Acc:  0.9615 Val_loss =  0.3432981 Val_acc =  0.8878\n",
            "Iteration  1192 : Loss =  0.12216386  Acc:  0.9623333 Val_loss =  0.34278667 Val_acc =  0.8883\n",
            "Iteration  1193 : Loss =  0.122348726  Acc:  0.9611667 Val_loss =  0.34403554 Val_acc =  0.887\n",
            "Iteration  1194 : Loss =  0.12223284  Acc:  0.96218336 Val_loss =  0.3431476 Val_acc =  0.8885\n",
            "Iteration  1195 : Loss =  0.12215521  Acc:  0.96125 Val_loss =  0.34404957 Val_acc =  0.8869\n",
            "Iteration  1196 : Loss =  0.121800184  Acc:  0.96225 Val_loss =  0.34303966 Val_acc =  0.8883\n",
            "Iteration  1197 : Loss =  0.121501185  Acc:  0.9618667 Val_loss =  0.34346718 Val_acc =  0.8873\n",
            "Iteration  1198 : Loss =  0.12115132  Acc:  0.96311665 Val_loss =  0.34288052 Val_acc =  0.8883\n",
            "Iteration  1199 : Loss =  0.120933935  Acc:  0.9623167 Val_loss =  0.3430179 Val_acc =  0.8877\n",
            "\n",
            "The final test accuracy of the Fashion MNIST dataset is 0.8876000046730042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5vDaxkAUmuW",
        "colab_type": "text"
      },
      "source": [
        "### Plot of Training/Validation Accuracy and Training/Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxR4-UrqUs6X",
        "colab_type": "code",
        "outputId": "49463686-11dd-4685-a6d7-3f3669c3159f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(test_loss, label=\"Val Loss\", color='blue')\n",
        "plt.plot(test_acc, label=\"Val Acc\", color='orange')\n",
        "plt.plot(training_loss, label=\"Train Loss\", color='green')\n",
        "plt.plot(training_acc, label=\"Train Acc\", color='red')\n",
        "plt.title(\"Number of Iterations vs Training/Validation Loss and Accuracy\")\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Print the test accuracy \n",
        "print()\n",
        "print(\"The final test accuracy of the Fashion MNIST dataset is {}\".format(final_test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The final test accuracy of the Fashion MNIST dataset is 0.8876000046730042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IVDGPXIUjXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}