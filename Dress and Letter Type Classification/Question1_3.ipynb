{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question1_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-fVVPUgTLBI",
        "colab_type": "text"
      },
      "source": [
        "### Network Architecture B\n",
        "\n",
        "- Layer 1 - 300 neurons (ReLU activation function)\n",
        "- Layer 2 - 100 neurons (ReLU activation function) \n",
        "- Layer 2 - SoftMax Layer (10 neurons)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW_aiNbQTQ32",
        "colab_type": "code",
        "outputId": "2cc0ab10-d6db-4ce7-c56a-e682ea37fbf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Colab has two versions of TensorFlow installed: a 1.x version and a 2.x version. \n",
        "# Colab currently uses TF 1.x by default\n",
        "# To enable TF2 execute the following code\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfjwVfZBUIzw",
        "colab_type": "text"
      },
      "source": [
        "### Importing the Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRIegl_KTby3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the libraries \n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnHLpkGCiizM",
        "colab_type": "text"
      },
      "source": [
        "### L1 Regularisation to Network Architecture B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skswo6BtUM4f",
        "colab_type": "text"
      },
      "source": [
        "### Creation of 3 Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvR6_Zp-UD2I",
        "colab_type": "code",
        "outputId": "57d2b1a6-8511-420c-f118-2d95621bbd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -------------------------------------------CODE TO PREPARE THE DATASET---------------------------------------------------- \n",
        "\n",
        "def prepare_dataset(fashion_mnist):\n",
        "  # load the training and test data    \n",
        "  (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
        "  # reshape the feature data\n",
        "  tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
        "  te_x = te_x.reshape(te_x.shape[0], 784)\n",
        "  # noramlise feature data\n",
        "  tr_x = tr_x / 255.0\n",
        "  te_x = te_x / 255.0\n",
        "  # one hot encode the training labels and get the transpose\n",
        "  tr_y = np_utils.to_categorical(tr_y,10)\n",
        "  tr_y = tr_y.T\n",
        "  # one hot encode the test labels and get the transpose\n",
        "  te_y = np_utils.to_categorical(te_y,10)\n",
        "  te_y = te_y.T\n",
        "  return tr_x, tr_y, te_x, te_y\n",
        "\n",
        "# -----------------------------------CODE TO CALCULATE THE PROBABILITY OF EACH CLASS GIVEN THE TRAINING INSTANCE--------------------------------------\n",
        "\n",
        "def forward_pass(X_train, W1, b1, W2, b2, W3, b3):\n",
        "  \"\"\"\n",
        "  Return the predicted 10 class probabilities matrix for each of the training instances \n",
        "  \"\"\"\n",
        "  # Calculate the pre-activation outputs for each of the 300 neurons in the hidden layer 1 for each of the training instance \n",
        "  # Will get 300 outputs for a single training instance in the form of (300*60000) matrix \n",
        "  # The size of the W1 is (300*784)\n",
        "  # The size of the training feature matrix is (784*60000)\n",
        "  # The size of the b1 is (300*1)\n",
        "  A1=  tf.matmul(W1, X_train) + b1\n",
        "  # Convert each element in A1 through relu activation function \n",
        "  H1= tf.math.maximum(A1, 0)\n",
        "\n",
        "  # Calculate the pre-activation outputs for each of the 100 neurons in the hidden layer 2 for each of the training instance \n",
        "  # Will get 100 outputs for a single training instance in the form of (100*60000) matrix \n",
        "  # The size of the W2 is (100*300)\n",
        "  # The size of the H1 matrix is (300*60000)\n",
        "  # The size of the b2 is (100*1)\n",
        "  A2=  tf.matmul(W2, H1) + b2\n",
        "  # Convert each element in A1 through relu activation function \n",
        "  H2= tf.math.maximum(A2, 0)\n",
        "\n",
        "  # Calculate the pre-activation outputs for each of the 10 neurons in the softmax layer for each of the training instance \n",
        "  # Will get 10 outputs for a single training instance in the form of (10*60000) matrix \n",
        "  # The size of the W3 is (10*100)\n",
        "  # The size of the H2 matrix is (100*60000)\n",
        "  # The size of the bias matrix b3 is (10*1)\n",
        "  A3= tf.matmul(W3, H2) + b3\n",
        "  # Calculate a new matrix where each element is e to the power of pre-activation outputs \n",
        "  exponential_matrix= tf.math.exp(A3)\n",
        "  # Calculation of the final probabilities of each of the 10 classes for each instance in the training set \n",
        "  # Column wise sum calculation \n",
        "  column_sum= tf.reduce_sum(exponential_matrix, 0)\n",
        "  # Divide each element by the column sum so that each column is the probability of each class of a single instance \n",
        "  H3= exponential_matrix/column_sum \n",
        "  # Set the range so that the loss does not come out to be nan \n",
        "  H3= tf.clip_by_value(H3 ,1e-10, 1.0) \n",
        "  \n",
        "  return H3\n",
        "\n",
        "# -------------------------------- CODE TO CALCULATE THE LOSS FOR THE CURRENT SET OF TUNABLE PARAMETERS / WEIGHTS-------------------------------------\n",
        "\n",
        "def cross_entropy(y_train, y_pred_matrix, W1, W2, W3):\n",
        "  \"\"\"\n",
        "  Return the loss value given the predicted probabilities matrix and the actual probabilities matrix\n",
        "  Loss function also adds an additional component being the sum of elements in W1, W2 and W3 \n",
        "  \"\"\"\n",
        "  # Compute the log of each element of the prediction matrix \n",
        "  log_matrix= tf.math.log(y_pred_matrix)\n",
        "  # Multiply each element of the actual labels matrix with the log matrix \n",
        "  product_matrix= y_train *log_matrix\n",
        "  # Take the negation of each element in the product matrix \n",
        "  negated_product_matrix= -1*(product_matrix)\n",
        "  # Compute the cross entropy loss for each of the training instances \n",
        "  # This will contain individual loss for the all training instances \n",
        "  # This operation will perform the column wise sum \n",
        "  single_loss_matrix= tf.reduce_sum(negated_product_matrix, 0)\n",
        "  # Compute the mean cross entropy loss \n",
        "  mean_loss= tf.reduce_mean(single_loss_matrix)\n",
        "  \n",
        "  # Calculate the sum of elements of W1, W2, W3 \n",
        "  W1= tf.math.abs(W1)\n",
        "  W1_sum= tf.math.reduce_sum(W1)\n",
        "  W2= tf.math.abs(W2)\n",
        "  W2_sum= tf.math.reduce_sum(W2)\n",
        "  W3= tf.math.abs(W3)\n",
        "  W3_sum= tf.math.reduce_sum(W3)\n",
        "  weights_sum= W1_sum + W2_sum + W3_sum \n",
        "\n",
        "  # Multiply the regularisation rate with weights sum \n",
        "  reg_rate= 0.0001\n",
        "  product= reg_rate * weights_sum \n",
        "  # Final loss after applying L1 regularisation \n",
        "  final_loss= mean_loss + product \n",
        "\n",
        "  return final_loss\n",
        "\n",
        "# ----------------------------------------- CALCULATION OF THE TRAINING AND TEST SET ACCURACY------------------------------------------------------\n",
        "\n",
        "def return_labels(matrix):\n",
        "  \"\"\"\n",
        "  Return the corrosponding class label for each vector of probability instance \n",
        "  \"\"\" \n",
        "  class_labels= tf.argmax(matrix) \n",
        "  \n",
        "  return class_labels\n",
        "\n",
        "def calculate_accuracy(feature_data, label_data, W1, b1, W2, b2, W3, b3):\n",
        "  \"\"\" \n",
        "  Return the accuracy value (applicable for both train and the test set) for the given set of weights and the biases \n",
        "  \"\"\"\n",
        "  # Calculate the matrix of predicted probabilities through calling of forward pass \n",
        "  predicted_matrix= forward_pass(feature_data, W1, b1, W2, b2, W3, b3)\n",
        "  # Get the class labels of the actual labels \n",
        "  actual_labels= return_labels(label_data)\n",
        "  # Get the class labels of the predicted probabilities \n",
        "  predicted_labels= return_labels(predicted_matrix)\n",
        "  # Get the correct prediction in the form of boolean array where 1 is correct prediction and 0 is the wrong prediction \n",
        "  correct_predictions= tf.cast(tf.equal(predicted_labels, actual_labels), tf.float32)\n",
        "  # Calculate the accuracy \n",
        "  accuracy= tf.reduce_mean(correct_predictions)\n",
        "\n",
        "  return accuracy \n",
        "\n",
        "# ------------------------------------------------BEGINNING OF OUR TENSORFLOW PROGRAM -----------------------------------------------\n",
        "\n",
        "# Loading the fashion MNIST data-set \n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "# Prepare the dataset \n",
        "train_features, train_labels, test_features, test_labels= prepare_dataset(fashion_mnist)\n",
        "\n",
        "# Get the transpose of feature data \n",
        "train_features= train_features.T \n",
        "test_features= test_features.T\n",
        "\n",
        "# Print the shape of our 4 data structures \n",
        "print( \"Shape of training features \", train_features.shape)\n",
        "print (\"Shape of training labels \", train_labels.shape)\n",
        "print()\n",
        "print( \"Shape of test features \", test_features.shape)\n",
        "print (\"Shape of test labels \", test_labels.shape)\n",
        "print()\n",
        "print(\"The training process of our Softmax Neural Network begins.....\")\n",
        "print()\n",
        "\n",
        "X_train= tf.cast(train_features, tf.float32)\n",
        "y_train= tf.cast(train_labels, tf.float32)\n",
        "X_test= tf.cast(test_features, tf.float32)\n",
        "y_test= tf.cast(test_labels, tf.float32)\n",
        "\n",
        "# Set the Number of features\n",
        "num_features=  X_train.shape[0]\n",
        "# We now specify the size of hidden layer 1\n",
        "hidden1_neurons= 300\n",
        "# We now specify the size of hidden layer 2\n",
        "hidden2_neurons= 100\n",
        "# We now specify the size of output layer \n",
        "output_neurons= 10 \n",
        "\n",
        "# Initialize the weight_matrix 1 and bias_matrix 1 \n",
        "# Each row of this matrix represents the 784 weights of a single neuron in the hidden layer \n",
        "W1= tf.Variable(tf.random.normal([hidden1_neurons, num_features], mean=0.0, stddev=0.05))\n",
        "# It is a column vector where each row/element represents the bias value for a single neuron in the hidden layer \n",
        "b1= tf.Variable(tf.random.normal([hidden1_neurons, 1], mean=0.0, stddev=0.05))\n",
        "\n",
        "# Initialize the weight_matrix 2 and bias_matrix 2 \n",
        "# Each row of this matrix represents the 300 weights of a single neuron in the softmax layer \n",
        "W2= tf.Variable(tf.random.normal([hidden2_neurons, hidden1_neurons], mean=0.0, stddev=0.05))\n",
        "# It is a column vector where each row/element represents the bias value for a single neuron in the softmax layer \n",
        "b2= tf.Variable(tf.random.normal([hidden2_neurons, 1], mean=0.0, stddev=0.05))\n",
        "\n",
        "# Initialize the weight_matrix 3 and bias_matrix 3\n",
        "# Each row of this matrix represents the 300 weights of a single neuron in the softmax layer \n",
        "W3= tf.Variable(tf.random.normal([output_neurons, hidden2_neurons], mean=0.0, stddev=0.05))\n",
        "# It is a column vector where each row/element represents the bias value for a single neuron in the softmax layer \n",
        "b3= tf.Variable(tf.random.normal([output_neurons, 1], mean=0.0, stddev=0.05))\n",
        "\n",
        "# Set the learning rate and the number of iterations \n",
        "learning_rate= 0.01\n",
        "num_iterations= 1200\n",
        "\n",
        "# Adam optimizer to update the weights of the neural network \n",
        "adam_optimizer= tf.keras.optimizers.Adam()\n",
        "\n",
        "# Create the list to store the training accuracy and loss with each iteration \n",
        "training_loss= []\n",
        "training_acc= []\n",
        "# Create the list to store the test accuracy and loss with each iteration \n",
        "test_loss= []\n",
        "test_acc= []\n",
        "\n",
        "# Run the gradient descent to num_iterations number of times \n",
        "for iteration in range(num_iterations):\n",
        "  \n",
        "  # Create an instance of GradientTape to monitor the forward pass and loss calculations\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Calculate the predicted probability matrix for the current weights and the biases for the training set\n",
        "    y_pred_matrix= forward_pass(X_train, W1, b1, W2, b2, W3, b3)\n",
        "    # Calculate the predicted probability matrix for the current weights and the biases for the test set\n",
        "    y_pred_test= forward_pass(X_test, W1, b1, W2, b2, W3, b3)\n",
        "    # Calculate the current training loss with the current predictions and the actual labels of the training set \n",
        "    current_loss_training= cross_entropy(y_train, y_pred_matrix, W1, W2, W3)\n",
        "    # Calculate the current test loss with the current prediction and the actual labels of the test set \n",
        "    current_loss_test= cross_entropy(y_test, y_pred_test, W1, W2, W3)\n",
        "  \n",
        "  # Calculate the gradients (partial derivates) of the loss with respect to each of the tunable weights \n",
        "  gradients= tape.gradient(current_loss_training, [W1, b1, W2, b2, W3, b3])\n",
        "\n",
        "  # Calculate the training accuracy with each iteration \n",
        "  training_accuracy= calculate_accuracy(X_train, y_train, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "  # Calculate the test accuracy with each each iteration \n",
        "  test_accuracy= calculate_accuracy(X_test, y_test, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "  # Print out the current iteration, current loss and current training accuracy \n",
        "  print(\"Iteration \",iteration, \": Loss = \",current_loss_training.numpy(),\" Acc: \", training_accuracy.numpy(),   'Val_loss = ',current_loss_test.numpy(),   'Val_acc = ', test_accuracy.numpy())\n",
        "\n",
        "  # Apply the Adam optimizer to update the weights and biases \n",
        "  adam_optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
        "\n",
        "  # Append the 4 values (train loss, train acc, test loss, test acc) with the each current iteration for the plotting \n",
        "  training_loss.append(current_loss_training.numpy())\n",
        "  training_acc.append(training_accuracy.numpy())\n",
        "  test_loss.append(current_loss_test.numpy())\n",
        "  test_acc.append(test_accuracy.numpy())\n",
        "\n",
        "# Calculate the test accuracy with the final updated weights and the biases through adam optimizer after running for certain number of iterations \n",
        "final_test_accuracy= calculate_accuracy(X_test, y_test, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "# Print the test accuracy \n",
        "print()\n",
        "print(\"The final test accuracy of the Fashion MNIST dataset is {}\".format(final_test_accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training features  (784, 60000)\n",
            "Shape of training labels  (10, 60000)\n",
            "\n",
            "Shape of test features  (784, 10000)\n",
            "Shape of test labels  (10, 10000)\n",
            "\n",
            "The training process of our Softmax Neural Network begins.....\n",
            "\n",
            "Iteration  0 : Loss =  3.3752136  Acc:  0.07186667 Val_loss =  3.374908 Val_acc =  0.0692\n",
            "Iteration  1 : Loss =  3.2523565  Acc:  0.34816667 Val_loss =  3.2527678 Val_acc =  0.3454\n",
            "Iteration  2 : Loss =  3.1521416  Acc:  0.40265 Val_loss =  3.152936 Val_acc =  0.3982\n",
            "Iteration  3 : Loss =  3.0534358  Acc:  0.44533333 Val_loss =  3.054955 Val_acc =  0.4434\n",
            "Iteration  4 : Loss =  2.9481277  Acc:  0.46561667 Val_loss =  2.9505143 Val_acc =  0.4625\n",
            "Iteration  5 : Loss =  2.835685  Acc:  0.49286667 Val_loss =  2.8391213 Val_acc =  0.4907\n",
            "Iteration  6 : Loss =  2.7221162  Acc:  0.5298667 Val_loss =  2.726476 Val_acc =  0.527\n",
            "Iteration  7 : Loss =  2.6088796  Acc:  0.5711167 Val_loss =  2.6139688 Val_acc =  0.5659\n",
            "Iteration  8 : Loss =  2.4966824  Acc:  0.61151665 Val_loss =  2.5024924 Val_acc =  0.6052\n",
            "Iteration  9 : Loss =  2.3875647  Acc:  0.6432667 Val_loss =  2.3940692 Val_acc =  0.6384\n",
            "Iteration  10 : Loss =  2.2844987  Acc:  0.65885 Val_loss =  2.2917764 Val_acc =  0.6533\n",
            "Iteration  11 : Loss =  2.1899261  Acc:  0.66573334 Val_loss =  2.1980243 Val_acc =  0.6578\n",
            "Iteration  12 : Loss =  2.1051452  Acc:  0.66971666 Val_loss =  2.1140618 Val_acc =  0.6624\n",
            "Iteration  13 : Loss =  2.0297978  Acc:  0.6734167 Val_loss =  2.0395253 Val_acc =  0.6667\n",
            "Iteration  14 : Loss =  1.9631586  Acc:  0.67553335 Val_loss =  1.9736625 Val_acc =  0.6668\n",
            "Iteration  15 : Loss =  1.9050622  Acc:  0.67775 Val_loss =  1.9163315 Val_acc =  0.6679\n",
            "Iteration  16 : Loss =  1.8544023  Acc:  0.68085 Val_loss =  1.8665127 Val_acc =  0.6699\n",
            "Iteration  17 : Loss =  1.8098034  Acc:  0.68556666 Val_loss =  1.8228765 Val_acc =  0.6742\n",
            "Iteration  18 : Loss =  1.77112  Acc:  0.6925833 Val_loss =  1.7851584 Val_acc =  0.6825\n",
            "Iteration  19 : Loss =  1.7374921  Acc:  0.7032 Val_loss =  1.752493 Val_acc =  0.6922\n",
            "Iteration  20 : Loss =  1.7076483  Acc:  0.71388334 Val_loss =  1.7234715 Val_acc =  0.6999\n",
            "Iteration  21 : Loss =  1.6804731  Acc:  0.7201333 Val_loss =  1.6968501 Val_acc =  0.7067\n",
            "Iteration  22 : Loss =  1.6556154  Acc:  0.72438335 Val_loss =  1.672456 Val_acc =  0.7114\n",
            "Iteration  23 : Loss =  1.6329921  Acc:  0.72746664 Val_loss =  1.6504946 Val_acc =  0.7152\n",
            "Iteration  24 : Loss =  1.6112571  Acc:  0.7337833 Val_loss =  1.6294901 Val_acc =  0.7202\n",
            "Iteration  25 : Loss =  1.5904651  Acc:  0.74155 Val_loss =  1.6096891 Val_acc =  0.7295\n",
            "Iteration  26 : Loss =  1.5707049  Acc:  0.74778336 Val_loss =  1.5904021 Val_acc =  0.7348\n",
            "Iteration  27 : Loss =  1.5518899  Acc:  0.75315 Val_loss =  1.57255 Val_acc =  0.7406\n",
            "Iteration  28 : Loss =  1.5341147  Acc:  0.7550333 Val_loss =  1.5542135 Val_acc =  0.744\n",
            "Iteration  29 : Loss =  1.5161774  Acc:  0.76246667 Val_loss =  1.537297 Val_acc =  0.7486\n",
            "Iteration  30 : Loss =  1.4981822  Acc:  0.7667 Val_loss =  1.5185679 Val_acc =  0.7548\n",
            "Iteration  31 : Loss =  1.4810289  Acc:  0.7715 Val_loss =  1.5013661 Val_acc =  0.7578\n",
            "Iteration  32 : Loss =  1.4646876  Acc:  0.7777 Val_loss =  1.486059 Val_acc =  0.7649\n",
            "Iteration  33 : Loss =  1.4488211  Acc:  0.7817 Val_loss =  1.469709 Val_acc =  0.7674\n",
            "Iteration  34 : Loss =  1.4339784  Acc:  0.78645 Val_loss =  1.455435 Val_acc =  0.7743\n",
            "Iteration  35 : Loss =  1.4195503  Acc:  0.79143333 Val_loss =  1.4412289 Val_acc =  0.7775\n",
            "Iteration  36 : Loss =  1.4048498  Acc:  0.79185 Val_loss =  1.4258335 Val_acc =  0.7807\n",
            "Iteration  37 : Loss =  1.39098  Acc:  0.7959167 Val_loss =  1.4124277 Val_acc =  0.7837\n",
            "Iteration  38 : Loss =  1.3783464  Acc:  0.79941666 Val_loss =  1.400101 Val_acc =  0.786\n",
            "Iteration  39 : Loss =  1.3653021  Acc:  0.79985 Val_loss =  1.3872356 Val_acc =  0.7907\n",
            "Iteration  40 : Loss =  1.3521221  Acc:  0.80446666 Val_loss =  1.3744173 Val_acc =  0.79\n",
            "Iteration  41 : Loss =  1.3399236  Acc:  0.80681664 Val_loss =  1.3626463 Val_acc =  0.7929\n",
            "Iteration  42 : Loss =  1.3283733  Acc:  0.8077667 Val_loss =  1.3508482 Val_acc =  0.7958\n",
            "Iteration  43 : Loss =  1.3165718  Acc:  0.8115 Val_loss =  1.3394337 Val_acc =  0.7982\n",
            "Iteration  44 : Loss =  1.3050444  Acc:  0.81255 Val_loss =  1.3282112 Val_acc =  0.8014\n",
            "Iteration  45 : Loss =  1.2943693  Acc:  0.81441665 Val_loss =  1.317292 Val_acc =  0.8038\n",
            "Iteration  46 : Loss =  1.2838081  Acc:  0.81673336 Val_loss =  1.3079145 Val_acc =  0.8052\n",
            "Iteration  47 : Loss =  1.2730384  Acc:  0.81796664 Val_loss =  1.2966619 Val_acc =  0.805\n",
            "Iteration  48 : Loss =  1.2624798  Acc:  0.8196167 Val_loss =  1.2870669 Val_acc =  0.8064\n",
            "Iteration  49 : Loss =  1.2525852  Acc:  0.82123333 Val_loss =  1.2772403 Val_acc =  0.8088\n",
            "Iteration  50 : Loss =  1.2427813  Acc:  0.82168335 Val_loss =  1.2678121 Val_acc =  0.8071\n",
            "Iteration  51 : Loss =  1.2328727  Acc:  0.82371664 Val_loss =  1.2580612 Val_acc =  0.8105\n",
            "Iteration  52 : Loss =  1.2232676  Acc:  0.82488334 Val_loss =  1.2492349 Val_acc =  0.8114\n",
            "Iteration  53 : Loss =  1.2143435  Acc:  0.8253833 Val_loss =  1.2397732 Val_acc =  0.8111\n",
            "Iteration  54 : Loss =  1.2056754  Acc:  0.82678336 Val_loss =  1.2324208 Val_acc =  0.8129\n",
            "Iteration  55 : Loss =  1.197199  Acc:  0.82698333 Val_loss =  1.2227967 Val_acc =  0.8139\n",
            "Iteration  56 : Loss =  1.1885095  Acc:  0.8282167 Val_loss =  1.2157059 Val_acc =  0.8156\n",
            "Iteration  57 : Loss =  1.1799786  Acc:  0.829 Val_loss =  1.2061996 Val_acc =  0.8146\n",
            "Iteration  58 : Loss =  1.1713612  Acc:  0.82948333 Val_loss =  1.1984892 Val_acc =  0.8158\n",
            "Iteration  59 : Loss =  1.1622298  Acc:  0.8308833 Val_loss =  1.189166 Val_acc =  0.818\n",
            "Iteration  60 : Loss =  1.1537458  Acc:  0.83211666 Val_loss =  1.1809167 Val_acc =  0.8179\n",
            "Iteration  61 : Loss =  1.1463286  Acc:  0.83211666 Val_loss =  1.1738758 Val_acc =  0.8192\n",
            "Iteration  62 : Loss =  1.1391759  Acc:  0.83278334 Val_loss =  1.1664289 Val_acc =  0.818\n",
            "Iteration  63 : Loss =  1.1316806  Acc:  0.83335 Val_loss =  1.15976 Val_acc =  0.8194\n",
            "Iteration  64 : Loss =  1.1239135  Acc:  0.8343167 Val_loss =  1.1512477 Val_acc =  0.821\n",
            "Iteration  65 : Loss =  1.1164504  Acc:  0.83526665 Val_loss =  1.1448257 Val_acc =  0.8208\n",
            "Iteration  66 : Loss =  1.1092069  Acc:  0.83531666 Val_loss =  1.136883 Val_acc =  0.8229\n",
            "Iteration  67 : Loss =  1.1017109  Acc:  0.83676666 Val_loss =  1.1300066 Val_acc =  0.8222\n",
            "Iteration  68 : Loss =  1.0944787  Acc:  0.83743334 Val_loss =  1.1227325 Val_acc =  0.8238\n",
            "Iteration  69 : Loss =  1.0878708  Acc:  0.83745 Val_loss =  1.1158214 Val_acc =  0.8246\n",
            "Iteration  70 : Loss =  1.0814893  Acc:  0.83776665 Val_loss =  1.1101899 Val_acc =  0.8241\n",
            "Iteration  71 : Loss =  1.0750127  Acc:  0.8380833 Val_loss =  1.1029639 Val_acc =  0.8246\n",
            "Iteration  72 : Loss =  1.0682223  Acc:  0.8390333 Val_loss =  1.0970923 Val_acc =  0.8237\n",
            "Iteration  73 : Loss =  1.0616888  Acc:  0.83895 Val_loss =  1.089923 Val_acc =  0.8258\n",
            "Iteration  74 : Loss =  1.0553837  Acc:  0.83986664 Val_loss =  1.0842272 Val_acc =  0.8259\n",
            "Iteration  75 : Loss =  1.0490376  Acc:  0.83961666 Val_loss =  1.0776062 Val_acc =  0.8261\n",
            "Iteration  76 : Loss =  1.042651  Acc:  0.8409167 Val_loss =  1.0714922 Val_acc =  0.8271\n",
            "Iteration  77 : Loss =  1.0363857  Acc:  0.8409167 Val_loss =  1.0652053 Val_acc =  0.8279\n",
            "Iteration  78 : Loss =  1.0304298  Acc:  0.8411 Val_loss =  1.0593889 Val_acc =  0.8283\n",
            "Iteration  79 : Loss =  1.0246761  Acc:  0.8423667 Val_loss =  1.0537508 Val_acc =  0.8282\n",
            "Iteration  80 : Loss =  1.018976  Acc:  0.84175 Val_loss =  1.0480283 Val_acc =  0.829\n",
            "Iteration  81 : Loss =  1.0132726  Acc:  0.8426167 Val_loss =  1.0425978 Val_acc =  0.8297\n",
            "Iteration  82 : Loss =  1.0076529  Acc:  0.84241664 Val_loss =  1.0367291 Val_acc =  0.8299\n",
            "Iteration  83 : Loss =  1.0023859  Acc:  0.84323335 Val_loss =  1.032086 Val_acc =  0.8302\n",
            "Iteration  84 : Loss =  0.99797213  Acc:  0.8422167 Val_loss =  1.0269532 Val_acc =  0.8294\n",
            "Iteration  85 : Loss =  0.9945029  Acc:  0.84141666 Val_loss =  1.024818 Val_acc =  0.8285\n",
            "Iteration  86 : Loss =  0.99335086  Acc:  0.8398 Val_loss =  1.0220155 Val_acc =  0.8263\n",
            "Iteration  87 : Loss =  0.9862072  Acc:  0.8408 Val_loss =  1.0167992 Val_acc =  0.8284\n",
            "Iteration  88 : Loss =  0.97797644  Acc:  0.8437 Val_loss =  1.0071416 Val_acc =  0.83\n",
            "Iteration  89 : Loss =  0.9713962  Acc:  0.84595 Val_loss =  1.0010756 Val_acc =  0.8321\n",
            "Iteration  90 : Loss =  0.9689435  Acc:  0.84346664 Val_loss =  0.9993112 Val_acc =  0.8316\n",
            "Iteration  91 : Loss =  0.96608865  Acc:  0.8430167 Val_loss =  0.99525845 Val_acc =  0.8295\n",
            "Iteration  92 : Loss =  0.95798683  Acc:  0.84595 Val_loss =  0.98824924 Val_acc =  0.8336\n",
            "Iteration  93 : Loss =  0.95340526  Acc:  0.84543335 Val_loss =  0.98358583 Val_acc =  0.833\n",
            "Iteration  94 : Loss =  0.951465  Acc:  0.84508336 Val_loss =  0.98108965 Val_acc =  0.8321\n",
            "Iteration  95 : Loss =  0.9452459  Acc:  0.84538335 Val_loss =  0.97582054 Val_acc =  0.8328\n",
            "Iteration  96 : Loss =  0.9393493  Acc:  0.84748334 Val_loss =  0.9694947 Val_acc =  0.8342\n",
            "Iteration  97 : Loss =  0.9364391  Acc:  0.84706664 Val_loss =  0.9664402 Val_acc =  0.8341\n",
            "Iteration  98 : Loss =  0.93279153  Acc:  0.8462 Val_loss =  0.96352357 Val_acc =  0.8336\n",
            "Iteration  99 : Loss =  0.92739195  Acc:  0.84793335 Val_loss =  0.95770884 Val_acc =  0.836\n",
            "Iteration  100 : Loss =  0.92283195  Acc:  0.84833336 Val_loss =  0.9532149 Val_acc =  0.8358\n",
            "Iteration  101 : Loss =  0.91987306  Acc:  0.8476833 Val_loss =  0.9507179 Val_acc =  0.8351\n",
            "Iteration  102 : Loss =  0.9161413  Acc:  0.8483 Val_loss =  0.94653285 Val_acc =  0.8354\n",
            "Iteration  103 : Loss =  0.9111403  Acc:  0.8487 Val_loss =  0.9419053 Val_acc =  0.8363\n",
            "Iteration  104 : Loss =  0.90741634  Acc:  0.84928334 Val_loss =  0.93833387 Val_acc =  0.8375\n",
            "Iteration  105 : Loss =  0.9044659  Acc:  0.8491333 Val_loss =  0.9349818 Val_acc =  0.8357\n",
            "Iteration  106 : Loss =  0.9002521  Acc:  0.84903336 Val_loss =  0.9313631 Val_acc =  0.8368\n",
            "Iteration  107 : Loss =  0.8958721  Acc:  0.85031664 Val_loss =  0.9267237 Val_acc =  0.8365\n",
            "Iteration  108 : Loss =  0.89241606  Acc:  0.8504 Val_loss =  0.92324007 Val_acc =  0.8367\n",
            "Iteration  109 : Loss =  0.889269  Acc:  0.8498167 Val_loss =  0.92052233 Val_acc =  0.8376\n",
            "Iteration  110 : Loss =  0.8856206  Acc:  0.85036665 Val_loss =  0.916505 Val_acc =  0.8361\n",
            "Iteration  111 : Loss =  0.88156235  Acc:  0.8509667 Val_loss =  0.912781 Val_acc =  0.8378\n",
            "Iteration  112 : Loss =  0.87807494  Acc:  0.85081667 Val_loss =  0.9093123 Val_acc =  0.8378\n",
            "Iteration  113 : Loss =  0.87502503  Acc:  0.85116667 Val_loss =  0.9061804 Val_acc =  0.8372\n",
            "Iteration  114 : Loss =  0.87168074  Acc:  0.85071665 Val_loss =  0.90313315 Val_acc =  0.8376\n",
            "Iteration  115 : Loss =  0.8680496  Acc:  0.85181665 Val_loss =  0.89935744 Val_acc =  0.8376\n",
            "Iteration  116 : Loss =  0.8645234  Acc:  0.85175 Val_loss =  0.8959367 Val_acc =  0.8389\n",
            "Iteration  117 : Loss =  0.86141324  Acc:  0.85248333 Val_loss =  0.8930635 Val_acc =  0.8398\n",
            "Iteration  118 : Loss =  0.8585597  Acc:  0.8519667 Val_loss =  0.88992 Val_acc =  0.8376\n",
            "Iteration  119 : Loss =  0.855737  Acc:  0.85275 Val_loss =  0.88759863 Val_acc =  0.8388\n",
            "Iteration  120 : Loss =  0.85294354  Acc:  0.8521 Val_loss =  0.88438153 Val_acc =  0.8382\n",
            "Iteration  121 : Loss =  0.85072243  Acc:  0.85258335 Val_loss =  0.8826852 Val_acc =  0.8381\n",
            "Iteration  122 : Loss =  0.848842  Acc:  0.85073334 Val_loss =  0.88047516 Val_acc =  0.8377\n",
            "Iteration  123 : Loss =  0.84697104  Acc:  0.8511 Val_loss =  0.8790031 Val_acc =  0.8382\n",
            "Iteration  124 : Loss =  0.842954  Acc:  0.8505833 Val_loss =  0.87475675 Val_acc =  0.8375\n",
            "Iteration  125 : Loss =  0.83750325  Acc:  0.8534 Val_loss =  0.8694566 Val_acc =  0.8394\n",
            "Iteration  126 : Loss =  0.83350134  Acc:  0.8542167 Val_loss =  0.8654739 Val_acc =  0.8401\n",
            "Iteration  127 : Loss =  0.83193123  Acc:  0.8533667 Val_loss =  0.8638587 Val_acc =  0.8398\n",
            "Iteration  128 : Loss =  0.8306174  Acc:  0.85295 Val_loss =  0.8628502 Val_acc =  0.8388\n",
            "Iteration  129 : Loss =  0.82720506  Acc:  0.85295 Val_loss =  0.85926735 Val_acc =  0.8391\n",
            "Iteration  130 : Loss =  0.8227786  Acc:  0.8549167 Val_loss =  0.85503703 Val_acc =  0.8391\n",
            "Iteration  131 : Loss =  0.8197081  Acc:  0.8549333 Val_loss =  0.8519622 Val_acc =  0.8397\n",
            "Iteration  132 : Loss =  0.81811273  Acc:  0.85405 Val_loss =  0.8503876 Val_acc =  0.841\n",
            "Iteration  133 : Loss =  0.8161068  Acc:  0.85441667 Val_loss =  0.8485639 Val_acc =  0.8389\n",
            "Iteration  134 : Loss =  0.8126888  Acc:  0.8545667 Val_loss =  0.84503484 Val_acc =  0.8413\n",
            "Iteration  135 : Loss =  0.8091533  Acc:  0.85565 Val_loss =  0.8416438 Val_acc =  0.8411\n",
            "Iteration  136 : Loss =  0.8067217  Acc:  0.85611665 Val_loss =  0.83926296 Val_acc =  0.8406\n",
            "Iteration  137 : Loss =  0.80494064  Acc:  0.85496664 Val_loss =  0.83745885 Val_acc =  0.8419\n",
            "Iteration  138 : Loss =  0.80266035  Acc:  0.8559 Val_loss =  0.835327 Val_acc =  0.8401\n",
            "Iteration  139 : Loss =  0.7995858  Acc:  0.8557 Val_loss =  0.8322127 Val_acc =  0.8424\n",
            "Iteration  140 : Loss =  0.79657036  Acc:  0.8563 Val_loss =  0.8292439 Val_acc =  0.8416\n",
            "Iteration  141 : Loss =  0.79422426  Acc:  0.85641664 Val_loss =  0.82702696 Val_acc =  0.8423\n",
            "Iteration  142 : Loss =  0.79227257  Acc:  0.8563667 Val_loss =  0.82496136 Val_acc =  0.8431\n",
            "Iteration  143 : Loss =  0.7901441  Acc:  0.85705 Val_loss =  0.82307744 Val_acc =  0.8413\n",
            "Iteration  144 : Loss =  0.7875236  Acc:  0.85656667 Val_loss =  0.82028466 Val_acc =  0.8431\n",
            "Iteration  145 : Loss =  0.78482723  Acc:  0.8574 Val_loss =  0.8178383 Val_acc =  0.8425\n",
            "Iteration  146 : Loss =  0.7824558  Acc:  0.8573 Val_loss =  0.8153355 Val_acc =  0.8423\n",
            "Iteration  147 : Loss =  0.7805835  Acc:  0.8568 Val_loss =  0.813672 Val_acc =  0.8425\n",
            "Iteration  148 : Loss =  0.7792666  Acc:  0.85648334 Val_loss =  0.81221724 Val_acc =  0.8406\n",
            "Iteration  149 : Loss =  0.77807945  Acc:  0.8556833 Val_loss =  0.81136334 Val_acc =  0.8414\n",
            "Iteration  150 : Loss =  0.7780297  Acc:  0.85566664 Val_loss =  0.8109518 Val_acc =  0.8407\n",
            "Iteration  151 : Loss =  0.776177  Acc:  0.85475 Val_loss =  0.8097073 Val_acc =  0.8394\n",
            "Iteration  152 : Loss =  0.77539694  Acc:  0.8552 Val_loss =  0.8082135 Val_acc =  0.8421\n",
            "Iteration  153 : Loss =  0.77063906  Acc:  0.85718334 Val_loss =  0.8040704 Val_acc =  0.8415\n",
            "Iteration  154 : Loss =  0.7664465  Acc:  0.8571333 Val_loss =  0.79940146 Val_acc =  0.843\n",
            "Iteration  155 : Loss =  0.7640419  Acc:  0.85855 Val_loss =  0.797228 Val_acc =  0.8424\n",
            "Iteration  156 : Loss =  0.7629101  Acc:  0.8570333 Val_loss =  0.7963816 Val_acc =  0.843\n",
            "Iteration  157 : Loss =  0.7622149  Acc:  0.8571333 Val_loss =  0.79530776 Val_acc =  0.8425\n",
            "Iteration  158 : Loss =  0.75918376  Acc:  0.85785 Val_loss =  0.7927697 Val_acc =  0.8431\n",
            "Iteration  159 : Loss =  0.7559811  Acc:  0.8580833 Val_loss =  0.7892008 Val_acc =  0.8442\n",
            "Iteration  160 : Loss =  0.75376  Acc:  0.85938334 Val_loss =  0.78717864 Val_acc =  0.8431\n",
            "Iteration  161 : Loss =  0.752404  Acc:  0.85856664 Val_loss =  0.7859932 Val_acc =  0.8441\n",
            "Iteration  162 : Loss =  0.7511412  Acc:  0.85873336 Val_loss =  0.7845066 Val_acc =  0.8437\n",
            "Iteration  163 : Loss =  0.7484702  Acc:  0.85925 Val_loss =  0.7821739 Val_acc =  0.8443\n",
            "Iteration  164 : Loss =  0.7459121  Acc:  0.85915 Val_loss =  0.7793797 Val_acc =  0.8455\n",
            "Iteration  165 : Loss =  0.7440976  Acc:  0.86006665 Val_loss =  0.77772963 Val_acc =  0.8438\n",
            "Iteration  166 : Loss =  0.74275947  Acc:  0.8592333 Val_loss =  0.77648294 Val_acc =  0.8449\n",
            "Iteration  167 : Loss =  0.7412678  Acc:  0.8598 Val_loss =  0.77485555 Val_acc =  0.8432\n",
            "Iteration  168 : Loss =  0.7388025  Acc:  0.86016667 Val_loss =  0.7726315 Val_acc =  0.8449\n",
            "Iteration  169 : Loss =  0.73643297  Acc:  0.86051667 Val_loss =  0.7700753 Val_acc =  0.8463\n",
            "Iteration  170 : Loss =  0.73461163  Acc:  0.86075 Val_loss =  0.7683801 Val_acc =  0.8448\n",
            "Iteration  171 : Loss =  0.7333069  Acc:  0.86045 Val_loss =  0.7671335 Val_acc =  0.8457\n",
            "Iteration  172 : Loss =  0.7319927  Acc:  0.86065 Val_loss =  0.7657707 Val_acc =  0.8443\n",
            "Iteration  173 : Loss =  0.7300508  Acc:  0.8603 Val_loss =  0.76398957 Val_acc =  0.8462\n",
            "Iteration  174 : Loss =  0.7278608  Acc:  0.8612667 Val_loss =  0.76171064 Val_acc =  0.8449\n",
            "Iteration  175 : Loss =  0.7256946  Acc:  0.8612 Val_loss =  0.7596185 Val_acc =  0.8464\n",
            "Iteration  176 : Loss =  0.72395843  Acc:  0.86156666 Val_loss =  0.75786996 Val_acc =  0.847\n",
            "Iteration  177 : Loss =  0.72257173  Acc:  0.86191666 Val_loss =  0.7565086 Val_acc =  0.8458\n",
            "Iteration  178 : Loss =  0.721243  Acc:  0.8614 Val_loss =  0.7552937 Val_acc =  0.8467\n",
            "Iteration  179 : Loss =  0.7198119  Acc:  0.86196667 Val_loss =  0.7538294 Val_acc =  0.8448\n",
            "Iteration  180 : Loss =  0.7180749  Acc:  0.8613167 Val_loss =  0.752204 Val_acc =  0.847\n",
            "Iteration  181 : Loss =  0.71622884  Acc:  0.86253333 Val_loss =  0.75032735 Val_acc =  0.8463\n",
            "Iteration  182 : Loss =  0.7143862  Acc:  0.8620167 Val_loss =  0.7485126 Val_acc =  0.8487\n",
            "Iteration  183 : Loss =  0.71267605  Acc:  0.86303335 Val_loss =  0.74688876 Val_acc =  0.8463\n",
            "Iteration  184 : Loss =  0.71112186  Acc:  0.8624167 Val_loss =  0.74527895 Val_acc =  0.8479\n",
            "Iteration  185 : Loss =  0.7097138  Acc:  0.8629 Val_loss =  0.7440289 Val_acc =  0.8461\n",
            "Iteration  186 : Loss =  0.7083628  Acc:  0.86275 Val_loss =  0.7425504 Val_acc =  0.8485\n",
            "Iteration  187 : Loss =  0.7070292  Acc:  0.86301666 Val_loss =  0.7414223 Val_acc =  0.8459\n",
            "Iteration  188 : Loss =  0.7056731  Acc:  0.8628333 Val_loss =  0.73991203 Val_acc =  0.848\n",
            "Iteration  189 : Loss =  0.7042719  Acc:  0.86305 Val_loss =  0.7387421 Val_acc =  0.8459\n",
            "Iteration  190 : Loss =  0.7027856  Acc:  0.86266667 Val_loss =  0.7370719 Val_acc =  0.8483\n",
            "Iteration  191 : Loss =  0.7013798  Acc:  0.8635 Val_loss =  0.7359017 Val_acc =  0.8466\n",
            "Iteration  192 : Loss =  0.6999389  Acc:  0.8629 Val_loss =  0.7342949 Val_acc =  0.8491\n",
            "Iteration  193 : Loss =  0.6987866  Acc:  0.8633 Val_loss =  0.7333435 Val_acc =  0.8471\n",
            "Iteration  194 : Loss =  0.6977894  Acc:  0.8621 Val_loss =  0.7322794 Val_acc =  0.8478\n",
            "Iteration  195 : Loss =  0.6971412  Acc:  0.8628333 Val_loss =  0.73174006 Val_acc =  0.848\n",
            "Iteration  196 : Loss =  0.6964867  Acc:  0.8620167 Val_loss =  0.7311317 Val_acc =  0.8477\n",
            "Iteration  197 : Loss =  0.6955658  Acc:  0.8628167 Val_loss =  0.73019946 Val_acc =  0.847\n",
            "Iteration  198 : Loss =  0.69319904  Acc:  0.86263335 Val_loss =  0.72795236 Val_acc =  0.8478\n",
            "Iteration  199 : Loss =  0.6906067  Acc:  0.86365 Val_loss =  0.7252139 Val_acc =  0.8479\n",
            "Iteration  200 : Loss =  0.6878283  Acc:  0.8645833 Val_loss =  0.7226132 Val_acc =  0.8476\n",
            "Iteration  201 : Loss =  0.6864121  Acc:  0.86445 Val_loss =  0.7210617 Val_acc =  0.8499\n",
            "Iteration  202 : Loss =  0.68600523  Acc:  0.86448336 Val_loss =  0.7208797 Val_acc =  0.8481\n",
            "Iteration  203 : Loss =  0.6855254  Acc:  0.8635333 Val_loss =  0.7203499 Val_acc =  0.8496\n",
            "Iteration  204 : Loss =  0.6842115  Acc:  0.8641 Val_loss =  0.71914303 Val_acc =  0.8484\n",
            "Iteration  205 : Loss =  0.68186545  Acc:  0.86465 Val_loss =  0.7168336 Val_acc =  0.8507\n",
            "Iteration  206 : Loss =  0.6794512  Acc:  0.86525 Val_loss =  0.7143168 Val_acc =  0.8499\n",
            "Iteration  207 : Loss =  0.67778885  Acc:  0.8659 Val_loss =  0.71278715 Val_acc =  0.8497\n",
            "Iteration  208 : Loss =  0.67707145  Acc:  0.86516666 Val_loss =  0.7119953 Val_acc =  0.8511\n",
            "Iteration  209 : Loss =  0.6767378  Acc:  0.8653167 Val_loss =  0.7118511 Val_acc =  0.8497\n",
            "Iteration  210 : Loss =  0.6757637  Acc:  0.86485 Val_loss =  0.71077216 Val_acc =  0.8504\n",
            "Iteration  211 : Loss =  0.6740725  Acc:  0.86565 Val_loss =  0.7092935 Val_acc =  0.85\n",
            "Iteration  212 : Loss =  0.6717745  Acc:  0.86576664 Val_loss =  0.70683634 Val_acc =  0.8518\n",
            "Iteration  213 : Loss =  0.6698232  Acc:  0.8664167 Val_loss =  0.70502305 Val_acc =  0.8515\n",
            "Iteration  214 : Loss =  0.66856134  Acc:  0.8663667 Val_loss =  0.7037587 Val_acc =  0.8511\n",
            "Iteration  215 : Loss =  0.6678246  Acc:  0.86651665 Val_loss =  0.7030527 Val_acc =  0.8527\n",
            "Iteration  216 : Loss =  0.66715884  Acc:  0.86626667 Val_loss =  0.70250916 Val_acc =  0.8508\n",
            "Iteration  217 : Loss =  0.66613483  Acc:  0.8659833 Val_loss =  0.7014345 Val_acc =  0.8526\n",
            "Iteration  218 : Loss =  0.664659  Acc:  0.86665 Val_loss =  0.7000811 Val_acc =  0.8511\n",
            "Iteration  219 : Loss =  0.6629032  Acc:  0.86685 Val_loss =  0.6983109 Val_acc =  0.8532\n",
            "Iteration  220 : Loss =  0.661227  Acc:  0.8674333 Val_loss =  0.6966653 Val_acc =  0.8517\n",
            "Iteration  221 : Loss =  0.65984553  Acc:  0.8675333 Val_loss =  0.6953567 Val_acc =  0.8524\n",
            "Iteration  222 : Loss =  0.6587671  Acc:  0.86791664 Val_loss =  0.69425917 Val_acc =  0.8533\n",
            "Iteration  223 : Loss =  0.6579038  Acc:  0.86825 Val_loss =  0.6934949 Val_acc =  0.8516\n",
            "Iteration  224 : Loss =  0.65711606  Acc:  0.86768335 Val_loss =  0.69268286 Val_acc =  0.8535\n",
            "Iteration  225 : Loss =  0.65639424  Acc:  0.86795 Val_loss =  0.6921129 Val_acc =  0.8507\n",
            "Iteration  226 : Loss =  0.65560603  Acc:  0.8673 Val_loss =  0.6912135 Val_acc =  0.8543\n",
            "Iteration  227 : Loss =  0.65508854  Acc:  0.8678333 Val_loss =  0.6909299 Val_acc =  0.8497\n",
            "Iteration  228 : Loss =  0.6549864  Acc:  0.86653334 Val_loss =  0.6906491 Val_acc =  0.8528\n",
            "Iteration  229 : Loss =  0.65532434  Acc:  0.8670833 Val_loss =  0.69130504 Val_acc =  0.8487\n",
            "Iteration  230 : Loss =  0.6576035  Acc:  0.8649167 Val_loss =  0.6934136 Val_acc =  0.8502\n",
            "Iteration  231 : Loss =  0.65558076  Acc:  0.86516666 Val_loss =  0.6916292 Val_acc =  0.8489\n",
            "Iteration  232 : Loss =  0.65426666  Acc:  0.8660167 Val_loss =  0.690099 Val_acc =  0.8509\n",
            "Iteration  233 : Loss =  0.64901435  Acc:  0.8678167 Val_loss =  0.6848548 Val_acc =  0.853\n",
            "Iteration  234 : Loss =  0.64700526  Acc:  0.86936665 Val_loss =  0.6828416 Val_acc =  0.8518\n",
            "Iteration  235 : Loss =  0.64778227  Acc:  0.8671833 Val_loss =  0.68361646 Val_acc =  0.8541\n",
            "Iteration  236 : Loss =  0.64754355  Acc:  0.8679 Val_loss =  0.68371004 Val_acc =  0.8498\n",
            "Iteration  237 : Loss =  0.64569974  Acc:  0.8678167 Val_loss =  0.6816174 Val_acc =  0.8534\n",
            "Iteration  238 : Loss =  0.6421706  Acc:  0.8689833 Val_loss =  0.67829466 Val_acc =  0.8541\n",
            "Iteration  239 : Loss =  0.6412935  Acc:  0.86986667 Val_loss =  0.6773501 Val_acc =  0.8519\n",
            "Iteration  240 : Loss =  0.64200693  Acc:  0.8682 Val_loss =  0.67800725 Val_acc =  0.8545\n",
            "Iteration  241 : Loss =  0.64150333  Acc:  0.86873335 Val_loss =  0.6777902 Val_acc =  0.8515\n",
            "Iteration  242 : Loss =  0.6387679  Acc:  0.8691667 Val_loss =  0.6748092 Val_acc =  0.8552\n",
            "Iteration  243 : Loss =  0.63606346  Acc:  0.8696667 Val_loss =  0.67237335 Val_acc =  0.8547\n",
            "Iteration  244 : Loss =  0.6356064  Acc:  0.87021667 Val_loss =  0.6719637 Val_acc =  0.8541\n",
            "Iteration  245 : Loss =  0.636163  Acc:  0.86905 Val_loss =  0.67239845 Val_acc =  0.8549\n",
            "Iteration  246 : Loss =  0.63550097  Acc:  0.87005 Val_loss =  0.6719642 Val_acc =  0.8518\n",
            "Iteration  247 : Loss =  0.6331679  Acc:  0.86941665 Val_loss =  0.6694492 Val_acc =  0.8562\n",
            "Iteration  248 : Loss =  0.63121164  Acc:  0.87081665 Val_loss =  0.66763157 Val_acc =  0.8546\n",
            "Iteration  249 : Loss =  0.63060737  Acc:  0.8703833 Val_loss =  0.6671699 Val_acc =  0.8547\n",
            "Iteration  250 : Loss =  0.6305531  Acc:  0.8704 Val_loss =  0.6670201 Val_acc =  0.857\n",
            "Iteration  251 : Loss =  0.62950206  Acc:  0.87035 Val_loss =  0.66614234 Val_acc =  0.8547\n",
            "Iteration  252 : Loss =  0.62779045  Acc:  0.87098336 Val_loss =  0.66436535 Val_acc =  0.8555\n",
            "Iteration  253 : Loss =  0.62632835  Acc:  0.87095 Val_loss =  0.66291696 Val_acc =  0.8554\n",
            "Iteration  254 : Loss =  0.62565565  Acc:  0.8707 Val_loss =  0.6624321 Val_acc =  0.8563\n",
            "Iteration  255 : Loss =  0.6253703  Acc:  0.8711333 Val_loss =  0.66205335 Val_acc =  0.8568\n",
            "Iteration  256 : Loss =  0.62456274  Acc:  0.8712 Val_loss =  0.66136694 Val_acc =  0.8545\n",
            "Iteration  257 : Loss =  0.62341404  Acc:  0.87168336 Val_loss =  0.66024816 Val_acc =  0.8562\n",
            "Iteration  258 : Loss =  0.62209785  Acc:  0.87123334 Val_loss =  0.6588725 Val_acc =  0.8572\n",
            "Iteration  259 : Loss =  0.6212186  Acc:  0.8721833 Val_loss =  0.65817606 Val_acc =  0.8556\n",
            "Iteration  260 : Loss =  0.6207349  Acc:  0.871 Val_loss =  0.65760577 Val_acc =  0.8565\n",
            "Iteration  261 : Loss =  0.62038666  Acc:  0.87233335 Val_loss =  0.6573493 Val_acc =  0.8546\n",
            "Iteration  262 : Loss =  0.6197526  Acc:  0.8710833 Val_loss =  0.6567965 Val_acc =  0.8557\n",
            "Iteration  263 : Loss =  0.6191085  Acc:  0.8724 Val_loss =  0.6560751 Val_acc =  0.8552\n",
            "Iteration  264 : Loss =  0.61843836  Acc:  0.87121665 Val_loss =  0.65566224 Val_acc =  0.8565\n",
            "Iteration  265 : Loss =  0.61810803  Acc:  0.87205 Val_loss =  0.6552247 Val_acc =  0.8546\n",
            "Iteration  266 : Loss =  0.6178073  Acc:  0.87118334 Val_loss =  0.6550814 Val_acc =  0.8566\n",
            "Iteration  267 : Loss =  0.61744016  Acc:  0.87123334 Val_loss =  0.65480244 Val_acc =  0.855\n",
            "Iteration  268 : Loss =  0.6157873  Acc:  0.8715 Val_loss =  0.65300167 Val_acc =  0.8573\n",
            "Iteration  269 : Loss =  0.6138216  Acc:  0.87275 Val_loss =  0.6512944 Val_acc =  0.856\n",
            "Iteration  270 : Loss =  0.61181617  Acc:  0.872 Val_loss =  0.6490957 Val_acc =  0.8577\n",
            "Iteration  271 : Loss =  0.61052287  Acc:  0.87345 Val_loss =  0.64794147 Val_acc =  0.8566\n",
            "Iteration  272 : Loss =  0.60989  Acc:  0.8731833 Val_loss =  0.6474003 Val_acc =  0.8573\n",
            "Iteration  273 : Loss =  0.6094709  Acc:  0.8729333 Val_loss =  0.64687324 Val_acc =  0.8573\n",
            "Iteration  274 : Loss =  0.60907596  Acc:  0.87303334 Val_loss =  0.64677435 Val_acc =  0.8572\n",
            "Iteration  275 : Loss =  0.6083707  Acc:  0.8724833 Val_loss =  0.64589447 Val_acc =  0.8573\n",
            "Iteration  276 : Loss =  0.60777557  Acc:  0.87333333 Val_loss =  0.64545524 Val_acc =  0.8563\n",
            "Iteration  277 : Loss =  0.60732085  Acc:  0.87273335 Val_loss =  0.64508086 Val_acc =  0.8572\n",
            "Iteration  278 : Loss =  0.6069597  Acc:  0.87366664 Val_loss =  0.6445979 Val_acc =  0.857\n",
            "Iteration  279 : Loss =  0.6061708  Acc:  0.8728667 Val_loss =  0.6441155 Val_acc =  0.8572\n",
            "Iteration  280 : Loss =  0.6049968  Acc:  0.8739333 Val_loss =  0.64270604 Val_acc =  0.8566\n",
            "Iteration  281 : Loss =  0.6032458  Acc:  0.87333333 Val_loss =  0.641209 Val_acc =  0.8575\n",
            "Iteration  282 : Loss =  0.6016767  Acc:  0.8742833 Val_loss =  0.6395842 Val_acc =  0.857\n",
            "Iteration  283 : Loss =  0.6005746  Acc:  0.87383336 Val_loss =  0.6384767 Val_acc =  0.8576\n",
            "Iteration  284 : Loss =  0.59996045  Acc:  0.8739167 Val_loss =  0.6380839 Val_acc =  0.8575\n",
            "Iteration  285 : Loss =  0.59962654  Acc:  0.8743333 Val_loss =  0.63756645 Val_acc =  0.8574\n",
            "Iteration  286 : Loss =  0.59931827  Acc:  0.87368333 Val_loss =  0.6375769 Val_acc =  0.858\n",
            "Iteration  287 : Loss =  0.599025  Acc:  0.8748 Val_loss =  0.6371343 Val_acc =  0.8566\n",
            "Iteration  288 : Loss =  0.5985107  Acc:  0.8737 Val_loss =  0.636794 Val_acc =  0.858\n",
            "Iteration  289 : Loss =  0.597944  Acc:  0.8747 Val_loss =  0.63627017 Val_acc =  0.8573\n",
            "Iteration  290 : Loss =  0.5972041  Acc:  0.8734 Val_loss =  0.63547647 Val_acc =  0.8572\n",
            "Iteration  291 : Loss =  0.596437  Acc:  0.87475 Val_loss =  0.63498497 Val_acc =  0.8566\n",
            "Iteration  292 : Loss =  0.5955841  Acc:  0.8739167 Val_loss =  0.6339324 Val_acc =  0.858\n",
            "Iteration  293 : Loss =  0.594892  Acc:  0.8745667 Val_loss =  0.6335387 Val_acc =  0.8588\n",
            "Iteration  294 : Loss =  0.59393406  Acc:  0.87481666 Val_loss =  0.6324632 Val_acc =  0.859\n",
            "Iteration  295 : Loss =  0.5932346  Acc:  0.87445 Val_loss =  0.63184446 Val_acc =  0.8595\n",
            "Iteration  296 : Loss =  0.592092  Acc:  0.8749833 Val_loss =  0.63080776 Val_acc =  0.8591\n",
            "Iteration  297 : Loss =  0.5910728  Acc:  0.87515 Val_loss =  0.629629 Val_acc =  0.86\n",
            "Iteration  298 : Loss =  0.58990926  Acc:  0.8753 Val_loss =  0.6287549 Val_acc =  0.8589\n",
            "Iteration  299 : Loss =  0.58886343  Acc:  0.8752 Val_loss =  0.62748104 Val_acc =  0.8589\n",
            "Iteration  300 : Loss =  0.58802414  Acc:  0.8757333 Val_loss =  0.6269091 Val_acc =  0.8592\n",
            "Iteration  301 : Loss =  0.5874347  Acc:  0.87555 Val_loss =  0.6262386 Val_acc =  0.8595\n",
            "Iteration  302 : Loss =  0.5871437  Acc:  0.87581664 Val_loss =  0.6260501 Val_acc =  0.8592\n",
            "Iteration  303 : Loss =  0.58712006  Acc:  0.87551665 Val_loss =  0.6261431 Val_acc =  0.8592\n",
            "Iteration  304 : Loss =  0.5875263  Acc:  0.87551665 Val_loss =  0.6264809 Val_acc =  0.8596\n",
            "Iteration  305 : Loss =  0.5877522  Acc:  0.8753167 Val_loss =  0.62696624 Val_acc =  0.8591\n",
            "Iteration  306 : Loss =  0.58823913  Acc:  0.8749167 Val_loss =  0.627287 Val_acc =  0.8587\n",
            "Iteration  307 : Loss =  0.5874303  Acc:  0.87551665 Val_loss =  0.62675524 Val_acc =  0.8591\n",
            "Iteration  308 : Loss =  0.58612895  Acc:  0.87548333 Val_loss =  0.6253058 Val_acc =  0.8584\n",
            "Iteration  309 : Loss =  0.5840346  Acc:  0.87575 Val_loss =  0.6232722 Val_acc =  0.8602\n",
            "Iteration  310 : Loss =  0.582293  Acc:  0.8767667 Val_loss =  0.62164366 Val_acc =  0.8585\n",
            "Iteration  311 : Loss =  0.5814055  Acc:  0.87658334 Val_loss =  0.62059695 Val_acc =  0.8615\n",
            "Iteration  312 : Loss =  0.58133906  Acc:  0.8760667 Val_loss =  0.62088287 Val_acc =  0.8594\n",
            "Iteration  313 : Loss =  0.5816142  Acc:  0.8756 Val_loss =  0.62099814 Val_acc =  0.8604\n",
            "Iteration  314 : Loss =  0.5809163  Acc:  0.87615 Val_loss =  0.6204591 Val_acc =  0.8597\n",
            "Iteration  315 : Loss =  0.58016276  Acc:  0.8761 Val_loss =  0.6197845 Val_acc =  0.8601\n",
            "Iteration  316 : Loss =  0.5784847  Acc:  0.87685 Val_loss =  0.61781025 Val_acc =  0.8606\n",
            "Iteration  317 : Loss =  0.57722783  Acc:  0.8771667 Val_loss =  0.6170517 Val_acc =  0.8608\n",
            "Iteration  318 : Loss =  0.57642764  Acc:  0.8768 Val_loss =  0.61571467 Val_acc =  0.8614\n",
            "Iteration  319 : Loss =  0.5760582  Acc:  0.87701666 Val_loss =  0.61597353 Val_acc =  0.8603\n",
            "Iteration  320 : Loss =  0.5758351  Acc:  0.87666667 Val_loss =  0.6154681 Val_acc =  0.8617\n",
            "Iteration  321 : Loss =  0.5756148  Acc:  0.87776667 Val_loss =  0.6153337 Val_acc =  0.8603\n",
            "Iteration  322 : Loss =  0.5754475  Acc:  0.87686664 Val_loss =  0.6155305 Val_acc =  0.8612\n",
            "Iteration  323 : Loss =  0.5750838  Acc:  0.8778833 Val_loss =  0.61451346 Val_acc =  0.8604\n",
            "Iteration  324 : Loss =  0.57428634  Acc:  0.8768833 Val_loss =  0.6146374 Val_acc =  0.8608\n",
            "Iteration  325 : Loss =  0.57341874  Acc:  0.87765 Val_loss =  0.6128855 Val_acc =  0.8609\n",
            "Iteration  326 : Loss =  0.5722666  Acc:  0.8779333 Val_loss =  0.6126037 Val_acc =  0.8612\n",
            "Iteration  327 : Loss =  0.5716368  Acc:  0.87833333 Val_loss =  0.61159927 Val_acc =  0.8612\n",
            "Iteration  328 : Loss =  0.57157207  Acc:  0.8774667 Val_loss =  0.61161524 Val_acc =  0.8624\n",
            "Iteration  329 : Loss =  0.5718026  Acc:  0.8776 Val_loss =  0.61230767 Val_acc =  0.8597\n",
            "Iteration  330 : Loss =  0.57131296  Acc:  0.8772 Val_loss =  0.6112568 Val_acc =  0.8625\n",
            "Iteration  331 : Loss =  0.57007706  Acc:  0.8778667 Val_loss =  0.61068445 Val_acc =  0.8594\n",
            "Iteration  332 : Loss =  0.56823444  Acc:  0.8779167 Val_loss =  0.60847247 Val_acc =  0.8641\n",
            "Iteration  333 : Loss =  0.5668203  Acc:  0.87895 Val_loss =  0.60712403 Val_acc =  0.8631\n",
            "Iteration  334 : Loss =  0.56614065  Acc:  0.8790333 Val_loss =  0.60682577 Val_acc =  0.8627\n",
            "Iteration  335 : Loss =  0.5658982  Acc:  0.8789 Val_loss =  0.60600954 Val_acc =  0.8634\n",
            "Iteration  336 : Loss =  0.56558496  Acc:  0.87845 Val_loss =  0.6064791 Val_acc =  0.8622\n",
            "Iteration  337 : Loss =  0.56498575  Acc:  0.87901664 Val_loss =  0.6053076 Val_acc =  0.8621\n",
            "Iteration  338 : Loss =  0.56444645  Acc:  0.8786333 Val_loss =  0.60516214 Val_acc =  0.8622\n",
            "Iteration  339 : Loss =  0.5638473  Acc:  0.8792 Val_loss =  0.6046058 Val_acc =  0.8618\n",
            "Iteration  340 : Loss =  0.56342137  Acc:  0.87905 Val_loss =  0.6039408 Val_acc =  0.8625\n",
            "Iteration  341 : Loss =  0.5627254  Acc:  0.8787 Val_loss =  0.6037799 Val_acc =  0.8623\n",
            "Iteration  342 : Loss =  0.56184614  Acc:  0.8789833 Val_loss =  0.60242075 Val_acc =  0.8633\n",
            "Iteration  343 : Loss =  0.56089693  Acc:  0.8797 Val_loss =  0.60196227 Val_acc =  0.8642\n",
            "Iteration  344 : Loss =  0.5602318  Acc:  0.87976664 Val_loss =  0.6011069 Val_acc =  0.8641\n",
            "Iteration  345 : Loss =  0.5600071  Acc:  0.8793667 Val_loss =  0.6009534 Val_acc =  0.8638\n",
            "Iteration  346 : Loss =  0.56029576  Acc:  0.87911665 Val_loss =  0.6014348 Val_acc =  0.8626\n",
            "Iteration  347 : Loss =  0.5609354  Acc:  0.87871665 Val_loss =  0.60197765 Val_acc =  0.8633\n",
            "Iteration  348 : Loss =  0.5623691  Acc:  0.8785167 Val_loss =  0.6035266 Val_acc =  0.861\n",
            "Iteration  349 : Loss =  0.5634639  Acc:  0.8764 Val_loss =  0.60492 Val_acc =  0.8605\n",
            "Iteration  350 : Loss =  0.5652211  Acc:  0.87763333 Val_loss =  0.606171 Val_acc =  0.8591\n",
            "Iteration  351 : Loss =  0.5630103  Acc:  0.87651664 Val_loss =  0.6047689 Val_acc =  0.8601\n",
            "Iteration  352 : Loss =  0.55961794  Acc:  0.8796667 Val_loss =  0.6004919 Val_acc =  0.861\n",
            "Iteration  353 : Loss =  0.55578095  Acc:  0.88021666 Val_loss =  0.59737515 Val_acc =  0.8641\n",
            "Iteration  354 : Loss =  0.55512726  Acc:  0.8799 Val_loss =  0.5966006 Val_acc =  0.8636\n",
            "Iteration  355 : Loss =  0.5570596  Acc:  0.88013333 Val_loss =  0.5983603 Val_acc =  0.8634\n",
            "Iteration  356 : Loss =  0.5580035  Acc:  0.8785333 Val_loss =  0.6000101 Val_acc =  0.8617\n",
            "Iteration  357 : Loss =  0.557089  Acc:  0.87988335 Val_loss =  0.59838045 Val_acc =  0.8626\n",
            "Iteration  358 : Loss =  0.55370706  Acc:  0.8807 Val_loss =  0.5955725 Val_acc =  0.8639\n",
            "Iteration  359 : Loss =  0.55171144  Acc:  0.8812 Val_loss =  0.5934525 Val_acc =  0.8648\n",
            "Iteration  360 : Loss =  0.55196166  Acc:  0.88133335 Val_loss =  0.59351444 Val_acc =  0.8643\n",
            "Iteration  361 : Loss =  0.5529404  Acc:  0.8802 Val_loss =  0.59507084 Val_acc =  0.8635\n",
            "Iteration  362 : Loss =  0.5530627  Acc:  0.8807833 Val_loss =  0.59463334 Val_acc =  0.8637\n",
            "Iteration  363 : Loss =  0.5512163  Acc:  0.88096666 Val_loss =  0.59330434 Val_acc =  0.8631\n",
            "Iteration  364 : Loss =  0.5494746  Acc:  0.88135 Val_loss =  0.59148073 Val_acc =  0.865\n",
            "Iteration  365 : Loss =  0.54886943  Acc:  0.88121665 Val_loss =  0.5906891 Val_acc =  0.8647\n",
            "Iteration  366 : Loss =  0.54913586  Acc:  0.88105 Val_loss =  0.5914659 Val_acc =  0.8643\n",
            "Iteration  367 : Loss =  0.54930234  Acc:  0.88121665 Val_loss =  0.59114575 Val_acc =  0.8635\n",
            "Iteration  368 : Loss =  0.5485248  Acc:  0.8808333 Val_loss =  0.5908327 Val_acc =  0.8634\n",
            "Iteration  369 : Loss =  0.54748905  Acc:  0.88155 Val_loss =  0.5897701 Val_acc =  0.8654\n",
            "Iteration  370 : Loss =  0.546797  Acc:  0.88115 Val_loss =  0.5888568 Val_acc =  0.866\n",
            "Iteration  371 : Loss =  0.5467941  Acc:  0.8808333 Val_loss =  0.58948797 Val_acc =  0.8637\n",
            "Iteration  372 : Loss =  0.5470308  Acc:  0.88173336 Val_loss =  0.5890573 Val_acc =  0.8638\n",
            "Iteration  373 : Loss =  0.547484  Acc:  0.88058335 Val_loss =  0.5903267 Val_acc =  0.8626\n",
            "Iteration  374 : Loss =  0.54738915  Acc:  0.8814833 Val_loss =  0.5898217 Val_acc =  0.864\n",
            "Iteration  375 : Loss =  0.5484756  Acc:  0.88055 Val_loss =  0.5910816 Val_acc =  0.8626\n",
            "Iteration  376 : Loss =  0.5485867  Acc:  0.88026667 Val_loss =  0.5914778 Val_acc =  0.8622\n",
            "Iteration  377 : Loss =  0.54887736  Acc:  0.88035 Val_loss =  0.5913592 Val_acc =  0.8626\n",
            "Iteration  378 : Loss =  0.5470892  Acc:  0.88051665 Val_loss =  0.5899975 Val_acc =  0.8635\n",
            "Iteration  379 : Loss =  0.5446333  Acc:  0.8815 Val_loss =  0.58744633 Val_acc =  0.8638\n",
            "Iteration  380 : Loss =  0.54262614  Acc:  0.8816 Val_loss =  0.58510476 Val_acc =  0.8663\n",
            "Iteration  381 : Loss =  0.541802  Acc:  0.8818833 Val_loss =  0.58498675 Val_acc =  0.8666\n",
            "Iteration  382 : Loss =  0.5415863  Acc:  0.88165 Val_loss =  0.5841482 Val_acc =  0.8645\n",
            "Iteration  383 : Loss =  0.541564  Acc:  0.8822 Val_loss =  0.5846872 Val_acc =  0.8659\n",
            "Iteration  384 : Loss =  0.54205585  Acc:  0.88185 Val_loss =  0.58529145 Val_acc =  0.8639\n",
            "Iteration  385 : Loss =  0.5423939  Acc:  0.8818167 Val_loss =  0.58503914 Val_acc =  0.8647\n",
            "Iteration  386 : Loss =  0.542011  Acc:  0.8814333 Val_loss =  0.5856449 Val_acc =  0.863\n",
            "Iteration  387 : Loss =  0.5398502  Acc:  0.8825 Val_loss =  0.5825688 Val_acc =  0.8657\n",
            "Iteration  388 : Loss =  0.53776073  Acc:  0.88261664 Val_loss =  0.58103085 Val_acc =  0.8657\n",
            "Iteration  389 : Loss =  0.53702855  Acc:  0.8828 Val_loss =  0.5803466 Val_acc =  0.8664\n",
            "Iteration  390 : Loss =  0.5373565  Acc:  0.8821 Val_loss =  0.58022946 Val_acc =  0.8672\n",
            "Iteration  391 : Loss =  0.5375212  Acc:  0.88215 Val_loss =  0.58124745 Val_acc =  0.8644\n",
            "Iteration  392 : Loss =  0.53702235  Acc:  0.8832 Val_loss =  0.5801827 Val_acc =  0.8661\n",
            "Iteration  393 : Loss =  0.5365208  Acc:  0.88228333 Val_loss =  0.5799823 Val_acc =  0.8655\n",
            "Iteration  394 : Loss =  0.53609836  Acc:  0.88306665 Val_loss =  0.57968915 Val_acc =  0.866\n",
            "Iteration  395 : Loss =  0.5355308  Acc:  0.8824833 Val_loss =  0.5787331 Val_acc =  0.8653\n",
            "Iteration  396 : Loss =  0.5344931  Acc:  0.8829 Val_loss =  0.57820684 Val_acc =  0.8656\n",
            "Iteration  397 : Loss =  0.5335606  Acc:  0.8828833 Val_loss =  0.57712394 Val_acc =  0.8662\n",
            "Iteration  398 : Loss =  0.53310585  Acc:  0.88353336 Val_loss =  0.5765569 Val_acc =  0.8672\n",
            "Iteration  399 : Loss =  0.5329082  Acc:  0.88343334 Val_loss =  0.57682014 Val_acc =  0.8669\n",
            "Iteration  400 : Loss =  0.5325826  Acc:  0.88371664 Val_loss =  0.57601833 Val_acc =  0.8676\n",
            "Iteration  401 : Loss =  0.5319479  Acc:  0.8837 Val_loss =  0.57588863 Val_acc =  0.8678\n",
            "Iteration  402 : Loss =  0.53136176  Acc:  0.8836833 Val_loss =  0.5751947 Val_acc =  0.8674\n",
            "Iteration  403 : Loss =  0.5310358  Acc:  0.88385 Val_loss =  0.5747473 Val_acc =  0.8674\n",
            "Iteration  404 : Loss =  0.5309526  Acc:  0.8833 Val_loss =  0.5751732 Val_acc =  0.8656\n",
            "Iteration  405 : Loss =  0.53074825  Acc:  0.88385 Val_loss =  0.57442886 Val_acc =  0.8662\n",
            "Iteration  406 : Loss =  0.5305976  Acc:  0.88325 Val_loss =  0.57493544 Val_acc =  0.8649\n",
            "Iteration  407 : Loss =  0.53025436  Acc:  0.8843167 Val_loss =  0.57420784 Val_acc =  0.866\n",
            "Iteration  408 : Loss =  0.53029114  Acc:  0.8835667 Val_loss =  0.5744666 Val_acc =  0.8658\n",
            "Iteration  409 : Loss =  0.5299199  Acc:  0.8840167 Val_loss =  0.5741354 Val_acc =  0.866\n",
            "Iteration  410 : Loss =  0.5295055  Acc:  0.88346666 Val_loss =  0.5736413 Val_acc =  0.8655\n",
            "Iteration  411 : Loss =  0.52842283  Acc:  0.88453335 Val_loss =  0.5726954 Val_acc =  0.8666\n",
            "Iteration  412 : Loss =  0.5274458  Acc:  0.88385 Val_loss =  0.57180744 Val_acc =  0.8669\n",
            "Iteration  413 : Loss =  0.5265496  Acc:  0.8846833 Val_loss =  0.5707058 Val_acc =  0.867\n",
            "Iteration  414 : Loss =  0.5259502  Acc:  0.88421667 Val_loss =  0.5705506 Val_acc =  0.8667\n",
            "Iteration  415 : Loss =  0.52544093  Acc:  0.8844333 Val_loss =  0.5696768 Val_acc =  0.8676\n",
            "Iteration  416 : Loss =  0.5249656  Acc:  0.88481665 Val_loss =  0.56963134 Val_acc =  0.8671\n",
            "Iteration  417 : Loss =  0.5245881  Acc:  0.8842667 Val_loss =  0.56911194 Val_acc =  0.8684\n",
            "Iteration  418 : Loss =  0.5244298  Acc:  0.88505 Val_loss =  0.5689684 Val_acc =  0.8679\n",
            "Iteration  419 : Loss =  0.5244526  Acc:  0.88418335 Val_loss =  0.569307 Val_acc =  0.8679\n",
            "Iteration  420 : Loss =  0.5247141  Acc:  0.88515 Val_loss =  0.5692376 Val_acc =  0.8671\n",
            "Iteration  421 : Loss =  0.524849  Acc:  0.88413334 Val_loss =  0.5698944 Val_acc =  0.866\n",
            "Iteration  422 : Loss =  0.52521265  Acc:  0.88456666 Val_loss =  0.56989807 Val_acc =  0.8672\n",
            "Iteration  423 : Loss =  0.52503794  Acc:  0.88376665 Val_loss =  0.57004774 Val_acc =  0.8659\n",
            "Iteration  424 : Loss =  0.5248873  Acc:  0.8846167 Val_loss =  0.56989896 Val_acc =  0.866\n",
            "Iteration  425 : Loss =  0.52390677  Acc:  0.8843833 Val_loss =  0.56883645 Val_acc =  0.8664\n",
            "Iteration  426 : Loss =  0.52273285  Acc:  0.8847333 Val_loss =  0.5680076 Val_acc =  0.8659\n",
            "Iteration  427 : Loss =  0.5216194  Acc:  0.88558334 Val_loss =  0.56662166 Val_acc =  0.8678\n",
            "Iteration  428 : Loss =  0.52133805  Acc:  0.88448334 Val_loss =  0.5667509 Val_acc =  0.8669\n",
            "Iteration  429 : Loss =  0.52190244  Acc:  0.8854833 Val_loss =  0.5671191 Val_acc =  0.8667\n",
            "Iteration  430 : Loss =  0.5239253  Acc:  0.8836 Val_loss =  0.5694982 Val_acc =  0.8656\n",
            "Iteration  431 : Loss =  0.5246542  Acc:  0.8839833 Val_loss =  0.56999224 Val_acc =  0.8644\n",
            "Iteration  432 : Loss =  0.5256994  Acc:  0.88273335 Val_loss =  0.5714451 Val_acc =  0.8661\n",
            "Iteration  433 : Loss =  0.5222876  Acc:  0.8847333 Val_loss =  0.5675623 Val_acc =  0.8665\n",
            "Iteration  434 : Loss =  0.5194464  Acc:  0.88476664 Val_loss =  0.5650995 Val_acc =  0.8664\n",
            "Iteration  435 : Loss =  0.5176319  Acc:  0.88635 Val_loss =  0.5629987 Val_acc =  0.8695\n",
            "Iteration  436 : Loss =  0.5181027  Acc:  0.88595 Val_loss =  0.5637371 Val_acc =  0.8678\n",
            "Iteration  437 : Loss =  0.5192737  Acc:  0.88533336 Val_loss =  0.5650663 Val_acc =  0.8673\n",
            "Iteration  438 : Loss =  0.51953155  Acc:  0.8853 Val_loss =  0.565041 Val_acc =  0.8681\n",
            "Iteration  439 : Loss =  0.5180679  Acc:  0.8853833 Val_loss =  0.5641166 Val_acc =  0.8677\n",
            "Iteration  440 : Loss =  0.51593626  Acc:  0.8861833 Val_loss =  0.5614344 Val_acc =  0.8695\n",
            "Iteration  441 : Loss =  0.5147246  Acc:  0.8865167 Val_loss =  0.5607127 Val_acc =  0.8685\n",
            "Iteration  442 : Loss =  0.51486003  Acc:  0.8857167 Val_loss =  0.56072575 Val_acc =  0.8691\n",
            "Iteration  443 : Loss =  0.5157203  Acc:  0.8864667 Val_loss =  0.5616355 Val_acc =  0.8685\n",
            "Iteration  444 : Loss =  0.51597947  Acc:  0.8857667 Val_loss =  0.5622681 Val_acc =  0.8686\n",
            "Iteration  445 : Loss =  0.51560444  Acc:  0.8862333 Val_loss =  0.56149876 Val_acc =  0.8689\n",
            "Iteration  446 : Loss =  0.514245  Acc:  0.88636667 Val_loss =  0.5606159 Val_acc =  0.8691\n",
            "Iteration  447 : Loss =  0.5133401  Acc:  0.8864 Val_loss =  0.55945104 Val_acc =  0.869\n",
            "Iteration  448 : Loss =  0.51331896  Acc:  0.8867 Val_loss =  0.55955815 Val_acc =  0.8687\n",
            "Iteration  449 : Loss =  0.5142486  Acc:  0.88566667 Val_loss =  0.5608423 Val_acc =  0.8675\n",
            "Iteration  450 : Loss =  0.51447326  Acc:  0.88595 Val_loss =  0.5606706 Val_acc =  0.8679\n",
            "Iteration  451 : Loss =  0.51466435  Acc:  0.8851167 Val_loss =  0.5615312 Val_acc =  0.867\n",
            "Iteration  452 : Loss =  0.51303756  Acc:  0.8864833 Val_loss =  0.55937713 Val_acc =  0.8682\n",
            "Iteration  453 : Loss =  0.5119694  Acc:  0.8862 Val_loss =  0.5586387 Val_acc =  0.8686\n",
            "Iteration  454 : Loss =  0.51152176  Acc:  0.8872333 Val_loss =  0.5582207 Val_acc =  0.8698\n",
            "Iteration  455 : Loss =  0.51167893  Acc:  0.88668334 Val_loss =  0.558209 Val_acc =  0.8691\n",
            "Iteration  456 : Loss =  0.5111843  Acc:  0.8864167 Val_loss =  0.558153 Val_acc =  0.8687\n",
            "Iteration  457 : Loss =  0.51018554  Acc:  0.8872333 Val_loss =  0.5568279 Val_acc =  0.8693\n",
            "Iteration  458 : Loss =  0.5086236  Acc:  0.88698334 Val_loss =  0.555529 Val_acc =  0.8701\n",
            "Iteration  459 : Loss =  0.5076074  Acc:  0.8878833 Val_loss =  0.55456877 Val_acc =  0.8697\n",
            "Iteration  460 : Loss =  0.50735265  Acc:  0.88775 Val_loss =  0.5541674 Val_acc =  0.8701\n",
            "Iteration  461 : Loss =  0.5075314  Acc:  0.88733333 Val_loss =  0.5547584 Val_acc =  0.87\n",
            "Iteration  462 : Loss =  0.5077353  Acc:  0.88731664 Val_loss =  0.5546624 Val_acc =  0.8695\n",
            "Iteration  463 : Loss =  0.5075073  Acc:  0.88733333 Val_loss =  0.5548178 Val_acc =  0.8695\n",
            "Iteration  464 : Loss =  0.5071959  Acc:  0.8878833 Val_loss =  0.5544355 Val_acc =  0.87\n",
            "Iteration  465 : Loss =  0.5068794  Acc:  0.88785 Val_loss =  0.5540758 Val_acc =  0.869\n",
            "Iteration  466 : Loss =  0.5069785  Acc:  0.88708335 Val_loss =  0.5545752 Val_acc =  0.8692\n",
            "Iteration  467 : Loss =  0.5074076  Acc:  0.8872667 Val_loss =  0.55457133 Val_acc =  0.8692\n",
            "Iteration  468 : Loss =  0.508312  Acc:  0.8857833 Val_loss =  0.5561938 Val_acc =  0.8683\n",
            "Iteration  469 : Loss =  0.50820696  Acc:  0.8868333 Val_loss =  0.55560535 Val_acc =  0.8682\n",
            "Iteration  470 : Loss =  0.5081773  Acc:  0.8858167 Val_loss =  0.55597585 Val_acc =  0.8683\n",
            "Iteration  471 : Loss =  0.50655556  Acc:  0.8872167 Val_loss =  0.554284 Val_acc =  0.8691\n",
            "Iteration  472 : Loss =  0.5049645  Acc:  0.8871833 Val_loss =  0.5524552 Val_acc =  0.869\n",
            "Iteration  473 : Loss =  0.5034424  Acc:  0.88865 Val_loss =  0.55144054 Val_acc =  0.8684\n",
            "Iteration  474 : Loss =  0.50248766  Acc:  0.88836664 Val_loss =  0.55004317 Val_acc =  0.871\n",
            "Iteration  475 : Loss =  0.5021968  Acc:  0.8882333 Val_loss =  0.5502403 Val_acc =  0.87\n",
            "Iteration  476 : Loss =  0.5024433  Acc:  0.8883333 Val_loss =  0.5503362 Val_acc =  0.8695\n",
            "Iteration  477 : Loss =  0.5031272  Acc:  0.88743335 Val_loss =  0.5511036 Val_acc =  0.8688\n",
            "Iteration  478 : Loss =  0.50349957  Acc:  0.88795 Val_loss =  0.55170435 Val_acc =  0.8691\n",
            "Iteration  479 : Loss =  0.50367033  Acc:  0.8868167 Val_loss =  0.5516722 Val_acc =  0.8683\n",
            "Iteration  480 : Loss =  0.5030218  Acc:  0.8882667 Val_loss =  0.5512613 Val_acc =  0.8693\n",
            "Iteration  481 : Loss =  0.5020853  Acc:  0.8878667 Val_loss =  0.5503067 Val_acc =  0.8698\n",
            "Iteration  482 : Loss =  0.5011721  Acc:  0.88883334 Val_loss =  0.5492796 Val_acc =  0.8693\n",
            "Iteration  483 : Loss =  0.5005565  Acc:  0.88848335 Val_loss =  0.54910356 Val_acc =  0.8704\n",
            "Iteration  484 : Loss =  0.50001484  Acc:  0.8886833 Val_loss =  0.54811203 Val_acc =  0.8694\n",
            "Iteration  485 : Loss =  0.49943823  Acc:  0.8887333 Val_loss =  0.5481053 Val_acc =  0.8706\n",
            "Iteration  486 : Loss =  0.4987688  Acc:  0.8884 Val_loss =  0.54714495 Val_acc =  0.8696\n",
            "Iteration  487 : Loss =  0.49823004  Acc:  0.88916665 Val_loss =  0.54676044 Val_acc =  0.8702\n",
            "Iteration  488 : Loss =  0.49812132  Acc:  0.8886167 Val_loss =  0.54693544 Val_acc =  0.8707\n",
            "Iteration  489 : Loss =  0.49823374  Acc:  0.8890667 Val_loss =  0.5465829 Val_acc =  0.8698\n",
            "Iteration  490 : Loss =  0.4984916  Acc:  0.88851666 Val_loss =  0.54757243 Val_acc =  0.8692\n",
            "Iteration  491 : Loss =  0.49845713  Acc:  0.88893336 Val_loss =  0.5469251 Val_acc =  0.8696\n",
            "Iteration  492 : Loss =  0.49821055  Acc:  0.88865 Val_loss =  0.5472404 Val_acc =  0.8695\n",
            "Iteration  493 : Loss =  0.4976827  Acc:  0.8893167 Val_loss =  0.5465199 Val_acc =  0.87\n",
            "Iteration  494 : Loss =  0.4970931  Acc:  0.8895 Val_loss =  0.54589903 Val_acc =  0.8706\n",
            "Iteration  495 : Loss =  0.49624556  Acc:  0.8894333 Val_loss =  0.54546344 Val_acc =  0.8708\n",
            "Iteration  496 : Loss =  0.49540704  Acc:  0.88958335 Val_loss =  0.54417574 Val_acc =  0.871\n",
            "Iteration  497 : Loss =  0.49456078  Acc:  0.88993335 Val_loss =  0.54393595 Val_acc =  0.8715\n",
            "Iteration  498 : Loss =  0.49399015  Acc:  0.88958335 Val_loss =  0.54298425 Val_acc =  0.8718\n",
            "Iteration  499 : Loss =  0.4937775  Acc:  0.8897333 Val_loss =  0.5431213 Val_acc =  0.8724\n",
            "Iteration  500 : Loss =  0.49390423  Acc:  0.89016664 Val_loss =  0.54318035 Val_acc =  0.8702\n",
            "Iteration  501 : Loss =  0.49430037  Acc:  0.88945 Val_loss =  0.54359573 Val_acc =  0.8712\n",
            "Iteration  502 : Loss =  0.4947364  Acc:  0.88951665 Val_loss =  0.54423136 Val_acc =  0.8704\n",
            "Iteration  503 : Loss =  0.49498093  Acc:  0.88881665 Val_loss =  0.54439163 Val_acc =  0.8701\n",
            "Iteration  504 : Loss =  0.4947916  Acc:  0.88925 Val_loss =  0.5443279 Val_acc =  0.8697\n",
            "Iteration  505 : Loss =  0.49413666  Acc:  0.8890167 Val_loss =  0.5438392 Val_acc =  0.8705\n",
            "Iteration  506 : Loss =  0.49308544  Acc:  0.88981664 Val_loss =  0.54254156 Val_acc =  0.871\n",
            "Iteration  507 : Loss =  0.49207032  Acc:  0.8896667 Val_loss =  0.5420502 Val_acc =  0.8709\n",
            "Iteration  508 : Loss =  0.49118614  Acc:  0.8897667 Val_loss =  0.54060936 Val_acc =  0.8705\n",
            "Iteration  509 : Loss =  0.49069172  Acc:  0.8901167 Val_loss =  0.5408079 Val_acc =  0.8714\n",
            "Iteration  510 : Loss =  0.49046904  Acc:  0.89035 Val_loss =  0.5400764 Val_acc =  0.8708\n",
            "Iteration  511 : Loss =  0.49044943  Acc:  0.8904833 Val_loss =  0.5405132 Val_acc =  0.8711\n",
            "Iteration  512 : Loss =  0.49075598  Acc:  0.89026666 Val_loss =  0.54067755 Val_acc =  0.8715\n",
            "Iteration  513 : Loss =  0.49104595  Acc:  0.8902 Val_loss =  0.5409815 Val_acc =  0.8706\n",
            "Iteration  514 : Loss =  0.49142343  Acc:  0.8898 Val_loss =  0.54165655 Val_acc =  0.8703\n",
            "Iteration  515 : Loss =  0.49112707  Acc:  0.88993335 Val_loss =  0.54106015 Val_acc =  0.8702\n",
            "Iteration  516 : Loss =  0.49042752  Acc:  0.8901 Val_loss =  0.5408281 Val_acc =  0.8704\n",
            "Iteration  517 : Loss =  0.4890911  Acc:  0.8905333 Val_loss =  0.5392948 Val_acc =  0.8709\n",
            "Iteration  518 : Loss =  0.48795646  Acc:  0.8907 Val_loss =  0.5382488 Val_acc =  0.8714\n",
            "Iteration  519 : Loss =  0.4873981  Acc:  0.89068335 Val_loss =  0.5379912 Val_acc =  0.872\n",
            "Iteration  520 : Loss =  0.48744822  Acc:  0.89061666 Val_loss =  0.53756887 Val_acc =  0.8707\n",
            "Iteration  521 : Loss =  0.4879703  Acc:  0.88996667 Val_loss =  0.53888327 Val_acc =  0.8703\n",
            "Iteration  522 : Loss =  0.48846653  Acc:  0.8903 Val_loss =  0.538673 Val_acc =  0.8695\n",
            "Iteration  523 : Loss =  0.4891186  Acc:  0.8893 Val_loss =  0.54012376 Val_acc =  0.8701\n",
            "Iteration  524 : Loss =  0.48930973  Acc:  0.88963336 Val_loss =  0.53986794 Val_acc =  0.8697\n",
            "Iteration  525 : Loss =  0.48961505  Acc:  0.8890167 Val_loss =  0.54048467 Val_acc =  0.8702\n",
            "Iteration  526 : Loss =  0.48912376  Acc:  0.88961667 Val_loss =  0.5400536 Val_acc =  0.8693\n",
            "Iteration  527 : Loss =  0.4886475  Acc:  0.8895 Val_loss =  0.539407 Val_acc =  0.8695\n",
            "Iteration  528 : Loss =  0.4873594  Acc:  0.8903 Val_loss =  0.5383773 Val_acc =  0.8687\n",
            "Iteration  529 : Loss =  0.48654127  Acc:  0.8907167 Val_loss =  0.53754485 Val_acc =  0.8711\n",
            "Iteration  530 : Loss =  0.48616806  Acc:  0.8909 Val_loss =  0.5370164 Val_acc =  0.8703\n",
            "Iteration  531 : Loss =  0.48610365  Acc:  0.8907667 Val_loss =  0.5375365 Val_acc =  0.8714\n",
            "Iteration  532 : Loss =  0.48631746  Acc:  0.8905333 Val_loss =  0.5369971 Val_acc =  0.8704\n",
            "Iteration  533 : Loss =  0.48530358  Acc:  0.89098334 Val_loss =  0.536876 Val_acc =  0.8706\n",
            "Iteration  534 : Loss =  0.48385996  Acc:  0.8914833 Val_loss =  0.534849 Val_acc =  0.872\n",
            "Iteration  535 : Loss =  0.48272878  Acc:  0.8914833 Val_loss =  0.53401434 Val_acc =  0.8712\n",
            "Iteration  536 : Loss =  0.48282468  Acc:  0.89125 Val_loss =  0.5344827 Val_acc =  0.872\n",
            "Iteration  537 : Loss =  0.48377532  Acc:  0.8912333 Val_loss =  0.5347652 Val_acc =  0.8709\n",
            "Iteration  538 : Loss =  0.48476112  Acc:  0.89075 Val_loss =  0.536861 Val_acc =  0.871\n",
            "Iteration  539 : Loss =  0.48533618  Acc:  0.89066666 Val_loss =  0.5364161 Val_acc =  0.8699\n",
            "Iteration  540 : Loss =  0.48437625  Acc:  0.891 Val_loss =  0.5363571 Val_acc =  0.8705\n",
            "Iteration  541 : Loss =  0.48290074  Acc:  0.89136666 Val_loss =  0.53442556 Val_acc =  0.8696\n",
            "Iteration  542 : Loss =  0.48128432  Acc:  0.89176667 Val_loss =  0.5327849 Val_acc =  0.8721\n",
            "Iteration  543 : Loss =  0.48034704  Acc:  0.8915667 Val_loss =  0.5324173 Val_acc =  0.8722\n",
            "Iteration  544 : Loss =  0.48014176  Acc:  0.8924 Val_loss =  0.53162396 Val_acc =  0.8722\n",
            "Iteration  545 : Loss =  0.4803409  Acc:  0.89176667 Val_loss =  0.5325366 Val_acc =  0.873\n",
            "Iteration  546 : Loss =  0.4808299  Acc:  0.89196664 Val_loss =  0.53269404 Val_acc =  0.8707\n",
            "Iteration  547 : Loss =  0.4808898  Acc:  0.8919333 Val_loss =  0.5329181 Val_acc =  0.871\n",
            "Iteration  548 : Loss =  0.48068962  Acc:  0.89225 Val_loss =  0.5329295 Val_acc =  0.8704\n",
            "Iteration  549 : Loss =  0.4796186  Acc:  0.89215 Val_loss =  0.5315444 Val_acc =  0.8729\n",
            "Iteration  550 : Loss =  0.47836334  Acc:  0.8919167 Val_loss =  0.5307261 Val_acc =  0.8716\n",
            "Iteration  551 : Loss =  0.477589  Acc:  0.8925167 Val_loss =  0.5298212 Val_acc =  0.8731\n",
            "Iteration  552 : Loss =  0.4776798  Acc:  0.89308333 Val_loss =  0.5299617 Val_acc =  0.8722\n",
            "Iteration  553 : Loss =  0.47840938  Acc:  0.89185 Val_loss =  0.5311188 Val_acc =  0.8714\n",
            "Iteration  554 : Loss =  0.47906965  Acc:  0.8922667 Val_loss =  0.53126895 Val_acc =  0.8717\n",
            "Iteration  555 : Loss =  0.4791971  Acc:  0.89126664 Val_loss =  0.53205335 Val_acc =  0.8697\n",
            "Iteration  556 : Loss =  0.4783256  Acc:  0.89246666 Val_loss =  0.5306768 Val_acc =  0.872\n",
            "Iteration  557 : Loss =  0.47731236  Acc:  0.8924 Val_loss =  0.53008777 Val_acc =  0.8722\n",
            "Iteration  558 : Loss =  0.47665572  Acc:  0.8928 Val_loss =  0.5293305 Val_acc =  0.8714\n",
            "Iteration  559 : Loss =  0.47676307  Acc:  0.8922333 Val_loss =  0.52951276 Val_acc =  0.8722\n",
            "Iteration  560 : Loss =  0.47701287  Acc:  0.89248335 Val_loss =  0.5298865 Val_acc =  0.8705\n",
            "Iteration  561 : Loss =  0.4774323  Acc:  0.8923 Val_loss =  0.5304512 Val_acc =  0.8712\n",
            "Iteration  562 : Loss =  0.47710314  Acc:  0.89243335 Val_loss =  0.5298424 Val_acc =  0.8708\n",
            "Iteration  563 : Loss =  0.476587  Acc:  0.89265 Val_loss =  0.5299331 Val_acc =  0.8708\n",
            "Iteration  564 : Loss =  0.47546276  Acc:  0.89278334 Val_loss =  0.5280061 Val_acc =  0.8701\n",
            "Iteration  565 : Loss =  0.47444874  Acc:  0.89295 Val_loss =  0.52793545 Val_acc =  0.8727\n",
            "Iteration  566 : Loss =  0.47358388  Acc:  0.8933667 Val_loss =  0.526339 Val_acc =  0.872\n",
            "Iteration  567 : Loss =  0.4731874  Acc:  0.89341664 Val_loss =  0.5265379 Val_acc =  0.8719\n",
            "Iteration  568 : Loss =  0.47323728  Acc:  0.8936 Val_loss =  0.5265312 Val_acc =  0.8732\n",
            "Iteration  569 : Loss =  0.47355834  Acc:  0.8932667 Val_loss =  0.5266527 Val_acc =  0.8717\n",
            "Iteration  570 : Loss =  0.47353324  Acc:  0.89313334 Val_loss =  0.527236 Val_acc =  0.8725\n",
            "Iteration  571 : Loss =  0.47316945  Acc:  0.89306664 Val_loss =  0.52624005 Val_acc =  0.8718\n",
            "Iteration  572 : Loss =  0.4721628  Acc:  0.8936 Val_loss =  0.525959 Val_acc =  0.8734\n",
            "Iteration  573 : Loss =  0.47124445  Acc:  0.89385 Val_loss =  0.5246285 Val_acc =  0.8726\n",
            "Iteration  574 : Loss =  0.47061378  Acc:  0.89415 Val_loss =  0.52428377 Val_acc =  0.8736\n",
            "Iteration  575 : Loss =  0.4704049  Acc:  0.89415 Val_loss =  0.52420783 Val_acc =  0.8735\n",
            "Iteration  576 : Loss =  0.47049502  Acc:  0.89435 Val_loss =  0.52408564 Val_acc =  0.8735\n",
            "Iteration  577 : Loss =  0.47072238  Acc:  0.8939833 Val_loss =  0.52476335 Val_acc =  0.8732\n",
            "Iteration  578 : Loss =  0.4709835  Acc:  0.89378333 Val_loss =  0.5246871 Val_acc =  0.8726\n",
            "Iteration  579 : Loss =  0.4712642  Acc:  0.89355 Val_loss =  0.52529895 Val_acc =  0.8722\n",
            "Iteration  580 : Loss =  0.47165757  Acc:  0.89321667 Val_loss =  0.52574575 Val_acc =  0.871\n",
            "Iteration  581 : Loss =  0.47221833  Acc:  0.8932 Val_loss =  0.52611756 Val_acc =  0.8718\n",
            "Iteration  582 : Loss =  0.47324663  Acc:  0.8921 Val_loss =  0.52789754 Val_acc =  0.872\n",
            "Iteration  583 : Loss =  0.47379306  Acc:  0.8924 Val_loss =  0.5275886 Val_acc =  0.8694\n",
            "Iteration  584 : Loss =  0.47525966  Acc:  0.89093333 Val_loss =  0.53021795 Val_acc =  0.8689\n",
            "Iteration  585 : Loss =  0.4736464  Acc:  0.8922 Val_loss =  0.5275734 Val_acc =  0.8693\n",
            "Iteration  586 : Loss =  0.47282135  Acc:  0.8919333 Val_loss =  0.52734697 Val_acc =  0.8721\n",
            "Iteration  587 : Loss =  0.47035167  Acc:  0.89376664 Val_loss =  0.5247183 Val_acc =  0.8717\n",
            "Iteration  588 : Loss =  0.46878576  Acc:  0.89415 Val_loss =  0.52283365 Val_acc =  0.8736\n",
            "Iteration  589 : Loss =  0.4682369  Acc:  0.89395 Val_loss =  0.52312815 Val_acc =  0.8727\n",
            "Iteration  590 : Loss =  0.46873927  Acc:  0.8939 Val_loss =  0.5231163 Val_acc =  0.8728\n",
            "Iteration  591 : Loss =  0.47028708  Acc:  0.89231664 Val_loss =  0.52521574 Val_acc =  0.8716\n",
            "Iteration  592 : Loss =  0.47073537  Acc:  0.89278334 Val_loss =  0.52557224 Val_acc =  0.8701\n",
            "Iteration  593 : Loss =  0.47058058  Acc:  0.89265 Val_loss =  0.52531445 Val_acc =  0.8717\n",
            "Iteration  594 : Loss =  0.46804225  Acc:  0.89393336 Val_loss =  0.5229369 Val_acc =  0.871\n",
            "Iteration  595 : Loss =  0.46575052  Acc:  0.8945 Val_loss =  0.52046835 Val_acc =  0.8743\n",
            "Iteration  596 : Loss =  0.46488652  Acc:  0.89496666 Val_loss =  0.51963484 Val_acc =  0.8733\n",
            "Iteration  597 : Loss =  0.46560907  Acc:  0.8947667 Val_loss =  0.5207157 Val_acc =  0.8729\n",
            "Iteration  598 : Loss =  0.46684986  Acc:  0.8937333 Val_loss =  0.5218329 Val_acc =  0.8732\n",
            "Iteration  599 : Loss =  0.4670935  Acc:  0.89388335 Val_loss =  0.5223507 Val_acc =  0.8708\n",
            "Iteration  600 : Loss =  0.46645492  Acc:  0.8941 Val_loss =  0.5217471 Val_acc =  0.8724\n",
            "Iteration  601 : Loss =  0.4648564  Acc:  0.89475 Val_loss =  0.5197782 Val_acc =  0.8717\n",
            "Iteration  602 : Loss =  0.46378887  Acc:  0.8948 Val_loss =  0.51928604 Val_acc =  0.8736\n",
            "Iteration  603 : Loss =  0.46328723  Acc:  0.89523333 Val_loss =  0.51818866 Val_acc =  0.8752\n",
            "Iteration  604 : Loss =  0.46324998  Acc:  0.8951167 Val_loss =  0.5188134 Val_acc =  0.8738\n",
            "Iteration  605 : Loss =  0.46343735  Acc:  0.89493334 Val_loss =  0.5188329 Val_acc =  0.8743\n",
            "Iteration  606 : Loss =  0.46351457  Acc:  0.8950833 Val_loss =  0.51891315 Val_acc =  0.872\n",
            "Iteration  607 : Loss =  0.4636166  Acc:  0.89433336 Val_loss =  0.5195126 Val_acc =  0.8727\n",
            "Iteration  608 : Loss =  0.46316722  Acc:  0.89523333 Val_loss =  0.518367 Val_acc =  0.8731\n",
            "Iteration  609 : Loss =  0.46250957  Acc:  0.89495 Val_loss =  0.51850295 Val_acc =  0.8737\n",
            "Iteration  610 : Loss =  0.4616105  Acc:  0.8957833 Val_loss =  0.5170048 Val_acc =  0.8749\n",
            "Iteration  611 : Loss =  0.46097064  Acc:  0.8957833 Val_loss =  0.51676655 Val_acc =  0.8736\n",
            "Iteration  612 : Loss =  0.4607801  Acc:  0.8957833 Val_loss =  0.51671755 Val_acc =  0.8748\n",
            "Iteration  613 : Loss =  0.4609732  Acc:  0.89568335 Val_loss =  0.5165547 Val_acc =  0.8732\n",
            "Iteration  614 : Loss =  0.46127722  Acc:  0.8954833 Val_loss =  0.5176231 Val_acc =  0.8741\n",
            "Iteration  615 : Loss =  0.46139923  Acc:  0.89521664 Val_loss =  0.51697457 Val_acc =  0.8726\n",
            "Iteration  616 : Loss =  0.46130216  Acc:  0.8955167 Val_loss =  0.5177031 Val_acc =  0.8738\n",
            "Iteration  617 : Loss =  0.46093553  Acc:  0.8955167 Val_loss =  0.51678 Val_acc =  0.8735\n",
            "Iteration  618 : Loss =  0.46053326  Acc:  0.89555 Val_loss =  0.51677233 Val_acc =  0.8744\n",
            "Iteration  619 : Loss =  0.4602514  Acc:  0.89565 Val_loss =  0.5164877 Val_acc =  0.8741\n",
            "Iteration  620 : Loss =  0.46005338  Acc:  0.89593333 Val_loss =  0.5162268 Val_acc =  0.8754\n",
            "Iteration  621 : Loss =  0.460077  Acc:  0.89533335 Val_loss =  0.5165371 Val_acc =  0.8733\n",
            "Iteration  622 : Loss =  0.46023047  Acc:  0.8958333 Val_loss =  0.51660216 Val_acc =  0.874\n",
            "Iteration  623 : Loss =  0.46058142  Acc:  0.89533335 Val_loss =  0.517094 Val_acc =  0.8715\n",
            "Iteration  624 : Loss =  0.46096396  Acc:  0.8950667 Val_loss =  0.5176415 Val_acc =  0.8717\n",
            "Iteration  625 : Loss =  0.46078068  Acc:  0.89496666 Val_loss =  0.5172558 Val_acc =  0.8717\n",
            "Iteration  626 : Loss =  0.45965403  Acc:  0.8954667 Val_loss =  0.5164146 Val_acc =  0.872\n",
            "Iteration  627 : Loss =  0.45826048  Acc:  0.8961 Val_loss =  0.51490766 Val_acc =  0.874\n",
            "Iteration  628 : Loss =  0.45705462  Acc:  0.89676666 Val_loss =  0.5137616 Val_acc =  0.8745\n",
            "Iteration  629 : Loss =  0.45636028  Acc:  0.89648336 Val_loss =  0.513253 Val_acc =  0.8741\n",
            "Iteration  630 : Loss =  0.4561111  Acc:  0.8965 Val_loss =  0.5128126 Val_acc =  0.8757\n",
            "Iteration  631 : Loss =  0.45612377  Acc:  0.8965167 Val_loss =  0.5132494 Val_acc =  0.8746\n",
            "Iteration  632 : Loss =  0.45628655  Acc:  0.8965667 Val_loss =  0.5132165 Val_acc =  0.8748\n",
            "Iteration  633 : Loss =  0.45658755  Acc:  0.8965 Val_loss =  0.51374143 Val_acc =  0.8727\n",
            "Iteration  634 : Loss =  0.45699424  Acc:  0.8962167 Val_loss =  0.5142117 Val_acc =  0.8734\n",
            "Iteration  635 : Loss =  0.45734388  Acc:  0.896 Val_loss =  0.51439846 Val_acc =  0.8732\n",
            "Iteration  636 : Loss =  0.45747817  Acc:  0.89538336 Val_loss =  0.5149858 Val_acc =  0.8728\n",
            "Iteration  637 : Loss =  0.45708653  Acc:  0.89648336 Val_loss =  0.5141181 Val_acc =  0.874\n",
            "Iteration  638 : Loss =  0.45628852  Acc:  0.89603335 Val_loss =  0.513855 Val_acc =  0.8732\n",
            "Iteration  639 : Loss =  0.45527464  Acc:  0.8968833 Val_loss =  0.51252884 Val_acc =  0.8745\n",
            "Iteration  640 : Loss =  0.45439646  Acc:  0.8969167 Val_loss =  0.51181686 Val_acc =  0.874\n",
            "Iteration  641 : Loss =  0.45396823  Acc:  0.89668334 Val_loss =  0.51164186 Val_acc =  0.8757\n",
            "Iteration  642 : Loss =  0.4542063  Acc:  0.89673334 Val_loss =  0.5114894 Val_acc =  0.8734\n",
            "Iteration  643 : Loss =  0.45503584  Acc:  0.8965 Val_loss =  0.51311743 Val_acc =  0.8728\n",
            "Iteration  644 : Loss =  0.45594463  Acc:  0.89575 Val_loss =  0.513235 Val_acc =  0.8724\n",
            "Iteration  645 : Loss =  0.4574269  Acc:  0.8954833 Val_loss =  0.5156852 Val_acc =  0.8729\n",
            "Iteration  646 : Loss =  0.4572822  Acc:  0.89568335 Val_loss =  0.51477045 Val_acc =  0.8726\n",
            "Iteration  647 : Loss =  0.4572211  Acc:  0.8951667 Val_loss =  0.515247 Val_acc =  0.8732\n",
            "Iteration  648 : Loss =  0.45522252  Acc:  0.8963 Val_loss =  0.5130035 Val_acc =  0.873\n",
            "Iteration  649 : Loss =  0.4533405  Acc:  0.89673334 Val_loss =  0.5110669 Val_acc =  0.8763\n",
            "Iteration  650 : Loss =  0.45222926  Acc:  0.8969167 Val_loss =  0.5103285 Val_acc =  0.8748\n",
            "Iteration  651 : Loss =  0.45234984  Acc:  0.89746666 Val_loss =  0.51028025 Val_acc =  0.8736\n",
            "Iteration  652 : Loss =  0.4534331  Acc:  0.89648336 Val_loss =  0.51172453 Val_acc =  0.874\n",
            "Iteration  653 : Loss =  0.4542762  Acc:  0.89645 Val_loss =  0.51251364 Val_acc =  0.8721\n",
            "Iteration  654 : Loss =  0.45460868  Acc:  0.89571667 Val_loss =  0.5128854 Val_acc =  0.8739\n",
            "Iteration  655 : Loss =  0.4531532  Acc:  0.8965833 Val_loss =  0.51145077 Val_acc =  0.8723\n",
            "Iteration  656 : Loss =  0.45137253  Acc:  0.8968833 Val_loss =  0.5096308 Val_acc =  0.8755\n",
            "Iteration  657 : Loss =  0.44987112  Acc:  0.89773333 Val_loss =  0.5081103 Val_acc =  0.8748\n",
            "Iteration  658 : Loss =  0.44942623  Acc:  0.8978 Val_loss =  0.50790167 Val_acc =  0.8749\n",
            "Iteration  659 : Loss =  0.44985515  Acc:  0.8973333 Val_loss =  0.5082523 Val_acc =  0.8761\n",
            "Iteration  660 : Loss =  0.4505713  Acc:  0.8973 Val_loss =  0.50926226 Val_acc =  0.8734\n",
            "Iteration  661 : Loss =  0.45112008  Acc:  0.8969333 Val_loss =  0.5097435 Val_acc =  0.8749\n",
            "Iteration  662 : Loss =  0.45094585  Acc:  0.89703333 Val_loss =  0.5095679 Val_acc =  0.8726\n",
            "Iteration  663 : Loss =  0.45048207  Acc:  0.89716667 Val_loss =  0.50930965 Val_acc =  0.8744\n",
            "Iteration  664 : Loss =  0.44970497  Acc:  0.89773333 Val_loss =  0.508224 Val_acc =  0.8736\n",
            "Iteration  665 : Loss =  0.44915277  Acc:  0.89776665 Val_loss =  0.5080745 Val_acc =  0.8745\n",
            "Iteration  666 : Loss =  0.4490972  Acc:  0.89781666 Val_loss =  0.5078179 Val_acc =  0.8759\n",
            "Iteration  667 : Loss =  0.4497306  Acc:  0.89705 Val_loss =  0.50860786 Val_acc =  0.8742\n",
            "Iteration  668 : Loss =  0.45080274  Acc:  0.89703333 Val_loss =  0.5100272 Val_acc =  0.8758\n",
            "Iteration  669 : Loss =  0.452097  Acc:  0.8961667 Val_loss =  0.5107837 Val_acc =  0.8738\n",
            "Iteration  670 : Loss =  0.4522298  Acc:  0.89626664 Val_loss =  0.5118291 Val_acc =  0.8726\n",
            "Iteration  671 : Loss =  0.45133585  Acc:  0.8957667 Val_loss =  0.5098956 Val_acc =  0.8738\n",
            "Iteration  672 : Loss =  0.44911307  Acc:  0.89708334 Val_loss =  0.5086309 Val_acc =  0.8747\n",
            "Iteration  673 : Loss =  0.44735348  Acc:  0.8979333 Val_loss =  0.5062773 Val_acc =  0.8744\n",
            "Iteration  674 : Loss =  0.44657752  Acc:  0.8983 Val_loss =  0.50584245 Val_acc =  0.8747\n",
            "Iteration  675 : Loss =  0.44725123  Acc:  0.89788336 Val_loss =  0.5068893 Val_acc =  0.8752\n",
            "Iteration  676 : Loss =  0.4477925  Acc:  0.8979 Val_loss =  0.50685644 Val_acc =  0.8732\n",
            "Iteration  677 : Loss =  0.44804972  Acc:  0.89741665 Val_loss =  0.5079474 Val_acc =  0.8735\n",
            "Iteration  678 : Loss =  0.44730335  Acc:  0.89755 Val_loss =  0.5063783 Val_acc =  0.8746\n",
            "Iteration  679 : Loss =  0.4462455  Acc:  0.89815 Val_loss =  0.5059527 Val_acc =  0.8758\n",
            "Iteration  680 : Loss =  0.4458475  Acc:  0.8981 Val_loss =  0.5053308 Val_acc =  0.8747\n",
            "Iteration  681 : Loss =  0.44625026  Acc:  0.8984 Val_loss =  0.50577354 Val_acc =  0.8747\n",
            "Iteration  682 : Loss =  0.44725302  Acc:  0.89741665 Val_loss =  0.5071982 Val_acc =  0.8742\n",
            "Iteration  683 : Loss =  0.4478841  Acc:  0.89743334 Val_loss =  0.50752187 Val_acc =  0.8725\n",
            "Iteration  684 : Loss =  0.44758433  Acc:  0.89725 Val_loss =  0.5076428 Val_acc =  0.8737\n",
            "Iteration  685 : Loss =  0.44589382  Acc:  0.8979 Val_loss =  0.5056951 Val_acc =  0.8732\n",
            "Iteration  686 : Loss =  0.44430614  Acc:  0.89875 Val_loss =  0.5040903 Val_acc =  0.8756\n",
            "Iteration  687 : Loss =  0.44319278  Acc:  0.8990167 Val_loss =  0.5032301 Val_acc =  0.8762\n",
            "Iteration  688 : Loss =  0.44289368  Acc:  0.8992 Val_loss =  0.5026117 Val_acc =  0.8769\n",
            "Iteration  689 : Loss =  0.4431883  Acc:  0.89891666 Val_loss =  0.50344974 Val_acc =  0.8756\n",
            "Iteration  690 : Loss =  0.44350332  Acc:  0.8991167 Val_loss =  0.5034913 Val_acc =  0.874\n",
            "Iteration  691 : Loss =  0.44389144  Acc:  0.8986833 Val_loss =  0.5041731 Val_acc =  0.8754\n",
            "Iteration  692 : Loss =  0.443888  Acc:  0.89825 Val_loss =  0.50423294 Val_acc =  0.874\n",
            "Iteration  693 : Loss =  0.4437356  Acc:  0.8989 Val_loss =  0.5038394 Val_acc =  0.8753\n",
            "Iteration  694 : Loss =  0.44325218  Acc:  0.8985 Val_loss =  0.5038067 Val_acc =  0.8742\n",
            "Iteration  695 : Loss =  0.44248638  Acc:  0.89891666 Val_loss =  0.50261176 Val_acc =  0.8744\n",
            "Iteration  696 : Loss =  0.44184554  Acc:  0.89923334 Val_loss =  0.50239015 Val_acc =  0.8771\n",
            "Iteration  697 : Loss =  0.44191563  Acc:  0.8991167 Val_loss =  0.50230783 Val_acc =  0.8756\n",
            "Iteration  698 : Loss =  0.4423266  Acc:  0.89905 Val_loss =  0.5028587 Val_acc =  0.878\n",
            "Iteration  699 : Loss =  0.4428408  Acc:  0.8987 Val_loss =  0.5034631 Val_acc =  0.875\n",
            "Iteration  700 : Loss =  0.44289872  Acc:  0.8984 Val_loss =  0.5035366 Val_acc =  0.877\n",
            "Iteration  701 : Loss =  0.44224522  Acc:  0.89875 Val_loss =  0.5028687 Val_acc =  0.8748\n",
            "Iteration  702 : Loss =  0.44134152  Acc:  0.8990333 Val_loss =  0.50220674 Val_acc =  0.8773\n",
            "Iteration  703 : Loss =  0.44039667  Acc:  0.89933336 Val_loss =  0.50095373 Val_acc =  0.8766\n",
            "Iteration  704 : Loss =  0.4398119  Acc:  0.89968336 Val_loss =  0.50093603 Val_acc =  0.8761\n",
            "Iteration  705 : Loss =  0.4395973  Acc:  0.8998167 Val_loss =  0.5002685 Val_acc =  0.8768\n",
            "Iteration  706 : Loss =  0.43954757  Acc:  0.8994833 Val_loss =  0.5007242 Val_acc =  0.876\n",
            "Iteration  707 : Loss =  0.43965733  Acc:  0.89963335 Val_loss =  0.5005965 Val_acc =  0.877\n",
            "Iteration  708 : Loss =  0.4397401  Acc:  0.8994333 Val_loss =  0.50076795 Val_acc =  0.8746\n",
            "Iteration  709 : Loss =  0.43988535  Acc:  0.9 Val_loss =  0.50113827 Val_acc =  0.8756\n",
            "Iteration  710 : Loss =  0.43965656  Acc:  0.89988333 Val_loss =  0.50059426 Val_acc =  0.8753\n",
            "Iteration  711 : Loss =  0.4393438  Acc:  0.89993334 Val_loss =  0.5007998 Val_acc =  0.8753\n",
            "Iteration  712 : Loss =  0.43872947  Acc:  0.89995 Val_loss =  0.49977404 Val_acc =  0.8751\n",
            "Iteration  713 : Loss =  0.4381772  Acc:  0.89988333 Val_loss =  0.49963307 Val_acc =  0.876\n",
            "Iteration  714 : Loss =  0.43785208  Acc:  0.9001833 Val_loss =  0.499246 Val_acc =  0.8771\n",
            "Iteration  715 : Loss =  0.43787372  Acc:  0.8998333 Val_loss =  0.49917102 Val_acc =  0.8757\n",
            "Iteration  716 : Loss =  0.43820465  Acc:  0.8998333 Val_loss =  0.49992812 Val_acc =  0.878\n",
            "Iteration  717 : Loss =  0.43912363  Acc:  0.89916664 Val_loss =  0.50031406 Val_acc =  0.8758\n",
            "Iteration  718 : Loss =  0.4402668  Acc:  0.8987667 Val_loss =  0.50223863 Val_acc =  0.8753\n",
            "Iteration  719 : Loss =  0.44178864  Acc:  0.8979333 Val_loss =  0.5030519 Val_acc =  0.874\n",
            "Iteration  720 : Loss =  0.44115102  Acc:  0.8983333 Val_loss =  0.50303113 Val_acc =  0.8752\n",
            "Iteration  721 : Loss =  0.4404287  Acc:  0.8983167 Val_loss =  0.50202817 Val_acc =  0.8737\n",
            "Iteration  722 : Loss =  0.43868747  Acc:  0.89935 Val_loss =  0.50044155 Val_acc =  0.877\n",
            "Iteration  723 : Loss =  0.4371609  Acc:  0.8995 Val_loss =  0.49906158 Val_acc =  0.8752\n",
            "Iteration  724 : Loss =  0.43590403  Acc:  0.90068334 Val_loss =  0.49771762 Val_acc =  0.8761\n",
            "Iteration  725 : Loss =  0.43595907  Acc:  0.9005 Val_loss =  0.49802628 Val_acc =  0.8774\n",
            "Iteration  726 : Loss =  0.4367059  Acc:  0.89995 Val_loss =  0.49866277 Val_acc =  0.8754\n",
            "Iteration  727 : Loss =  0.4378292  Acc:  0.8992 Val_loss =  0.50003725 Val_acc =  0.8767\n",
            "Iteration  728 : Loss =  0.43827564  Acc:  0.899 Val_loss =  0.5002308 Val_acc =  0.8743\n",
            "Iteration  729 : Loss =  0.43769366  Acc:  0.89926666 Val_loss =  0.49997872 Val_acc =  0.8773\n",
            "Iteration  730 : Loss =  0.43619245  Acc:  0.89955 Val_loss =  0.49813095 Val_acc =  0.8763\n",
            "Iteration  731 : Loss =  0.43467075  Acc:  0.9008333 Val_loss =  0.49703532 Val_acc =  0.8773\n",
            "Iteration  732 : Loss =  0.43402618  Acc:  0.9012333 Val_loss =  0.49616843 Val_acc =  0.8771\n",
            "Iteration  733 : Loss =  0.43438601  Acc:  0.9005167 Val_loss =  0.49687666 Val_acc =  0.8761\n",
            "Iteration  734 : Loss =  0.43539757  Acc:  0.9004833 Val_loss =  0.4978546 Val_acc =  0.8778\n",
            "Iteration  735 : Loss =  0.43600798  Acc:  0.90001667 Val_loss =  0.4984495 Val_acc =  0.875\n",
            "Iteration  736 : Loss =  0.43658176  Acc:  0.89961666 Val_loss =  0.4992909 Val_acc =  0.8774\n",
            "Iteration  737 : Loss =  0.43553957  Acc:  0.9001333 Val_loss =  0.49786535 Val_acc =  0.8752\n",
            "Iteration  738 : Loss =  0.43462273  Acc:  0.9007667 Val_loss =  0.49732068 Val_acc =  0.8763\n",
            "Iteration  739 : Loss =  0.4335622  Acc:  0.90075 Val_loss =  0.49603403 Val_acc =  0.876\n",
            "Iteration  740 : Loss =  0.4332379  Acc:  0.9005167 Val_loss =  0.495885 Val_acc =  0.8768\n",
            "Iteration  741 : Loss =  0.4335241  Acc:  0.9004 Val_loss =  0.49637872 Val_acc =  0.8781\n",
            "Iteration  742 : Loss =  0.43403456  Acc:  0.8999 Val_loss =  0.49662927 Val_acc =  0.8766\n",
            "Iteration  743 : Loss =  0.43395135  Acc:  0.9002333 Val_loss =  0.49711224 Val_acc =  0.8775\n",
            "Iteration  744 : Loss =  0.4332342  Acc:  0.90026665 Val_loss =  0.49582952 Val_acc =  0.8771\n",
            "Iteration  745 : Loss =  0.4318996  Acc:  0.90135 Val_loss =  0.49502957 Val_acc =  0.8775\n",
            "Iteration  746 : Loss =  0.4309172  Acc:  0.9014 Val_loss =  0.4937213 Val_acc =  0.8782\n",
            "Iteration  747 : Loss =  0.43047082  Acc:  0.90165 Val_loss =  0.49352995 Val_acc =  0.8779\n",
            "Iteration  748 : Loss =  0.43054962  Acc:  0.90165 Val_loss =  0.4937257 Val_acc =  0.878\n",
            "Iteration  749 : Loss =  0.4309281  Acc:  0.90141666 Val_loss =  0.4939698 Val_acc =  0.8776\n",
            "Iteration  750 : Loss =  0.43127382  Acc:  0.90106666 Val_loss =  0.49471754 Val_acc =  0.8782\n",
            "Iteration  751 : Loss =  0.43156457  Acc:  0.9008667 Val_loss =  0.49461702 Val_acc =  0.8773\n",
            "Iteration  752 : Loss =  0.43161935  Acc:  0.9008 Val_loss =  0.49518514 Val_acc =  0.8781\n",
            "Iteration  753 : Loss =  0.43162906  Acc:  0.90085 Val_loss =  0.49477887 Val_acc =  0.8764\n",
            "Iteration  754 : Loss =  0.43119496  Acc:  0.9009167 Val_loss =  0.49473178 Val_acc =  0.8774\n",
            "Iteration  755 : Loss =  0.43114716  Acc:  0.90095 Val_loss =  0.4945935 Val_acc =  0.8762\n",
            "Iteration  756 : Loss =  0.43178216  Acc:  0.90065 Val_loss =  0.49533853 Val_acc =  0.8766\n",
            "Iteration  757 : Loss =  0.4330305  Acc:  0.8999 Val_loss =  0.49675667 Val_acc =  0.875\n",
            "Iteration  758 : Loss =  0.43383318  Acc:  0.89958334 Val_loss =  0.49743256 Val_acc =  0.875\n",
            "Iteration  759 : Loss =  0.43447828  Acc:  0.89855 Val_loss =  0.49831367 Val_acc =  0.8759\n",
            "Iteration  760 : Loss =  0.43279043  Acc:  0.89998335 Val_loss =  0.4964229 Val_acc =  0.875\n",
            "Iteration  761 : Loss =  0.43081352  Acc:  0.90113336 Val_loss =  0.49453712 Val_acc =  0.8764\n",
            "Iteration  762 : Loss =  0.42860237  Acc:  0.9019167 Val_loss =  0.49241945 Val_acc =  0.878\n",
            "Iteration  763 : Loss =  0.4275989  Acc:  0.90245 Val_loss =  0.49133092 Val_acc =  0.8791\n",
            "Iteration  764 : Loss =  0.42787433  Acc:  0.90248334 Val_loss =  0.49197137 Val_acc =  0.8786\n",
            "Iteration  765 : Loss =  0.42875051  Acc:  0.90208334 Val_loss =  0.49270606 Val_acc =  0.8762\n",
            "Iteration  766 : Loss =  0.42989764  Acc:  0.9011833 Val_loss =  0.49414912 Val_acc =  0.8779\n",
            "Iteration  767 : Loss =  0.42994815  Acc:  0.9008667 Val_loss =  0.49405894 Val_acc =  0.8758\n",
            "Iteration  768 : Loss =  0.42983526  Acc:  0.90096664 Val_loss =  0.49401355 Val_acc =  0.8769\n",
            "Iteration  769 : Loss =  0.42888093  Acc:  0.90111667 Val_loss =  0.49314028 Val_acc =  0.8772\n",
            "Iteration  770 : Loss =  0.42807764  Acc:  0.9016 Val_loss =  0.4921602 Val_acc =  0.876\n",
            "Iteration  771 : Loss =  0.4278372  Acc:  0.9016 Val_loss =  0.49232578 Val_acc =  0.8776\n",
            "Iteration  772 : Loss =  0.42809397  Acc:  0.90141666 Val_loss =  0.49228686 Val_acc =  0.8768\n",
            "Iteration  773 : Loss =  0.42826718  Acc:  0.9012667 Val_loss =  0.4929036 Val_acc =  0.8777\n",
            "Iteration  774 : Loss =  0.4281743  Acc:  0.9011 Val_loss =  0.49251255 Val_acc =  0.8772\n",
            "Iteration  775 : Loss =  0.42750293  Acc:  0.9016167 Val_loss =  0.49211586 Val_acc =  0.8786\n",
            "Iteration  776 : Loss =  0.4263271  Acc:  0.9022667 Val_loss =  0.49084812 Val_acc =  0.877\n",
            "Iteration  777 : Loss =  0.42532098  Acc:  0.9030667 Val_loss =  0.48997557 Val_acc =  0.8794\n",
            "Iteration  778 : Loss =  0.42480117  Acc:  0.9030333 Val_loss =  0.48949006 Val_acc =  0.8785\n",
            "Iteration  779 : Loss =  0.4249233  Acc:  0.90283334 Val_loss =  0.48968005 Val_acc =  0.8787\n",
            "Iteration  780 : Loss =  0.42553723  Acc:  0.90218335 Val_loss =  0.4904245 Val_acc =  0.8799\n",
            "Iteration  781 : Loss =  0.42641348  Acc:  0.90173334 Val_loss =  0.49119532 Val_acc =  0.8771\n",
            "Iteration  782 : Loss =  0.42707074  Acc:  0.9015833 Val_loss =  0.49216938 Val_acc =  0.8779\n",
            "Iteration  783 : Loss =  0.4273317  Acc:  0.901 Val_loss =  0.49205452 Val_acc =  0.8767\n",
            "Iteration  784 : Loss =  0.42629  Acc:  0.90183336 Val_loss =  0.4914269 Val_acc =  0.8775\n",
            "Iteration  785 : Loss =  0.4251417  Acc:  0.90215 Val_loss =  0.48995912 Val_acc =  0.8777\n",
            "Iteration  786 : Loss =  0.42431742  Acc:  0.90243334 Val_loss =  0.4895363 Val_acc =  0.878\n",
            "Iteration  787 : Loss =  0.42399555  Acc:  0.90283334 Val_loss =  0.4890629 Val_acc =  0.8786\n",
            "Iteration  788 : Loss =  0.42404994  Acc:  0.903 Val_loss =  0.48929968 Val_acc =  0.8776\n",
            "Iteration  789 : Loss =  0.42444015  Acc:  0.90258336 Val_loss =  0.48978484 Val_acc =  0.8785\n",
            "Iteration  790 : Loss =  0.42444843  Acc:  0.90253335 Val_loss =  0.4896214 Val_acc =  0.8775\n",
            "Iteration  791 : Loss =  0.42441326  Acc:  0.90201664 Val_loss =  0.49000812 Val_acc =  0.8781\n",
            "Iteration  792 : Loss =  0.42379504  Acc:  0.9028 Val_loss =  0.48890617 Val_acc =  0.8772\n",
            "Iteration  793 : Loss =  0.42302114  Acc:  0.90313333 Val_loss =  0.4887193 Val_acc =  0.8782\n",
            "Iteration  794 : Loss =  0.42231834  Acc:  0.90323335 Val_loss =  0.48755467 Val_acc =  0.8792\n",
            "Iteration  795 : Loss =  0.42190439  Acc:  0.90285 Val_loss =  0.48767635 Val_acc =  0.8792\n",
            "Iteration  796 : Loss =  0.4219514  Acc:  0.9033167 Val_loss =  0.48747933 Val_acc =  0.8789\n",
            "Iteration  797 : Loss =  0.4224324  Acc:  0.9026167 Val_loss =  0.48820215 Val_acc =  0.8787\n",
            "Iteration  798 : Loss =  0.42336285  Acc:  0.90283334 Val_loss =  0.48914105 Val_acc =  0.8771\n",
            "Iteration  799 : Loss =  0.4244923  Acc:  0.9015 Val_loss =  0.4902323 Val_acc =  0.8774\n",
            "Iteration  800 : Loss =  0.42606494  Acc:  0.90098333 Val_loss =  0.49202073 Val_acc =  0.876\n",
            "Iteration  801 : Loss =  0.42682594  Acc:  0.9007667 Val_loss =  0.4926821 Val_acc =  0.8755\n",
            "Iteration  802 : Loss =  0.4265901  Acc:  0.90073335 Val_loss =  0.49255016 Val_acc =  0.876\n",
            "Iteration  803 : Loss =  0.42420393  Acc:  0.9018 Val_loss =  0.4900493 Val_acc =  0.8775\n",
            "Iteration  804 : Loss =  0.42206478  Acc:  0.9027167 Val_loss =  0.487881 Val_acc =  0.877\n",
            "Iteration  805 : Loss =  0.4208237  Acc:  0.9033167 Val_loss =  0.48690337 Val_acc =  0.8798\n",
            "Iteration  806 : Loss =  0.4208429  Acc:  0.9034 Val_loss =  0.48669392 Val_acc =  0.879\n",
            "Iteration  807 : Loss =  0.42158997  Acc:  0.90285 Val_loss =  0.48799586 Val_acc =  0.8782\n",
            "Iteration  808 : Loss =  0.42199612  Acc:  0.90311664 Val_loss =  0.4880126 Val_acc =  0.8772\n",
            "Iteration  809 : Loss =  0.42271006  Acc:  0.9026833 Val_loss =  0.48913753 Val_acc =  0.8775\n",
            "Iteration  810 : Loss =  0.4222819  Acc:  0.90218335 Val_loss =  0.48858374 Val_acc =  0.8775\n",
            "Iteration  811 : Loss =  0.42189458  Acc:  0.9027 Val_loss =  0.48815858 Val_acc =  0.8773\n",
            "Iteration  812 : Loss =  0.42149335  Acc:  0.902 Val_loss =  0.4881521 Val_acc =  0.8771\n",
            "Iteration  813 : Loss =  0.4210118  Acc:  0.90283334 Val_loss =  0.48723704 Val_acc =  0.8771\n",
            "Iteration  814 : Loss =  0.42013586  Acc:  0.90305 Val_loss =  0.4868583 Val_acc =  0.8795\n",
            "Iteration  815 : Loss =  0.42040172  Acc:  0.9029 Val_loss =  0.48675132 Val_acc =  0.8795\n",
            "Iteration  816 : Loss =  0.42075542  Acc:  0.90285 Val_loss =  0.48748973 Val_acc =  0.8787\n",
            "Iteration  817 : Loss =  0.42061228  Acc:  0.90313333 Val_loss =  0.48713458 Val_acc =  0.8786\n",
            "Iteration  818 : Loss =  0.42011788  Acc:  0.9029667 Val_loss =  0.48692912 Val_acc =  0.8786\n",
            "Iteration  819 : Loss =  0.41883177  Acc:  0.90375 Val_loss =  0.48556578 Val_acc =  0.8791\n",
            "Iteration  820 : Loss =  0.4178016  Acc:  0.90458333 Val_loss =  0.48453254 Val_acc =  0.8804\n",
            "Iteration  821 : Loss =  0.41756895  Acc:  0.90395 Val_loss =  0.48445296 Val_acc =  0.8794\n",
            "Iteration  822 : Loss =  0.41794038  Acc:  0.9040833 Val_loss =  0.48476958 Val_acc =  0.8795\n",
            "Iteration  823 : Loss =  0.41882557  Acc:  0.90288335 Val_loss =  0.48601204 Val_acc =  0.8792\n",
            "Iteration  824 : Loss =  0.42016882  Acc:  0.9023833 Val_loss =  0.4870173 Val_acc =  0.8781\n",
            "Iteration  825 : Loss =  0.41991696  Acc:  0.9025 Val_loss =  0.4872188 Val_acc =  0.8787\n",
            "Iteration  826 : Loss =  0.41924936  Acc:  0.90285 Val_loss =  0.48605835 Val_acc =  0.8789\n",
            "Iteration  827 : Loss =  0.4180171  Acc:  0.9033333 Val_loss =  0.48527178 Val_acc =  0.8799\n",
            "Iteration  828 : Loss =  0.41691232  Acc:  0.90425 Val_loss =  0.48383898 Val_acc =  0.8794\n",
            "Iteration  829 : Loss =  0.4161541  Acc:  0.9040167 Val_loss =  0.48348004 Val_acc =  0.8792\n",
            "Iteration  830 : Loss =  0.41599452  Acc:  0.90491664 Val_loss =  0.48319113 Val_acc =  0.8807\n",
            "Iteration  831 : Loss =  0.4163112  Acc:  0.90431666 Val_loss =  0.48370206 Val_acc =  0.8792\n",
            "Iteration  832 : Loss =  0.41680664  Acc:  0.90398335 Val_loss =  0.48429373 Val_acc =  0.8793\n",
            "Iteration  833 : Loss =  0.41700268  Acc:  0.9040833 Val_loss =  0.48432457 Val_acc =  0.8792\n",
            "Iteration  834 : Loss =  0.41693348  Acc:  0.9038 Val_loss =  0.48460305 Val_acc =  0.8788\n",
            "Iteration  835 : Loss =  0.41629595  Acc:  0.9040167 Val_loss =  0.4835518 Val_acc =  0.8788\n",
            "Iteration  836 : Loss =  0.41553563  Acc:  0.90435 Val_loss =  0.48334634 Val_acc =  0.88\n",
            "Iteration  837 : Loss =  0.41487324  Acc:  0.9049 Val_loss =  0.4822257 Val_acc =  0.8802\n",
            "Iteration  838 : Loss =  0.4144496  Acc:  0.9042 Val_loss =  0.48235065 Val_acc =  0.8797\n",
            "Iteration  839 : Loss =  0.41432428  Acc:  0.90535 Val_loss =  0.48184836 Val_acc =  0.8805\n",
            "Iteration  840 : Loss =  0.41440773  Acc:  0.90451664 Val_loss =  0.48232284 Val_acc =  0.8792\n",
            "Iteration  841 : Loss =  0.4147007  Acc:  0.90528333 Val_loss =  0.4824431 Val_acc =  0.881\n",
            "Iteration  842 : Loss =  0.4151271  Acc:  0.9041333 Val_loss =  0.48303208 Val_acc =  0.879\n",
            "Iteration  843 : Loss =  0.41588616  Acc:  0.90435 Val_loss =  0.4838404 Val_acc =  0.8782\n",
            "Iteration  844 : Loss =  0.4165191  Acc:  0.9031 Val_loss =  0.48445034 Val_acc =  0.8775\n",
            "Iteration  845 : Loss =  0.41760227  Acc:  0.90325 Val_loss =  0.48572174 Val_acc =  0.8771\n",
            "Iteration  846 : Loss =  0.41801858  Acc:  0.9028 Val_loss =  0.4859928 Val_acc =  0.8775\n",
            "Iteration  847 : Loss =  0.41845167  Acc:  0.9023333 Val_loss =  0.48657185 Val_acc =  0.8775\n",
            "Iteration  848 : Loss =  0.4181332  Acc:  0.9026 Val_loss =  0.48634678 Val_acc =  0.8774\n",
            "Iteration  849 : Loss =  0.41688663  Acc:  0.9036667 Val_loss =  0.48487222 Val_acc =  0.8763\n",
            "Iteration  850 : Loss =  0.4145768  Acc:  0.9044167 Val_loss =  0.4828995 Val_acc =  0.8791\n",
            "Iteration  851 : Loss =  0.41361457  Acc:  0.9043 Val_loss =  0.4816254 Val_acc =  0.8801\n",
            "Iteration  852 : Loss =  0.4136948  Acc:  0.90463334 Val_loss =  0.4822324 Val_acc =  0.8792\n",
            "Iteration  853 : Loss =  0.41415137  Acc:  0.9044667 Val_loss =  0.48231566 Val_acc =  0.879\n",
            "Iteration  854 : Loss =  0.41509736  Acc:  0.90435 Val_loss =  0.48380506 Val_acc =  0.8781\n",
            "Iteration  855 : Loss =  0.4148597  Acc:  0.90381664 Val_loss =  0.48320472 Val_acc =  0.878\n",
            "Iteration  856 : Loss =  0.4147642  Acc:  0.90415 Val_loss =  0.48333722 Val_acc =  0.8777\n",
            "Iteration  857 : Loss =  0.41434866  Acc:  0.90416664 Val_loss =  0.48311555 Val_acc =  0.8777\n",
            "Iteration  858 : Loss =  0.41419744  Acc:  0.9039 Val_loss =  0.48253417 Val_acc =  0.8775\n",
            "Iteration  859 : Loss =  0.41405272  Acc:  0.9042 Val_loss =  0.48327357 Val_acc =  0.8784\n",
            "Iteration  860 : Loss =  0.41365325  Acc:  0.90353334 Val_loss =  0.4820363 Val_acc =  0.8792\n",
            "Iteration  861 : Loss =  0.41252422  Acc:  0.9045333 Val_loss =  0.4815954 Val_acc =  0.8795\n",
            "Iteration  862 : Loss =  0.412316  Acc:  0.90508336 Val_loss =  0.480998 Val_acc =  0.8796\n",
            "Iteration  863 : Loss =  0.41260687  Acc:  0.90493333 Val_loss =  0.48145154 Val_acc =  0.8793\n",
            "Iteration  864 : Loss =  0.41254574  Acc:  0.9048 Val_loss =  0.48166505 Val_acc =  0.8795\n",
            "Iteration  865 : Loss =  0.41268727  Acc:  0.9045 Val_loss =  0.48143846 Val_acc =  0.8782\n",
            "Iteration  866 : Loss =  0.4122749  Acc:  0.90433335 Val_loss =  0.48174652 Val_acc =  0.8792\n",
            "Iteration  867 : Loss =  0.41119182  Acc:  0.9051167 Val_loss =  0.4800371 Val_acc =  0.8788\n",
            "Iteration  868 : Loss =  0.41022342  Acc:  0.9055667 Val_loss =  0.47950816 Val_acc =  0.8807\n",
            "Iteration  869 : Loss =  0.41013694  Acc:  0.9051667 Val_loss =  0.47930717 Val_acc =  0.8813\n",
            "Iteration  870 : Loss =  0.41039842  Acc:  0.90526664 Val_loss =  0.47962245 Val_acc =  0.881\n",
            "Iteration  871 : Loss =  0.41074216  Acc:  0.9051833 Val_loss =  0.4802407 Val_acc =  0.8803\n",
            "Iteration  872 : Loss =  0.41097528  Acc:  0.90495 Val_loss =  0.480254 Val_acc =  0.8798\n",
            "Iteration  873 : Loss =  0.41054112  Acc:  0.9051667 Val_loss =  0.48006666 Val_acc =  0.8799\n",
            "Iteration  874 : Loss =  0.41005123  Acc:  0.9055667 Val_loss =  0.47940725 Val_acc =  0.8797\n",
            "Iteration  875 : Loss =  0.40927845  Acc:  0.9055667 Val_loss =  0.47874993 Val_acc =  0.8798\n",
            "Iteration  876 : Loss =  0.4087456  Acc:  0.9059167 Val_loss =  0.47830772 Val_acc =  0.88\n",
            "Iteration  877 : Loss =  0.40847015  Acc:  0.9058167 Val_loss =  0.47800025 Val_acc =  0.8805\n",
            "Iteration  878 : Loss =  0.4083816  Acc:  0.906 Val_loss =  0.47817802 Val_acc =  0.8812\n",
            "Iteration  879 : Loss =  0.40851948  Acc:  0.9059333 Val_loss =  0.47814843 Val_acc =  0.8813\n",
            "Iteration  880 : Loss =  0.40877256  Acc:  0.90531665 Val_loss =  0.47864145 Val_acc =  0.8817\n",
            "Iteration  881 : Loss =  0.40906835  Acc:  0.90525 Val_loss =  0.47892892 Val_acc =  0.881\n",
            "Iteration  882 : Loss =  0.4093106  Acc:  0.9053 Val_loss =  0.47905394 Val_acc =  0.881\n",
            "Iteration  883 : Loss =  0.40931782  Acc:  0.905 Val_loss =  0.47941312 Val_acc =  0.8794\n",
            "Iteration  884 : Loss =  0.40917283  Acc:  0.90506667 Val_loss =  0.4787652 Val_acc =  0.8796\n",
            "Iteration  885 : Loss =  0.40894946  Acc:  0.90543336 Val_loss =  0.47938284 Val_acc =  0.8793\n",
            "Iteration  886 : Loss =  0.4085524  Acc:  0.9051833 Val_loss =  0.47814488 Val_acc =  0.8805\n",
            "Iteration  887 : Loss =  0.40778422  Acc:  0.9061 Val_loss =  0.4783436 Val_acc =  0.8804\n",
            "Iteration  888 : Loss =  0.40703863  Acc:  0.90641665 Val_loss =  0.47682324 Val_acc =  0.8811\n",
            "Iteration  889 : Loss =  0.40652025  Acc:  0.90608335 Val_loss =  0.4769711 Val_acc =  0.8807\n",
            "Iteration  890 : Loss =  0.40639704  Acc:  0.90646666 Val_loss =  0.47660124 Val_acc =  0.8808\n",
            "Iteration  891 : Loss =  0.40661892  Acc:  0.9062833 Val_loss =  0.47696593 Val_acc =  0.8803\n",
            "Iteration  892 : Loss =  0.4071942  Acc:  0.9062167 Val_loss =  0.47771013 Val_acc =  0.8798\n",
            "Iteration  893 : Loss =  0.40779573  Acc:  0.9054667 Val_loss =  0.47805738 Val_acc =  0.8796\n",
            "Iteration  894 : Loss =  0.40879044  Acc:  0.90541667 Val_loss =  0.4794669 Val_acc =  0.8794\n",
            "Iteration  895 : Loss =  0.4091103  Acc:  0.905 Val_loss =  0.47945604 Val_acc =  0.8797\n",
            "Iteration  896 : Loss =  0.4095831  Acc:  0.90485 Val_loss =  0.48018554 Val_acc =  0.8786\n",
            "Iteration  897 : Loss =  0.40915293  Acc:  0.90515 Val_loss =  0.47985566 Val_acc =  0.8785\n",
            "Iteration  898 : Loss =  0.40829152  Acc:  0.90543336 Val_loss =  0.4787082 Val_acc =  0.8785\n",
            "Iteration  899 : Loss =  0.40720195  Acc:  0.9055167 Val_loss =  0.47827905 Val_acc =  0.8788\n",
            "Iteration  900 : Loss =  0.40632534  Acc:  0.90603334 Val_loss =  0.47667083 Val_acc =  0.8802\n",
            "Iteration  901 : Loss =  0.40616477  Acc:  0.90611666 Val_loss =  0.47738034 Val_acc =  0.8799\n",
            "Iteration  902 : Loss =  0.40663224  Acc:  0.9054833 Val_loss =  0.4771855 Val_acc =  0.8811\n",
            "Iteration  903 : Loss =  0.40741986  Acc:  0.9051333 Val_loss =  0.47867572 Val_acc =  0.8782\n",
            "Iteration  904 : Loss =  0.40770808  Acc:  0.90563333 Val_loss =  0.47865131 Val_acc =  0.8806\n",
            "Iteration  905 : Loss =  0.40797958  Acc:  0.90451664 Val_loss =  0.47908694 Val_acc =  0.8784\n",
            "Iteration  906 : Loss =  0.40663466  Acc:  0.90566665 Val_loss =  0.4778688 Val_acc =  0.8805\n",
            "Iteration  907 : Loss =  0.4052174  Acc:  0.90635 Val_loss =  0.47617328 Val_acc =  0.8807\n",
            "Iteration  908 : Loss =  0.40397948  Acc:  0.9066167 Val_loss =  0.4754501 Val_acc =  0.881\n",
            "Iteration  909 : Loss =  0.40366963  Acc:  0.907 Val_loss =  0.47477412 Val_acc =  0.8823\n",
            "Iteration  910 : Loss =  0.40433958  Acc:  0.90648335 Val_loss =  0.47597563 Val_acc =  0.8809\n",
            "Iteration  911 : Loss =  0.40572315  Acc:  0.90563333 Val_loss =  0.47701442 Val_acc =  0.8812\n",
            "Iteration  912 : Loss =  0.4070684  Acc:  0.90445 Val_loss =  0.47879398 Val_acc =  0.8782\n",
            "Iteration  913 : Loss =  0.40750846  Acc:  0.9043 Val_loss =  0.47888 Val_acc =  0.8798\n",
            "Iteration  914 : Loss =  0.4064364  Acc:  0.9048833 Val_loss =  0.47808853 Val_acc =  0.8793\n",
            "Iteration  915 : Loss =  0.40447566  Acc:  0.9055833 Val_loss =  0.47591087 Val_acc =  0.8815\n",
            "Iteration  916 : Loss =  0.40279603  Acc:  0.90688336 Val_loss =  0.47449332 Val_acc =  0.8811\n",
            "Iteration  917 : Loss =  0.40230194  Acc:  0.90748334 Val_loss =  0.47396246 Val_acc =  0.8813\n",
            "Iteration  918 : Loss =  0.4028774  Acc:  0.90695 Val_loss =  0.47477424 Val_acc =  0.8818\n",
            "Iteration  919 : Loss =  0.40397352  Acc:  0.9062333 Val_loss =  0.47589839 Val_acc =  0.8793\n",
            "Iteration  920 : Loss =  0.40447223  Acc:  0.90643334 Val_loss =  0.4764347 Val_acc =  0.8812\n",
            "Iteration  921 : Loss =  0.4046194  Acc:  0.9055833 Val_loss =  0.47673577 Val_acc =  0.879\n",
            "Iteration  922 : Loss =  0.40349776  Acc:  0.9066 Val_loss =  0.4753478 Val_acc =  0.8814\n",
            "Iteration  923 : Loss =  0.4023674  Acc:  0.9068 Val_loss =  0.47459853 Val_acc =  0.8806\n",
            "Iteration  924 : Loss =  0.4013679  Acc:  0.9076667 Val_loss =  0.47322425 Val_acc =  0.883\n",
            "Iteration  925 : Loss =  0.40098637  Acc:  0.9072 Val_loss =  0.4733637 Val_acc =  0.8821\n",
            "Iteration  926 : Loss =  0.40114486  Acc:  0.90755 Val_loss =  0.47323376 Val_acc =  0.8821\n",
            "Iteration  927 : Loss =  0.4015343  Acc:  0.90708333 Val_loss =  0.47397465 Val_acc =  0.8829\n",
            "Iteration  928 : Loss =  0.40198013  Acc:  0.9069667 Val_loss =  0.4743606 Val_acc =  0.8804\n",
            "Iteration  929 : Loss =  0.40205884  Acc:  0.90706664 Val_loss =  0.47445726 Val_acc =  0.8812\n",
            "Iteration  930 : Loss =  0.40220243  Acc:  0.90665 Val_loss =  0.47482556 Val_acc =  0.8802\n",
            "Iteration  931 : Loss =  0.40184337  Acc:  0.90695 Val_loss =  0.47416914 Val_acc =  0.8818\n",
            "Iteration  932 : Loss =  0.40177295  Acc:  0.9072667 Val_loss =  0.47452414 Val_acc =  0.8811\n",
            "Iteration  933 : Loss =  0.40182936  Acc:  0.9073 Val_loss =  0.4742905 Val_acc =  0.8802\n",
            "Iteration  934 : Loss =  0.4027118  Acc:  0.9062833 Val_loss =  0.47534537 Val_acc =  0.8794\n",
            "Iteration  935 : Loss =  0.4048797  Acc:  0.90541667 Val_loss =  0.47797415 Val_acc =  0.8783\n",
            "Iteration  936 : Loss =  0.4073913  Acc:  0.90395 Val_loss =  0.47975427 Val_acc =  0.8752\n",
            "Iteration  937 : Loss =  0.4065337  Acc:  0.9046 Val_loss =  0.479923 Val_acc =  0.878\n",
            "Iteration  938 : Loss =  0.40497613  Acc:  0.90498334 Val_loss =  0.47719383 Val_acc =  0.8771\n",
            "Iteration  939 : Loss =  0.4029557  Acc:  0.90566665 Val_loss =  0.47642604 Val_acc =  0.8795\n",
            "Iteration  940 : Loss =  0.40079337  Acc:  0.90686667 Val_loss =  0.4735186 Val_acc =  0.8812\n",
            "Iteration  941 : Loss =  0.39910513  Acc:  0.9075 Val_loss =  0.4723222 Val_acc =  0.8828\n",
            "Iteration  942 : Loss =  0.39920753  Acc:  0.90793335 Val_loss =  0.4723801 Val_acc =  0.8814\n",
            "Iteration  943 : Loss =  0.40148026  Acc:  0.9066833 Val_loss =  0.47423488 Val_acc =  0.8807\n",
            "Iteration  944 : Loss =  0.40509248  Acc:  0.9046 Val_loss =  0.47902626 Val_acc =  0.8774\n",
            "Iteration  945 : Loss =  0.40996212  Acc:  0.9025667 Val_loss =  0.48262167 Val_acc =  0.8754\n",
            "Iteration  946 : Loss =  0.4076268  Acc:  0.90353334 Val_loss =  0.48185956 Val_acc =  0.8772\n",
            "Iteration  947 : Loss =  0.40276742  Acc:  0.90541667 Val_loss =  0.47616062 Val_acc =  0.8778\n",
            "Iteration  948 : Loss =  0.4002086  Acc:  0.90706664 Val_loss =  0.47373593 Val_acc =  0.8817\n",
            "Iteration  949 : Loss =  0.403112  Acc:  0.90515 Val_loss =  0.47762728 Val_acc =  0.8782\n",
            "Iteration  950 : Loss =  0.40556616  Acc:  0.9046 Val_loss =  0.4783894 Val_acc =  0.8782\n",
            "Iteration  951 : Loss =  0.40117228  Acc:  0.9063 Val_loss =  0.4750517 Val_acc =  0.8802\n",
            "Iteration  952 : Loss =  0.3978762  Acc:  0.9076167 Val_loss =  0.4715092 Val_acc =  0.8833\n",
            "Iteration  953 : Loss =  0.39875293  Acc:  0.90755 Val_loss =  0.47204357 Val_acc =  0.8823\n",
            "Iteration  954 : Loss =  0.40199244  Acc:  0.90538335 Val_loss =  0.4767843 Val_acc =  0.8789\n",
            "Iteration  955 : Loss =  0.4050929  Acc:  0.9046 Val_loss =  0.47826952 Val_acc =  0.8774\n",
            "Iteration  956 : Loss =  0.40250453  Acc:  0.90526664 Val_loss =  0.4770292 Val_acc =  0.8783\n",
            "Iteration  957 : Loss =  0.39910343  Acc:  0.90671664 Val_loss =  0.47331664 Val_acc =  0.8815\n",
            "Iteration  958 : Loss =  0.39856398  Acc:  0.90746665 Val_loss =  0.47236305 Val_acc =  0.882\n",
            "Iteration  959 : Loss =  0.40030977  Acc:  0.90638334 Val_loss =  0.47546238 Val_acc =  0.8795\n",
            "Iteration  960 : Loss =  0.40047094  Acc:  0.9059 Val_loss =  0.4740899 Val_acc =  0.88\n",
            "Iteration  961 : Loss =  0.39774287  Acc:  0.9079 Val_loss =  0.47186333 Val_acc =  0.8811\n",
            "Iteration  962 : Loss =  0.3971094  Acc:  0.9083 Val_loss =  0.47175562 Val_acc =  0.8822\n",
            "Iteration  963 : Loss =  0.39921817  Acc:  0.90713334 Val_loss =  0.47277215 Val_acc =  0.8812\n",
            "Iteration  964 : Loss =  0.4007181  Acc:  0.90563333 Val_loss =  0.4760354 Val_acc =  0.8797\n",
            "Iteration  965 : Loss =  0.40141183  Acc:  0.90566665 Val_loss =  0.4752646 Val_acc =  0.8779\n",
            "Iteration  966 : Loss =  0.3994992  Acc:  0.90686667 Val_loss =  0.47407565 Val_acc =  0.8794\n",
            "Iteration  967 : Loss =  0.39847994  Acc:  0.90718335 Val_loss =  0.47376907 Val_acc =  0.8813\n",
            "Iteration  968 : Loss =  0.39867437  Acc:  0.90688336 Val_loss =  0.47285843 Val_acc =  0.8814\n",
            "Iteration  969 : Loss =  0.3981502  Acc:  0.9071 Val_loss =  0.47373396 Val_acc =  0.8796\n",
            "Iteration  970 : Loss =  0.39737326  Acc:  0.9072833 Val_loss =  0.4719165 Val_acc =  0.8817\n",
            "Iteration  971 : Loss =  0.3971091  Acc:  0.90781665 Val_loss =  0.47123337 Val_acc =  0.8813\n",
            "Iteration  972 : Loss =  0.39712346  Acc:  0.9076 Val_loss =  0.47257787 Val_acc =  0.8812\n",
            "Iteration  973 : Loss =  0.39701664  Acc:  0.90793335 Val_loss =  0.4711665 Val_acc =  0.8815\n",
            "Iteration  974 : Loss =  0.39652166  Acc:  0.9077167 Val_loss =  0.4717471 Val_acc =  0.8809\n",
            "Iteration  975 : Loss =  0.39720792  Acc:  0.90751666 Val_loss =  0.4721812 Val_acc =  0.8815\n",
            "Iteration  976 : Loss =  0.3975299  Acc:  0.9081333 Val_loss =  0.47210813 Val_acc =  0.8802\n",
            "Iteration  977 : Loss =  0.39643672  Acc:  0.90791667 Val_loss =  0.4721386 Val_acc =  0.8814\n",
            "Iteration  978 : Loss =  0.39462453  Acc:  0.90886664 Val_loss =  0.46969932 Val_acc =  0.8831\n",
            "Iteration  979 : Loss =  0.39453334  Acc:  0.9087 Val_loss =  0.46962708 Val_acc =  0.8816\n",
            "Iteration  980 : Loss =  0.39578587  Acc:  0.9080833 Val_loss =  0.47107852 Val_acc =  0.8824\n",
            "Iteration  981 : Loss =  0.3959594  Acc:  0.90825 Val_loss =  0.47063226 Val_acc =  0.8808\n",
            "Iteration  982 : Loss =  0.39457625  Acc:  0.90845 Val_loss =  0.4701317 Val_acc =  0.884\n",
            "Iteration  983 : Loss =  0.39336747  Acc:  0.90898335 Val_loss =  0.468867 Val_acc =  0.8826\n",
            "Iteration  984 : Loss =  0.3936085  Acc:  0.9089 Val_loss =  0.46877426 Val_acc =  0.8823\n",
            "Iteration  985 : Loss =  0.39429423  Acc:  0.9087833 Val_loss =  0.47010818 Val_acc =  0.8829\n",
            "Iteration  986 : Loss =  0.39399058  Acc:  0.90886664 Val_loss =  0.46922195 Val_acc =  0.8821\n",
            "Iteration  987 : Loss =  0.3932427  Acc:  0.90925 Val_loss =  0.46880892 Val_acc =  0.8823\n",
            "Iteration  988 : Loss =  0.39306813  Acc:  0.9092 Val_loss =  0.46890652 Val_acc =  0.8819\n",
            "Iteration  989 : Loss =  0.39327985  Acc:  0.90928334 Val_loss =  0.46845013 Val_acc =  0.8829\n",
            "Iteration  990 : Loss =  0.39337325  Acc:  0.9087167 Val_loss =  0.46935347 Val_acc =  0.8838\n",
            "Iteration  991 : Loss =  0.39340994  Acc:  0.90893334 Val_loss =  0.4690128 Val_acc =  0.8822\n",
            "Iteration  992 : Loss =  0.39389408  Acc:  0.90925 Val_loss =  0.46958727 Val_acc =  0.8828\n",
            "Iteration  993 : Loss =  0.3953932  Acc:  0.9076 Val_loss =  0.47155803 Val_acc =  0.8802\n",
            "Iteration  994 : Loss =  0.39625823  Acc:  0.90833336 Val_loss =  0.47177526 Val_acc =  0.8822\n",
            "Iteration  995 : Loss =  0.39795035  Acc:  0.9066667 Val_loss =  0.47395843 Val_acc =  0.8786\n",
            "Iteration  996 : Loss =  0.39832383  Acc:  0.90665 Val_loss =  0.4744256 Val_acc =  0.8805\n",
            "Iteration  997 : Loss =  0.39909476  Acc:  0.9058 Val_loss =  0.47466508 Val_acc =  0.8776\n",
            "Iteration  998 : Loss =  0.3972116  Acc:  0.90713334 Val_loss =  0.4736758 Val_acc =  0.8809\n",
            "Iteration  999 : Loss =  0.39381588  Acc:  0.90821666 Val_loss =  0.46939588 Val_acc =  0.8809\n",
            "Iteration  1000 : Loss =  0.39138326  Acc:  0.90956664 Val_loss =  0.46725196 Val_acc =  0.8839\n",
            "Iteration  1001 : Loss =  0.3914395  Acc:  0.90935 Val_loss =  0.46779877 Val_acc =  0.8827\n",
            "Iteration  1002 : Loss =  0.3928231  Acc:  0.90856665 Val_loss =  0.4685747 Val_acc =  0.8815\n",
            "Iteration  1003 : Loss =  0.39415532  Acc:  0.9080167 Val_loss =  0.47083738 Val_acc =  0.88\n",
            "Iteration  1004 : Loss =  0.39502168  Acc:  0.90755 Val_loss =  0.47135422 Val_acc =  0.8793\n",
            "Iteration  1005 : Loss =  0.39484036  Acc:  0.90845 Val_loss =  0.47109097 Val_acc =  0.882\n",
            "Iteration  1006 : Loss =  0.3947481  Acc:  0.9075 Val_loss =  0.47152027 Val_acc =  0.8799\n",
            "Iteration  1007 : Loss =  0.39288235  Acc:  0.90915 Val_loss =  0.46899968 Val_acc =  0.8818\n",
            "Iteration  1008 : Loss =  0.39159265  Acc:  0.90903336 Val_loss =  0.46810946 Val_acc =  0.8817\n",
            "Iteration  1009 : Loss =  0.39100093  Acc:  0.9091833 Val_loss =  0.467589 Val_acc =  0.8835\n",
            "Iteration  1010 : Loss =  0.39086485  Acc:  0.9098 Val_loss =  0.46728635 Val_acc =  0.8822\n",
            "Iteration  1011 : Loss =  0.39088196  Acc:  0.90963334 Val_loss =  0.46778196 Val_acc =  0.8815\n",
            "Iteration  1012 : Loss =  0.39130622  Acc:  0.90933335 Val_loss =  0.468094 Val_acc =  0.8832\n",
            "Iteration  1013 : Loss =  0.39247355  Acc:  0.90853333 Val_loss =  0.46915662 Val_acc =  0.8808\n",
            "Iteration  1014 : Loss =  0.3927487  Acc:  0.90861666 Val_loss =  0.46974075 Val_acc =  0.881\n",
            "Iteration  1015 : Loss =  0.39235404  Acc:  0.9084333 Val_loss =  0.46919623 Val_acc =  0.8809\n",
            "Iteration  1016 : Loss =  0.39074254  Acc:  0.90973336 Val_loss =  0.4676682 Val_acc =  0.8833\n",
            "Iteration  1017 : Loss =  0.3894773  Acc:  0.9095167 Val_loss =  0.46662346 Val_acc =  0.8822\n",
            "Iteration  1018 : Loss =  0.3888939  Acc:  0.91026664 Val_loss =  0.465693 Val_acc =  0.8834\n",
            "Iteration  1019 : Loss =  0.3888927  Acc:  0.9098167 Val_loss =  0.46613932 Val_acc =  0.8843\n",
            "Iteration  1020 : Loss =  0.3892486  Acc:  0.9098667 Val_loss =  0.4664562 Val_acc =  0.8823\n",
            "Iteration  1021 : Loss =  0.38973728  Acc:  0.9102833 Val_loss =  0.46681246 Val_acc =  0.8834\n",
            "Iteration  1022 : Loss =  0.39019674  Acc:  0.9090667 Val_loss =  0.46766347 Val_acc =  0.8813\n",
            "Iteration  1023 : Loss =  0.38995558  Acc:  0.90995 Val_loss =  0.46693516 Val_acc =  0.8837\n",
            "Iteration  1024 : Loss =  0.38962018  Acc:  0.90933335 Val_loss =  0.46703604 Val_acc =  0.881\n",
            "Iteration  1025 : Loss =  0.38905215  Acc:  0.9101 Val_loss =  0.46648765 Val_acc =  0.8826\n",
            "Iteration  1026 : Loss =  0.38882804  Acc:  0.90968335 Val_loss =  0.46604425 Val_acc =  0.8815\n",
            "Iteration  1027 : Loss =  0.38875324  Acc:  0.9098833 Val_loss =  0.46654624 Val_acc =  0.8814\n",
            "Iteration  1028 : Loss =  0.38845336  Acc:  0.90973336 Val_loss =  0.46574736 Val_acc =  0.8824\n",
            "Iteration  1029 : Loss =  0.38804245  Acc:  0.91008335 Val_loss =  0.46580547 Val_acc =  0.882\n",
            "Iteration  1030 : Loss =  0.3876791  Acc:  0.91015 Val_loss =  0.46528995 Val_acc =  0.8835\n",
            "Iteration  1031 : Loss =  0.38736957  Acc:  0.9102 Val_loss =  0.46496767 Val_acc =  0.8832\n",
            "Iteration  1032 : Loss =  0.3870613  Acc:  0.91015 Val_loss =  0.46489686 Val_acc =  0.8835\n",
            "Iteration  1033 : Loss =  0.38680017  Acc:  0.9101167 Val_loss =  0.4645264 Val_acc =  0.8831\n",
            "Iteration  1034 : Loss =  0.38667035  Acc:  0.9105167 Val_loss =  0.46457022 Val_acc =  0.8838\n",
            "Iteration  1035 : Loss =  0.3867296  Acc:  0.9105667 Val_loss =  0.4646794 Val_acc =  0.8827\n",
            "Iteration  1036 : Loss =  0.38691115  Acc:  0.91083336 Val_loss =  0.4648196 Val_acc =  0.8827\n",
            "Iteration  1037 : Loss =  0.38717687  Acc:  0.91013336 Val_loss =  0.46518636 Val_acc =  0.8821\n",
            "Iteration  1038 : Loss =  0.3875521  Acc:  0.91025 Val_loss =  0.46568298 Val_acc =  0.8817\n",
            "Iteration  1039 : Loss =  0.3879742  Acc:  0.9096 Val_loss =  0.46593693 Val_acc =  0.8809\n",
            "Iteration  1040 : Loss =  0.3884309  Acc:  0.9095167 Val_loss =  0.4667882 Val_acc =  0.8811\n",
            "Iteration  1041 : Loss =  0.38883728  Acc:  0.90896666 Val_loss =  0.46679553 Val_acc =  0.8813\n",
            "Iteration  1042 : Loss =  0.38911393  Acc:  0.90995 Val_loss =  0.46755528 Val_acc =  0.8816\n",
            "Iteration  1043 : Loss =  0.38939857  Acc:  0.9087 Val_loss =  0.4675601 Val_acc =  0.8804\n",
            "Iteration  1044 : Loss =  0.38897628  Acc:  0.90998334 Val_loss =  0.46729356 Val_acc =  0.8823\n",
            "Iteration  1045 : Loss =  0.38881013  Acc:  0.90926665 Val_loss =  0.46720746 Val_acc =  0.8804\n",
            "Iteration  1046 : Loss =  0.3877064  Acc:  0.91033334 Val_loss =  0.46592432 Val_acc =  0.8829\n",
            "Iteration  1047 : Loss =  0.38696116  Acc:  0.91015 Val_loss =  0.46554255 Val_acc =  0.8809\n",
            "Iteration  1048 : Loss =  0.38618168  Acc:  0.91081667 Val_loss =  0.46452937 Val_acc =  0.8838\n",
            "Iteration  1049 : Loss =  0.3857945  Acc:  0.9106 Val_loss =  0.46449262 Val_acc =  0.8823\n",
            "Iteration  1050 : Loss =  0.38569218  Acc:  0.91036665 Val_loss =  0.46429133 Val_acc =  0.8834\n",
            "Iteration  1051 : Loss =  0.385891  Acc:  0.9105833 Val_loss =  0.46473354 Val_acc =  0.8811\n",
            "Iteration  1052 : Loss =  0.38630545  Acc:  0.91001666 Val_loss =  0.465024 Val_acc =  0.8813\n",
            "Iteration  1053 : Loss =  0.38692796  Acc:  0.91015 Val_loss =  0.46589756 Val_acc =  0.8816\n",
            "Iteration  1054 : Loss =  0.38771403  Acc:  0.90925 Val_loss =  0.46645415 Val_acc =  0.881\n",
            "Iteration  1055 : Loss =  0.3880921  Acc:  0.91006666 Val_loss =  0.4671329 Val_acc =  0.8814\n",
            "Iteration  1056 : Loss =  0.38850802  Acc:  0.90893334 Val_loss =  0.46733618 Val_acc =  0.8804\n",
            "Iteration  1057 : Loss =  0.38756955  Acc:  0.91006666 Val_loss =  0.46653777 Val_acc =  0.8823\n",
            "Iteration  1058 : Loss =  0.3867787  Acc:  0.90958333 Val_loss =  0.46581614 Val_acc =  0.8809\n",
            "Iteration  1059 : Loss =  0.38530266  Acc:  0.911 Val_loss =  0.46420163 Val_acc =  0.8838\n",
            "Iteration  1060 : Loss =  0.38446242  Acc:  0.9108 Val_loss =  0.4637094 Val_acc =  0.8828\n",
            "Iteration  1061 : Loss =  0.38423732  Acc:  0.9106167 Val_loss =  0.46318722 Val_acc =  0.883\n",
            "Iteration  1062 : Loss =  0.38460636  Acc:  0.9109 Val_loss =  0.46403986 Val_acc =  0.8812\n",
            "Iteration  1063 : Loss =  0.38528675  Acc:  0.9102333 Val_loss =  0.4644088 Val_acc =  0.8813\n",
            "Iteration  1064 : Loss =  0.38611484  Acc:  0.91006666 Val_loss =  0.46562713 Val_acc =  0.8808\n",
            "Iteration  1065 : Loss =  0.3866968  Acc:  0.9094833 Val_loss =  0.46597633 Val_acc =  0.8809\n",
            "Iteration  1066 : Loss =  0.3865051  Acc:  0.91041666 Val_loss =  0.46598935 Val_acc =  0.8814\n",
            "Iteration  1067 : Loss =  0.38590232  Acc:  0.9097667 Val_loss =  0.4652572 Val_acc =  0.8805\n",
            "Iteration  1068 : Loss =  0.3845771  Acc:  0.9109167 Val_loss =  0.46410203 Val_acc =  0.8833\n",
            "Iteration  1069 : Loss =  0.3835591  Acc:  0.91103333 Val_loss =  0.4630272 Val_acc =  0.8817\n",
            "Iteration  1070 : Loss =  0.38297954  Acc:  0.91108334 Val_loss =  0.46252748 Val_acc =  0.8841\n",
            "Iteration  1071 : Loss =  0.38307494  Acc:  0.9112833 Val_loss =  0.46280864 Val_acc =  0.8817\n",
            "Iteration  1072 : Loss =  0.3836401  Acc:  0.91068333 Val_loss =  0.4632157 Val_acc =  0.8828\n",
            "Iteration  1073 : Loss =  0.38443106  Acc:  0.91073334 Val_loss =  0.46449852 Val_acc =  0.8808\n",
            "Iteration  1074 : Loss =  0.3849755  Acc:  0.91038334 Val_loss =  0.46453053 Val_acc =  0.8808\n",
            "Iteration  1075 : Loss =  0.38501793  Acc:  0.91033334 Val_loss =  0.4652086 Val_acc =  0.8809\n",
            "Iteration  1076 : Loss =  0.38420427  Acc:  0.91045 Val_loss =  0.4639165 Val_acc =  0.8811\n",
            "Iteration  1077 : Loss =  0.38306785  Acc:  0.91116667 Val_loss =  0.4631428 Val_acc =  0.8812\n",
            "Iteration  1078 : Loss =  0.3821259  Acc:  0.91138333 Val_loss =  0.46206468 Val_acc =  0.883\n",
            "Iteration  1079 : Loss =  0.38165474  Acc:  0.91188335 Val_loss =  0.461536 Val_acc =  0.8841\n",
            "Iteration  1080 : Loss =  0.38175422  Acc:  0.91153336 Val_loss =  0.46198547 Val_acc =  0.8824\n",
            "Iteration  1081 : Loss =  0.38237792  Acc:  0.91108334 Val_loss =  0.46233737 Val_acc =  0.8843\n",
            "Iteration  1082 : Loss =  0.38324302  Acc:  0.9112 Val_loss =  0.4637221 Val_acc =  0.881\n",
            "Iteration  1083 : Loss =  0.38402343  Acc:  0.91038334 Val_loss =  0.46406215 Val_acc =  0.8819\n",
            "Iteration  1084 : Loss =  0.38406152  Acc:  0.9104667 Val_loss =  0.46460974 Val_acc =  0.8809\n",
            "Iteration  1085 : Loss =  0.3833608  Acc:  0.9105167 Val_loss =  0.46355343 Val_acc =  0.8827\n",
            "Iteration  1086 : Loss =  0.3821144  Acc:  0.9112167 Val_loss =  0.4626915 Val_acc =  0.8817\n",
            "Iteration  1087 : Loss =  0.3810968  Acc:  0.9116 Val_loss =  0.4614612 Val_acc =  0.8847\n",
            "Iteration  1088 : Loss =  0.38074142  Acc:  0.91171664 Val_loss =  0.461312 Val_acc =  0.8826\n",
            "Iteration  1089 : Loss =  0.38093016  Acc:  0.91171664 Val_loss =  0.46149224 Val_acc =  0.8825\n",
            "Iteration  1090 : Loss =  0.3814692  Acc:  0.9116667 Val_loss =  0.4620539 Val_acc =  0.8827\n",
            "Iteration  1091 : Loss =  0.38213685  Acc:  0.91143334 Val_loss =  0.46293953 Val_acc =  0.8813\n",
            "Iteration  1092 : Loss =  0.38256365  Acc:  0.9106333 Val_loss =  0.4630949 Val_acc =  0.8814\n",
            "Iteration  1093 : Loss =  0.38263506  Acc:  0.9113167 Val_loss =  0.46362048 Val_acc =  0.881\n",
            "Iteration  1094 : Loss =  0.3821629  Acc:  0.91106665 Val_loss =  0.46268007 Val_acc =  0.882\n",
            "Iteration  1095 : Loss =  0.3814537  Acc:  0.91143334 Val_loss =  0.46250334 Val_acc =  0.8815\n",
            "Iteration  1096 : Loss =  0.3805883  Acc:  0.9115833 Val_loss =  0.46136293 Val_acc =  0.883\n",
            "Iteration  1097 : Loss =  0.3799633  Acc:  0.91171664 Val_loss =  0.46092844 Val_acc =  0.8841\n",
            "Iteration  1098 : Loss =  0.37985015  Acc:  0.9120333 Val_loss =  0.46090204 Val_acc =  0.8817\n",
            "Iteration  1099 : Loss =  0.38021642  Acc:  0.9119167 Val_loss =  0.46103162 Val_acc =  0.8843\n",
            "Iteration  1100 : Loss =  0.38116765  Acc:  0.91153336 Val_loss =  0.46256226 Val_acc =  0.8802\n",
            "Iteration  1101 : Loss =  0.38245052  Acc:  0.91026664 Val_loss =  0.46326816 Val_acc =  0.8813\n",
            "Iteration  1102 : Loss =  0.3832826  Acc:  0.9101 Val_loss =  0.46490592 Val_acc =  0.8808\n",
            "Iteration  1103 : Loss =  0.38325033  Acc:  0.91026664 Val_loss =  0.46421456 Val_acc =  0.8813\n",
            "Iteration  1104 : Loss =  0.3822844  Acc:  0.91076666 Val_loss =  0.46380544 Val_acc =  0.8805\n",
            "Iteration  1105 : Loss =  0.38109916  Acc:  0.9117333 Val_loss =  0.46241432 Val_acc =  0.8823\n",
            "Iteration  1106 : Loss =  0.38101417  Acc:  0.91135 Val_loss =  0.46237838 Val_acc =  0.8806\n",
            "Iteration  1107 : Loss =  0.3815251  Acc:  0.9117 Val_loss =  0.46315745 Val_acc =  0.8815\n",
            "Iteration  1108 : Loss =  0.382847  Acc:  0.9098333 Val_loss =  0.46414703 Val_acc =  0.8805\n",
            "Iteration  1109 : Loss =  0.38397288  Acc:  0.9102 Val_loss =  0.46592623 Val_acc =  0.8806\n",
            "Iteration  1110 : Loss =  0.3832391  Acc:  0.90961665 Val_loss =  0.46458715 Val_acc =  0.879\n",
            "Iteration  1111 : Loss =  0.38049972  Acc:  0.9121 Val_loss =  0.46227914 Val_acc =  0.8812\n",
            "Iteration  1112 : Loss =  0.3782498  Acc:  0.9124 Val_loss =  0.4597375 Val_acc =  0.8829\n",
            "Iteration  1113 : Loss =  0.37784544  Acc:  0.9127667 Val_loss =  0.45930848 Val_acc =  0.8827\n",
            "Iteration  1114 : Loss =  0.37896726  Acc:  0.9125 Val_loss =  0.460919 Val_acc =  0.8812\n",
            "Iteration  1115 : Loss =  0.380686  Acc:  0.91146666 Val_loss =  0.46223176 Val_acc =  0.8809\n",
            "Iteration  1116 : Loss =  0.38163865  Acc:  0.9111 Val_loss =  0.4638182 Val_acc =  0.881\n",
            "Iteration  1117 : Loss =  0.38080916  Acc:  0.9108667 Val_loss =  0.4626417 Val_acc =  0.8809\n",
            "Iteration  1118 : Loss =  0.37911028  Acc:  0.91228336 Val_loss =  0.46118075 Val_acc =  0.8821\n",
            "Iteration  1119 : Loss =  0.3783661  Acc:  0.91246665 Val_loss =  0.46043563 Val_acc =  0.882\n",
            "Iteration  1120 : Loss =  0.37860042  Acc:  0.91235 Val_loss =  0.4604091 Val_acc =  0.8841\n",
            "Iteration  1121 : Loss =  0.38011056  Acc:  0.91151667 Val_loss =  0.46263748 Val_acc =  0.8808\n",
            "Iteration  1122 : Loss =  0.38220778  Acc:  0.9098667 Val_loss =  0.46413338 Val_acc =  0.88\n",
            "Iteration  1123 : Loss =  0.38161558  Acc:  0.91055 Val_loss =  0.4642672 Val_acc =  0.8812\n",
            "Iteration  1124 : Loss =  0.37952167  Acc:  0.91176665 Val_loss =  0.46163595 Val_acc =  0.8827\n",
            "Iteration  1125 : Loss =  0.37785816  Acc:  0.91241664 Val_loss =  0.4601491 Val_acc =  0.8812\n",
            "Iteration  1126 : Loss =  0.3778811  Acc:  0.91286665 Val_loss =  0.46032408 Val_acc =  0.882\n",
            "Iteration  1127 : Loss =  0.37919343  Acc:  0.9120167 Val_loss =  0.46141925 Val_acc =  0.8819\n",
            "Iteration  1128 : Loss =  0.38081837  Acc:  0.91108334 Val_loss =  0.46377784 Val_acc =  0.8808\n",
            "Iteration  1129 : Loss =  0.3810547  Acc:  0.91043335 Val_loss =  0.4634297 Val_acc =  0.8809\n",
            "Iteration  1130 : Loss =  0.37816697  Acc:  0.91246665 Val_loss =  0.46108624 Val_acc =  0.8807\n",
            "Iteration  1131 : Loss =  0.37595874  Acc:  0.9132 Val_loss =  0.45841432 Val_acc =  0.883\n",
            "Iteration  1132 : Loss =  0.37618336  Acc:  0.91326666 Val_loss =  0.45849115 Val_acc =  0.8831\n",
            "Iteration  1133 : Loss =  0.3780467  Acc:  0.9124333 Val_loss =  0.46098626 Val_acc =  0.8816\n",
            "Iteration  1134 : Loss =  0.38003242  Acc:  0.91105 Val_loss =  0.46236908 Val_acc =  0.8802\n",
            "Iteration  1135 : Loss =  0.38001117  Acc:  0.9114 Val_loss =  0.4633492 Val_acc =  0.8813\n",
            "Iteration  1136 : Loss =  0.37780336  Acc:  0.9120167 Val_loss =  0.46072096 Val_acc =  0.8817\n",
            "Iteration  1137 : Loss =  0.37565106  Acc:  0.9131333 Val_loss =  0.45869976 Val_acc =  0.8831\n",
            "Iteration  1138 : Loss =  0.37615538  Acc:  0.91305 Val_loss =  0.4592933 Val_acc =  0.8807\n",
            "Iteration  1139 : Loss =  0.37896007  Acc:  0.9112 Val_loss =  0.4612496 Val_acc =  0.881\n",
            "Iteration  1140 : Loss =  0.38221487  Acc:  0.9096 Val_loss =  0.46595946 Val_acc =  0.8796\n",
            "Iteration  1141 : Loss =  0.38513267  Acc:  0.9075 Val_loss =  0.4676231 Val_acc =  0.8763\n",
            "Iteration  1142 : Loss =  0.3806548  Acc:  0.91041666 Val_loss =  0.46458483 Val_acc =  0.8801\n",
            "Iteration  1143 : Loss =  0.37742674  Acc:  0.91225 Val_loss =  0.46092844 Val_acc =  0.8827\n",
            "Iteration  1144 : Loss =  0.37952358  Acc:  0.9109833 Val_loss =  0.4627115 Val_acc =  0.8791\n",
            "Iteration  1145 : Loss =  0.3822118  Acc:  0.91033334 Val_loss =  0.46622327 Val_acc =  0.8798\n",
            "Iteration  1146 : Loss =  0.38035554  Acc:  0.91048336 Val_loss =  0.46322942 Val_acc =  0.8796\n",
            "Iteration  1147 : Loss =  0.37622756  Acc:  0.91323334 Val_loss =  0.4594558 Val_acc =  0.8824\n",
            "Iteration  1148 : Loss =  0.3770643  Acc:  0.9122 Val_loss =  0.46051425 Val_acc =  0.8806\n",
            "Iteration  1149 : Loss =  0.38063148  Acc:  0.91005 Val_loss =  0.46334022 Val_acc =  0.8798\n",
            "Iteration  1150 : Loss =  0.38446108  Acc:  0.90811664 Val_loss =  0.46901056 Val_acc =  0.8784\n",
            "Iteration  1151 : Loss =  0.38593543  Acc:  0.9073 Val_loss =  0.4687992 Val_acc =  0.8755\n",
            "Iteration  1152 : Loss =  0.37710586  Acc:  0.91258335 Val_loss =  0.46103764 Val_acc =  0.8817\n",
            "Iteration  1153 : Loss =  0.37768537  Acc:  0.91188335 Val_loss =  0.46204925 Val_acc =  0.8808\n",
            "Iteration  1154 : Loss =  0.38285777  Acc:  0.909 Val_loss =  0.46590176 Val_acc =  0.8783\n",
            "Iteration  1155 : Loss =  0.37844336  Acc:  0.91141665 Val_loss =  0.4626062 Val_acc =  0.8812\n",
            "Iteration  1156 : Loss =  0.37423217  Acc:  0.91316664 Val_loss =  0.45784536 Val_acc =  0.8836\n",
            "Iteration  1157 : Loss =  0.37582988  Acc:  0.9127833 Val_loss =  0.45906594 Val_acc =  0.8832\n",
            "Iteration  1158 : Loss =  0.37983495  Acc:  0.91038334 Val_loss =  0.46465227 Val_acc =  0.8802\n",
            "Iteration  1159 : Loss =  0.38052174  Acc:  0.9098667 Val_loss =  0.46405268 Val_acc =  0.8791\n",
            "Iteration  1160 : Loss =  0.37514448  Acc:  0.91295 Val_loss =  0.45963955 Val_acc =  0.8808\n",
            "Iteration  1161 : Loss =  0.374515  Acc:  0.9133667 Val_loss =  0.45911336 Val_acc =  0.8821\n",
            "Iteration  1162 : Loss =  0.37760323  Acc:  0.91138333 Val_loss =  0.46110457 Val_acc =  0.8809\n",
            "Iteration  1163 : Loss =  0.37564313  Acc:  0.91265 Val_loss =  0.46027765 Val_acc =  0.8812\n",
            "Iteration  1164 : Loss =  0.37304676  Acc:  0.9138833 Val_loss =  0.45708746 Val_acc =  0.8829\n",
            "Iteration  1165 : Loss =  0.37390757  Acc:  0.91396666 Val_loss =  0.45743823 Val_acc =  0.884\n",
            "Iteration  1166 : Loss =  0.3760221  Acc:  0.91185 Val_loss =  0.46101582 Val_acc =  0.8805\n",
            "Iteration  1167 : Loss =  0.37618923  Acc:  0.91186666 Val_loss =  0.46015775 Val_acc =  0.882\n",
            "Iteration  1168 : Loss =  0.37356353  Acc:  0.91356665 Val_loss =  0.45839608 Val_acc =  0.8818\n",
            "Iteration  1169 : Loss =  0.37294817  Acc:  0.91331667 Val_loss =  0.45798162 Val_acc =  0.8832\n",
            "Iteration  1170 : Loss =  0.37424803  Acc:  0.91318333 Val_loss =  0.4582724 Val_acc =  0.8825\n",
            "Iteration  1171 : Loss =  0.37363517  Acc:  0.9137167 Val_loss =  0.45860225 Val_acc =  0.8821\n",
            "Iteration  1172 : Loss =  0.37257546  Acc:  0.9141333 Val_loss =  0.45682788 Val_acc =  0.8831\n",
            "Iteration  1173 : Loss =  0.37226355  Acc:  0.91455 Val_loss =  0.45653427 Val_acc =  0.8825\n",
            "Iteration  1174 : Loss =  0.37266755  Acc:  0.91361666 Val_loss =  0.45792025 Val_acc =  0.8815\n",
            "Iteration  1175 : Loss =  0.37314937  Acc:  0.9134667 Val_loss =  0.45762223 Val_acc =  0.8845\n",
            "Iteration  1176 : Loss =  0.37265363  Acc:  0.91366667 Val_loss =  0.45784003 Val_acc =  0.8815\n",
            "Iteration  1177 : Loss =  0.37238768  Acc:  0.914 Val_loss =  0.45745182 Val_acc =  0.8831\n",
            "Iteration  1178 : Loss =  0.37311938  Acc:  0.91326666 Val_loss =  0.4575996 Val_acc =  0.8832\n",
            "Iteration  1179 : Loss =  0.37401295  Acc:  0.91353333 Val_loss =  0.45935792 Val_acc =  0.8824\n",
            "Iteration  1180 : Loss =  0.3747896  Acc:  0.91258335 Val_loss =  0.45933908 Val_acc =  0.8825\n",
            "Iteration  1181 : Loss =  0.3752296  Acc:  0.91283333 Val_loss =  0.46030474 Val_acc =  0.8821\n",
            "Iteration  1182 : Loss =  0.37697163  Acc:  0.91125 Val_loss =  0.46248817 Val_acc =  0.881\n",
            "Iteration  1183 : Loss =  0.37701857  Acc:  0.91218334 Val_loss =  0.46193314 Val_acc =  0.8825\n",
            "Iteration  1184 : Loss =  0.37732327  Acc:  0.9108667 Val_loss =  0.46281487 Val_acc =  0.8807\n",
            "Iteration  1185 : Loss =  0.37468538  Acc:  0.9127833 Val_loss =  0.4596439 Val_acc =  0.8815\n",
            "Iteration  1186 : Loss =  0.3727938  Acc:  0.9133833 Val_loss =  0.45741957 Val_acc =  0.8835\n",
            "Iteration  1187 : Loss =  0.37130857  Acc:  0.91435 Val_loss =  0.45685026 Val_acc =  0.8827\n",
            "Iteration  1188 : Loss =  0.37058884  Acc:  0.91475 Val_loss =  0.45553195 Val_acc =  0.8835\n",
            "Iteration  1189 : Loss =  0.37076288  Acc:  0.91421664 Val_loss =  0.45640635 Val_acc =  0.882\n",
            "Iteration  1190 : Loss =  0.37179327  Acc:  0.91398335 Val_loss =  0.45760638 Val_acc =  0.8823\n",
            "Iteration  1191 : Loss =  0.37324846  Acc:  0.9131333 Val_loss =  0.45862746 Val_acc =  0.8829\n",
            "Iteration  1192 : Loss =  0.3741538  Acc:  0.91286665 Val_loss =  0.4602254 Val_acc =  0.8812\n",
            "Iteration  1193 : Loss =  0.37511766  Acc:  0.91171664 Val_loss =  0.46058467 Val_acc =  0.8815\n",
            "Iteration  1194 : Loss =  0.3744674  Acc:  0.9131167 Val_loss =  0.45992517 Val_acc =  0.882\n",
            "Iteration  1195 : Loss =  0.37387514  Acc:  0.9123667 Val_loss =  0.45973963 Val_acc =  0.881\n",
            "Iteration  1196 : Loss =  0.37144652  Acc:  0.9142333 Val_loss =  0.45686936 Val_acc =  0.8837\n",
            "Iteration  1197 : Loss =  0.36968106  Acc:  0.91476667 Val_loss =  0.45556384 Val_acc =  0.8836\n",
            "Iteration  1198 : Loss =  0.36932594  Acc:  0.91465 Val_loss =  0.45525676 Val_acc =  0.8831\n",
            "Iteration  1199 : Loss =  0.37006348  Acc:  0.91513336 Val_loss =  0.45568374 Val_acc =  0.8833\n",
            "\n",
            "The final test accuracy of the Fashion MNIST dataset is 0.8817999958992004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5vDaxkAUmuW",
        "colab_type": "text"
      },
      "source": [
        "### Plot of Training/Validation Accuracy and Training/Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxR4-UrqUs6X",
        "colab_type": "code",
        "outputId": "f67d8310-8090-48a5-c461-0ca51c36be5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(test_loss, label=\"Val Loss\", color='blue')\n",
        "plt.plot(test_acc, label=\"Val Acc\", color='orange')\n",
        "plt.plot(training_loss, label=\"Train Loss\", color='green')\n",
        "plt.plot(training_acc, label=\"Train Acc\", color='red')\n",
        "plt.title(\"Number of Iterations vs Training/Validation Loss and Accuracy\")\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Print the test accuracy \n",
        "print()\n",
        "print(\"The final test accuracy of the Fashion MNIST dataset is {}\".format(final_test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The final test accuracy of the Fashion MNIST dataset is 0.8817999958992004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbgMBaisju34",
        "colab_type": "text"
      },
      "source": [
        "### L2 regularisation to Netowork Architecture B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whPr_42F4mOe",
        "colab_type": "code",
        "outputId": "eb37ffcd-d734-464c-9fba-aea3555fe528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# -------------------------------------------CODE TO PREPARE THE DATASET---------------------------------------------------- \n",
        "\n",
        "def prepare_dataset(fashion_mnist):\n",
        "  # load the training and test data    \n",
        "  (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
        "  # reshape the feature data\n",
        "  tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
        "  te_x = te_x.reshape(te_x.shape[0], 784)\n",
        "  # noramlise feature data\n",
        "  tr_x = tr_x / 255.0\n",
        "  te_x = te_x / 255.0\n",
        "  # one hot encode the training labels and get the transpose\n",
        "  tr_y = np_utils.to_categorical(tr_y,10)\n",
        "  tr_y = tr_y.T\n",
        "  # one hot encode the test labels and get the transpose\n",
        "  te_y = np_utils.to_categorical(te_y,10)\n",
        "  te_y = te_y.T\n",
        "  return tr_x, tr_y, te_x, te_y\n",
        "\n",
        "# -----------------------------------CODE TO CALCULATE THE PROBABILITY OF EACH CLASS GIVEN THE TRAINING INSTANCE--------------------------------------\n",
        "\n",
        "def forward_pass(X_train, W1, b1, W2, b2, W3, b3):\n",
        "  \"\"\"\n",
        "  Return the predicted 10 class probabilities matrix for each of the training instances \n",
        "  \"\"\"\n",
        "  # Calculate the pre-activation outputs for each of the 300 neurons in the hidden layer 1 for each of the training instance \n",
        "  # Will get 300 outputs for a single training instance in the form of (300*60000) matrix \n",
        "  # The size of the W1 is (300*784)\n",
        "  # The size of the training feature matrix is (784*60000)\n",
        "  # The size of the b1 is (300*1)\n",
        "  A1=  tf.matmul(W1, X_train) + b1\n",
        "  # Convert each element in A1 through relu activation function \n",
        "  H1= tf.math.maximum(A1, 0)\n",
        "\n",
        "  # Calculate the pre-activation outputs for each of the 100 neurons in the hidden layer 2 for each of the training instance \n",
        "  # Will get 100 outputs for a single training instance in the form of (100*60000) matrix \n",
        "  # The size of the W2 is (100*300)\n",
        "  # The size of the H1 matrix is (300*60000)\n",
        "  # The size of the b2 is (100*1)\n",
        "  A2=  tf.matmul(W2, H1) + b2\n",
        "  # Convert each element in A1 through relu activation function \n",
        "  H2= tf.math.maximum(A2, 0)\n",
        "\n",
        "  # Calculate the pre-activation outputs for each of the 10 neurons in the softmax layer for each of the training instance \n",
        "  # Will get 10 outputs for a single training instance in the form of (10*60000) matrix \n",
        "  # The size of the W3 is (10*100)\n",
        "  # The size of the H2 matrix is (100*60000)\n",
        "  # The size of the bias matrix b3 is (10*1)\n",
        "  A3= tf.matmul(W3, H2) + b3\n",
        "  # Calculate a new matrix where each element is e to the power of pre-activation outputs \n",
        "  exponential_matrix= tf.math.exp(A3)\n",
        "  # Calculation of the final probabilities of each of the 10 classes for each instance in the training set \n",
        "  # Column wise sum calculation \n",
        "  column_sum= tf.reduce_sum(exponential_matrix, 0)\n",
        "  # Divide each element by the column sum so that each column is the probability of each class of a single instance \n",
        "  H3= exponential_matrix/column_sum \n",
        "  # Set the range so that the loss does not come out to be nan \n",
        "  H3= tf.clip_by_value(H3 ,1e-10, 1.0) \n",
        "  \n",
        "  return H3\n",
        "\n",
        "# -------------------------------- CODE TO CALCULATE THE LOSS FOR THE CURRENT SET OF TUNABLE PARAMETERS / WEIGHTS-------------------------------------\n",
        "\n",
        "def cross_entropy(y_train, y_pred_matrix, W1, W2, W3):\n",
        "  \"\"\"\n",
        "  Return the loss value given the predicted probabilities matrix and the actual probabilities matrix\n",
        "  Loss function also adds an additional component being the sum of squared elements in W1, W2 and W3 \n",
        "  \"\"\"\n",
        "  # Compute the log of each element of the prediction matrix \n",
        "  log_matrix= tf.math.log(y_pred_matrix)\n",
        "  # Multiply each element of the actual labels matrix with the log matrix \n",
        "  product_matrix= y_train *log_matrix\n",
        "  # Take the negation of each element in the product matrix \n",
        "  negated_product_matrix= -1*(product_matrix)\n",
        "  # Compute the cross entropy loss for each of the training instances \n",
        "  # This will contain individual loss for the all training instances \n",
        "  # This operation will perform the column wise sum \n",
        "  single_loss_matrix= tf.reduce_sum(negated_product_matrix, 0)\n",
        "  # Compute the mean cross entropy loss \n",
        "  mean_loss= tf.reduce_mean(single_loss_matrix)\n",
        "  \n",
        "  # Calculate the sum of squared elements of W1, W2, W3 \n",
        "  W1_square= tf.math.square(W1)\n",
        "  W1_sum= tf.math.reduce_sum(W1_square)\n",
        "  W2_square= tf.math.square(W2)\n",
        "  W2_sum= tf.math.reduce_sum(W2_square)\n",
        "  W3_square= tf.math.square(W3)\n",
        "  W3_sum= tf.math.reduce_sum(W3_square)\n",
        "  squared_weights_sum= W1_sum + W2_sum + W3_sum \n",
        "\n",
        "  # Multiply the regularisation rate with weights sum \n",
        "  reg_rate= 0.0001\n",
        "  product= reg_rate * squared_weights_sum\n",
        "  # Final loss after applying L2 regularisation \n",
        "  final_loss= mean_loss + product \n",
        "\n",
        "  return final_loss\n",
        "\n",
        "# ----------------------------------------- CALCULATION OF THE TRAINING AND TEST SET ACCURACY------------------------------------------------------\n",
        "\n",
        "def return_labels(matrix):\n",
        "  \"\"\"\n",
        "  Return the corrosponding class label for each vector of probability instance \n",
        "  \"\"\" \n",
        "  class_labels= tf.argmax(matrix) \n",
        "  \n",
        "  return class_labels\n",
        "\n",
        "def calculate_accuracy(feature_data, label_data, W1, b1, W2, b2, W3, b3):\n",
        "  \"\"\" \n",
        "  Return the accuracy value (applicable for both train and the test set) for the given set of weights and the biases \n",
        "  \"\"\"\n",
        "  # Calculate the matrix of predicted probabilities through calling of forward pass \n",
        "  predicted_matrix= forward_pass(feature_data, W1, b1, W2, b2, W3, b3)\n",
        "  # Get the class labels of the actual labels \n",
        "  actual_labels= return_labels(label_data)\n",
        "  # Get the class labels of the predicted probabilities \n",
        "  predicted_labels= return_labels(predicted_matrix)\n",
        "  # Get the correct prediction in the form of boolean array where 1 is correct prediction and 0 is the wrong prediction \n",
        "  correct_predictions= tf.cast(tf.equal(predicted_labels, actual_labels), tf.float32)\n",
        "  # Calculate the accuracy \n",
        "  accuracy= tf.reduce_mean(correct_predictions)\n",
        "\n",
        "  return accuracy \n",
        "\n",
        "# ------------------------------------------------BEGINNING OF OUR TENSORFLOW PROGRAM -----------------------------------------------\n",
        "\n",
        "# Loading the fashion MNIST data-set \n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "# Prepare the dataset \n",
        "train_features, train_labels, test_features, test_labels= prepare_dataset(fashion_mnist)\n",
        "\n",
        "# Get the transpose of feature data \n",
        "train_features= train_features.T \n",
        "test_features= test_features.T\n",
        "\n",
        "# Print the shape of our 4 data structures \n",
        "print( \"Shape of training features \", train_features.shape)\n",
        "print (\"Shape of training labels \", train_labels.shape)\n",
        "print()\n",
        "print( \"Shape of test features \", test_features.shape)\n",
        "print (\"Shape of test labels \", test_labels.shape)\n",
        "print()\n",
        "print(\"The training process of our Softmax Neural Network begins.....\")\n",
        "print()\n",
        "\n",
        "X_train= tf.cast(train_features, tf.float32)\n",
        "y_train= tf.cast(train_labels, tf.float32)\n",
        "X_test= tf.cast(test_features, tf.float32)\n",
        "y_test= tf.cast(test_labels, tf.float32)\n",
        "\n",
        "# Set the Number of features\n",
        "num_features=  X_train.shape[0]\n",
        "# We now specify the size of hidden layer 1\n",
        "hidden1_neurons= 300\n",
        "# We now specify the size of hidden layer 2\n",
        "hidden2_neurons= 100\n",
        "# We now specify the size of output layer \n",
        "output_neurons= 10 \n",
        "\n",
        "# Initialize the weight_matrix 1 and bias_matrix 1 \n",
        "# Each row of this matrix represents the 784 weights of a single neuron in the hidden layer \n",
        "W1= tf.Variable(tf.random.normal([hidden1_neurons, num_features], mean=0.0, stddev=0.05))\n",
        "# It is a column vector where each row/element represents the bias value for a single neuron in the hidden layer \n",
        "b1= tf.Variable(tf.random.normal([hidden1_neurons, 1], mean=0.0, stddev=0.05))\n",
        "\n",
        "# Initialize the weight_matrix 2 and bias_matrix 2 \n",
        "# Each row of this matrix represents the 300 weights of a single neuron in the softmax layer \n",
        "W2= tf.Variable(tf.random.normal([hidden2_neurons, hidden1_neurons], mean=0.0, stddev=0.05))\n",
        "# It is a column vector where each row/element represents the bias value for a single neuron in the softmax layer \n",
        "b2= tf.Variable(tf.random.normal([hidden2_neurons, 1], mean=0.0, stddev=0.05))\n",
        "\n",
        "# Initialize the weight_matrix 3 and bias_matrix 3\n",
        "# Each row of this matrix represents the 300 weights of a single neuron in the softmax layer \n",
        "W3= tf.Variable(tf.random.normal([output_neurons, hidden2_neurons], mean=0.0, stddev=0.05))\n",
        "# It is a column vector where each row/element represents the bias value for a single neuron in the softmax layer \n",
        "b3= tf.Variable(tf.random.normal([output_neurons, 1], mean=0.0, stddev=0.05))\n",
        "\n",
        "# Set the learning rate and the number of iterations \n",
        "learning_rate= 0.01\n",
        "num_iterations= 1200\n",
        "\n",
        "# Adam optimizer to update the weights of the neural network \n",
        "adam_optimizer= tf.keras.optimizers.Adam()\n",
        "\n",
        "# Create the list to store the training accuracy and loss with each iteration \n",
        "training_loss= []\n",
        "training_acc= []\n",
        "# Create the list to store the test accuracy and loss with each iteration \n",
        "test_loss= []\n",
        "test_acc= []\n",
        "\n",
        "# Run the gradient descent to num_iterations number of times \n",
        "for iteration in range(num_iterations):\n",
        "  \n",
        "  # Create an instance of GradientTape to monitor the forward pass and loss calculations\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Calculate the predicted probability matrix for the current weights and the biases for the training set\n",
        "    y_pred_matrix= forward_pass(X_train, W1, b1, W2, b2, W3, b3)\n",
        "    # Calculate the predicted probability matrix for the current weights and the biases for the test set\n",
        "    y_pred_test= forward_pass(X_test, W1, b1, W2, b2, W3, b3)\n",
        "    # Calculate the current training loss with the current predictions and the actual labels of the training set \n",
        "    current_loss_training= cross_entropy(y_train, y_pred_matrix, W1, W2, W3)\n",
        "    # Calculate the current test loss with the current prediction and the actual labels of the test set \n",
        "    current_loss_test= cross_entropy(y_test, y_pred_test, W1, W2, W3)\n",
        "  \n",
        "  # Calculate the gradients (partial derivates) of the loss with respect to each of the tunable weights \n",
        "  gradients= tape.gradient(current_loss_training, [W1, b1, W2, b2, W3, b3])\n",
        "\n",
        "  # Calculate the training accuracy with each iteration \n",
        "  training_accuracy= calculate_accuracy(X_train, y_train, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "  # Calculate the test accuracy with each each iteration \n",
        "  test_accuracy= calculate_accuracy(X_test, y_test, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "  # Print out the current iteration, current loss and current training accuracy \n",
        "  print(\"Iteration \",iteration, \": Loss = \",current_loss_training.numpy(),\" Acc: \", training_accuracy.numpy(),   'Val_loss = ',current_loss_test.numpy(),   'Val_acc = ', test_accuracy.numpy())\n",
        "\n",
        "  # Apply the Adam optimizer to update the weights and biases \n",
        "  adam_optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
        "\n",
        "  # Append the 4 values (train loss, train acc, test loss, test acc) with the each current iteration for the plotting \n",
        "  training_loss.append(current_loss_training.numpy())\n",
        "  training_acc.append(training_accuracy.numpy())\n",
        "  test_loss.append(current_loss_test.numpy())\n",
        "  test_acc.append(test_accuracy.numpy())\n",
        "\n",
        "# Calculate the test accuracy with the final updated weights and the biases through adam optimizer after running for certain number of iterations \n",
        "final_test_accuracy= calculate_accuracy(X_test, y_test, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "# Print the test accuracy \n",
        "print()\n",
        "print(\"The final test accuracy of the Fashion MNIST dataset is {}\".format(final_test_accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training features  (784, 60000)\n",
            "Shape of training labels  (10, 60000)\n",
            "\n",
            "Shape of test features  (784, 10000)\n",
            "Shape of test labels  (10, 10000)\n",
            "\n",
            "The training process of our Softmax Neural Network begins.....\n",
            "\n",
            "Iteration  0 : Loss =  2.3841345  Acc:  0.10226667 Val_loss =  2.384115 Val_acc =  0.1017\n",
            "Iteration  1 : Loss =  2.2617607  Acc:  0.29883334 Val_loss =  2.2629838 Val_acc =  0.2986\n",
            "Iteration  2 : Loss =  2.1594565  Acc:  0.41093335 Val_loss =  2.161337 Val_acc =  0.4077\n",
            "Iteration  3 : Loss =  2.0583239  Acc:  0.474 Val_loss =  2.0607238 Val_acc =  0.468\n",
            "Iteration  4 : Loss =  1.9520458  Acc:  0.5304 Val_loss =  1.9548502 Val_acc =  0.5224\n",
            "Iteration  5 : Loss =  1.84087  Acc:  0.58598334 Val_loss =  1.8441254 Val_acc =  0.5786\n",
            "Iteration  6 : Loss =  1.7260547  Acc:  0.61698335 Val_loss =  1.729918 Val_acc =  0.6114\n",
            "Iteration  7 : Loss =  1.6105865  Acc:  0.63636667 Val_loss =  1.6153331 Val_acc =  0.63\n",
            "Iteration  8 : Loss =  1.4965091  Acc:  0.6521 Val_loss =  1.5024225 Val_acc =  0.6445\n",
            "Iteration  9 : Loss =  1.3880488  Acc:  0.66153336 Val_loss =  1.3952826 Val_acc =  0.6517\n",
            "Iteration  10 : Loss =  1.2878698  Acc:  0.66571665 Val_loss =  1.2963524 Val_acc =  0.6564\n",
            "Iteration  11 : Loss =  1.197911  Acc:  0.66796666 Val_loss =  1.2075148 Val_acc =  0.6604\n",
            "Iteration  12 : Loss =  1.119606  Acc:  0.67198336 Val_loss =  1.130306 Val_acc =  0.6629\n",
            "Iteration  13 : Loss =  1.053101  Acc:  0.67758334 Val_loss =  1.0649307 Val_acc =  0.6661\n",
            "Iteration  14 : Loss =  0.99728763  Acc:  0.6862 Val_loss =  1.0104091 Val_acc =  0.6734\n",
            "Iteration  15 : Loss =  0.9511533  Acc:  0.6948 Val_loss =  0.965544 Val_acc =  0.6814\n",
            "Iteration  16 : Loss =  0.91280764  Acc:  0.70243335 Val_loss =  0.92822653 Val_acc =  0.6895\n",
            "Iteration  17 : Loss =  0.8804123  Acc:  0.7081 Val_loss =  0.8969378 Val_acc =  0.6947\n",
            "Iteration  18 : Loss =  0.8531947  Acc:  0.7129667 Val_loss =  0.8711761 Val_acc =  0.7001\n",
            "Iteration  19 : Loss =  0.83001035  Acc:  0.71851665 Val_loss =  0.84921676 Val_acc =  0.7039\n",
            "Iteration  20 : Loss =  0.81002736  Acc:  0.72465 Val_loss =  0.8301559 Val_acc =  0.7129\n",
            "Iteration  21 : Loss =  0.79264724  Acc:  0.7335167 Val_loss =  0.81401366 Val_acc =  0.7203\n",
            "Iteration  22 : Loss =  0.77709544  Acc:  0.7386 Val_loss =  0.7990963 Val_acc =  0.7257\n",
            "Iteration  23 : Loss =  0.76352286  Acc:  0.74331665 Val_loss =  0.78583187 Val_acc =  0.7302\n",
            "Iteration  24 : Loss =  0.7507399  Acc:  0.74906665 Val_loss =  0.773855 Val_acc =  0.7359\n",
            "Iteration  25 : Loss =  0.73873144  Acc:  0.7538 Val_loss =  0.7618401 Val_acc =  0.7413\n",
            "Iteration  26 : Loss =  0.7271981  Acc:  0.75986665 Val_loss =  0.7506036 Val_acc =  0.7465\n",
            "Iteration  27 : Loss =  0.7160117  Acc:  0.7646833 Val_loss =  0.73945296 Val_acc =  0.7524\n",
            "Iteration  28 : Loss =  0.7052988  Acc:  0.76998335 Val_loss =  0.72856265 Val_acc =  0.7572\n",
            "Iteration  29 : Loss =  0.69488883  Acc:  0.77643335 Val_loss =  0.7187467 Val_acc =  0.7635\n",
            "Iteration  30 : Loss =  0.6852197  Acc:  0.78106666 Val_loss =  0.70843405 Val_acc =  0.7683\n",
            "Iteration  31 : Loss =  0.6761404  Acc:  0.7871 Val_loss =  0.70040774 Val_acc =  0.7752\n",
            "Iteration  32 : Loss =  0.6676896  Acc:  0.78826666 Val_loss =  0.6906764 Val_acc =  0.7772\n",
            "Iteration  33 : Loss =  0.65903574  Acc:  0.7945167 Val_loss =  0.68331754 Val_acc =  0.7816\n",
            "Iteration  34 : Loss =  0.65141267  Acc:  0.7985 Val_loss =  0.67544544 Val_acc =  0.7834\n",
            "Iteration  35 : Loss =  0.64583063  Acc:  0.79868335 Val_loss =  0.6696158 Val_acc =  0.7866\n",
            "Iteration  36 : Loss =  0.6390113  Acc:  0.80303335 Val_loss =  0.66401035 Val_acc =  0.7906\n",
            "Iteration  37 : Loss =  0.62947553  Acc:  0.80653334 Val_loss =  0.6535131 Val_acc =  0.7924\n",
            "Iteration  38 : Loss =  0.62301266  Acc:  0.8089167 Val_loss =  0.6474054 Val_acc =  0.7949\n",
            "Iteration  39 : Loss =  0.619218  Acc:  0.8102667 Val_loss =  0.64466757 Val_acc =  0.7977\n",
            "Iteration  40 : Loss =  0.61195517  Acc:  0.81268334 Val_loss =  0.63671595 Val_acc =  0.8007\n",
            "Iteration  41 : Loss =  0.60493946  Acc:  0.8146 Val_loss =  0.63018167 Val_acc =  0.803\n",
            "Iteration  42 : Loss =  0.6014179  Acc:  0.8156833 Val_loss =  0.6276939 Val_acc =  0.8021\n",
            "Iteration  43 : Loss =  0.5961726  Acc:  0.81703335 Val_loss =  0.6216763 Val_acc =  0.8056\n",
            "Iteration  44 : Loss =  0.5894035  Acc:  0.8203667 Val_loss =  0.61576444 Val_acc =  0.8086\n",
            "Iteration  45 : Loss =  0.5857192  Acc:  0.82123333 Val_loss =  0.6129116 Val_acc =  0.8079\n",
            "Iteration  46 : Loss =  0.58198595  Acc:  0.8213 Val_loss =  0.6081971 Val_acc =  0.8112\n",
            "Iteration  47 : Loss =  0.57578754  Acc:  0.82491666 Val_loss =  0.60354483 Val_acc =  0.8115\n",
            "Iteration  48 : Loss =  0.5715778  Acc:  0.8264667 Val_loss =  0.5994612 Val_acc =  0.8133\n",
            "Iteration  49 : Loss =  0.5684306  Acc:  0.82621664 Val_loss =  0.59576106 Val_acc =  0.8156\n",
            "Iteration  50 : Loss =  0.5634062  Acc:  0.82893336 Val_loss =  0.5921027 Val_acc =  0.8157\n",
            "Iteration  51 : Loss =  0.55887413  Acc:  0.83068335 Val_loss =  0.58742934 Val_acc =  0.8175\n",
            "Iteration  52 : Loss =  0.55602324  Acc:  0.8307833 Val_loss =  0.58444965 Val_acc =  0.819\n",
            "Iteration  53 : Loss =  0.5524231  Acc:  0.83225 Val_loss =  0.58176696 Val_acc =  0.8203\n",
            "Iteration  54 : Loss =  0.5480848  Acc:  0.834 Val_loss =  0.5774205 Val_acc =  0.8218\n",
            "Iteration  55 : Loss =  0.54488087  Acc:  0.8349 Val_loss =  0.5739614 Val_acc =  0.8225\n",
            "Iteration  56 : Loss =  0.5423004  Acc:  0.83536667 Val_loss =  0.5725462 Val_acc =  0.8243\n",
            "Iteration  57 : Loss =  0.5392921  Acc:  0.83598334 Val_loss =  0.56860465 Val_acc =  0.8244\n",
            "Iteration  58 : Loss =  0.53605324  Acc:  0.83746666 Val_loss =  0.5665695 Val_acc =  0.8262\n",
            "Iteration  59 : Loss =  0.5342514  Acc:  0.8386 Val_loss =  0.5639881 Val_acc =  0.8257\n",
            "Iteration  60 : Loss =  0.5331148  Acc:  0.8376833 Val_loss =  0.5640272 Val_acc =  0.8268\n",
            "Iteration  61 : Loss =  0.5296114  Acc:  0.8395333 Val_loss =  0.55960125 Val_acc =  0.8273\n",
            "Iteration  62 : Loss =  0.5252556  Acc:  0.8411667 Val_loss =  0.5564417 Val_acc =  0.8292\n",
            "Iteration  63 : Loss =  0.5225318  Acc:  0.8418667 Val_loss =  0.552904 Val_acc =  0.8315\n",
            "Iteration  64 : Loss =  0.52049273  Acc:  0.84218335 Val_loss =  0.55152637 Val_acc =  0.8306\n",
            "Iteration  65 : Loss =  0.51776904  Acc:  0.8438333 Val_loss =  0.54921544 Val_acc =  0.8311\n",
            "Iteration  66 : Loss =  0.5157232  Acc:  0.8441667 Val_loss =  0.5463217 Val_acc =  0.8309\n",
            "Iteration  67 : Loss =  0.5138341  Acc:  0.8444167 Val_loss =  0.54583275 Val_acc =  0.8315\n",
            "Iteration  68 : Loss =  0.5109189  Acc:  0.8458167 Val_loss =  0.5421408 Val_acc =  0.833\n",
            "Iteration  69 : Loss =  0.5078189  Acc:  0.8473667 Val_loss =  0.5393721 Val_acc =  0.8343\n",
            "Iteration  70 : Loss =  0.5063875  Acc:  0.84681666 Val_loss =  0.5385631 Val_acc =  0.8349\n",
            "Iteration  71 : Loss =  0.5053102  Acc:  0.84795 Val_loss =  0.5367793 Val_acc =  0.8347\n",
            "Iteration  72 : Loss =  0.50247693  Acc:  0.8484167 Val_loss =  0.5349227 Val_acc =  0.8342\n",
            "Iteration  73 : Loss =  0.49977717  Acc:  0.84985 Val_loss =  0.5317449 Val_acc =  0.8357\n",
            "Iteration  74 : Loss =  0.49805802  Acc:  0.8506167 Val_loss =  0.53036666 Val_acc =  0.8371\n",
            "Iteration  75 : Loss =  0.4963917  Acc:  0.85036665 Val_loss =  0.528981 Val_acc =  0.8369\n",
            "Iteration  76 : Loss =  0.49443898  Acc:  0.85158336 Val_loss =  0.526671 Val_acc =  0.8373\n",
            "Iteration  77 : Loss =  0.49269995  Acc:  0.8515667 Val_loss =  0.5258169 Val_acc =  0.8381\n",
            "Iteration  78 : Loss =  0.49131843  Acc:  0.8524333 Val_loss =  0.5237579 Val_acc =  0.8379\n",
            "Iteration  79 : Loss =  0.48940852  Acc:  0.85316664 Val_loss =  0.52276707 Val_acc =  0.8389\n",
            "Iteration  80 : Loss =  0.48684597  Acc:  0.8538333 Val_loss =  0.5198022 Val_acc =  0.839\n",
            "Iteration  81 : Loss =  0.4847169  Acc:  0.85415 Val_loss =  0.5181744 Val_acc =  0.8399\n",
            "Iteration  82 : Loss =  0.48319986  Acc:  0.855 Val_loss =  0.5167646 Val_acc =  0.84\n",
            "Iteration  83 : Loss =  0.48162025  Acc:  0.85513335 Val_loss =  0.5152875 Val_acc =  0.8402\n",
            "Iteration  84 : Loss =  0.47973308  Acc:  0.85648334 Val_loss =  0.5138564 Val_acc =  0.8412\n",
            "Iteration  85 : Loss =  0.47799966  Acc:  0.85693336 Val_loss =  0.51192224 Val_acc =  0.8413\n",
            "Iteration  86 : Loss =  0.47671825  Acc:  0.8567167 Val_loss =  0.5113591 Val_acc =  0.8416\n",
            "Iteration  87 : Loss =  0.47592485  Acc:  0.8573167 Val_loss =  0.5102002 Val_acc =  0.8409\n",
            "Iteration  88 : Loss =  0.47475663  Acc:  0.85641664 Val_loss =  0.5099149 Val_acc =  0.8422\n",
            "Iteration  89 : Loss =  0.47402012  Acc:  0.85756665 Val_loss =  0.5085676 Val_acc =  0.8414\n",
            "Iteration  90 : Loss =  0.47222987  Acc:  0.85746664 Val_loss =  0.50795454 Val_acc =  0.8434\n",
            "Iteration  91 : Loss =  0.47063166  Acc:  0.8591 Val_loss =  0.50552183 Val_acc =  0.8442\n",
            "Iteration  92 : Loss =  0.4681707  Acc:  0.8598833 Val_loss =  0.50420254 Val_acc =  0.8451\n",
            "Iteration  93 : Loss =  0.46548167  Acc:  0.861 Val_loss =  0.50121737 Val_acc =  0.8474\n",
            "Iteration  94 : Loss =  0.46362904  Acc:  0.86193335 Val_loss =  0.49964982 Val_acc =  0.8461\n",
            "Iteration  95 : Loss =  0.46283427  Acc:  0.86155 Val_loss =  0.49949643 Val_acc =  0.8458\n",
            "Iteration  96 : Loss =  0.46246508  Acc:  0.86186665 Val_loss =  0.4987178 Val_acc =  0.8465\n",
            "Iteration  97 : Loss =  0.46132532  Acc:  0.86261666 Val_loss =  0.49852732 Val_acc =  0.8464\n",
            "Iteration  98 : Loss =  0.4592285  Acc:  0.8628 Val_loss =  0.49596268 Val_acc =  0.8476\n",
            "Iteration  99 : Loss =  0.45680133  Acc:  0.8635333 Val_loss =  0.4941977 Val_acc =  0.847\n",
            "Iteration  100 : Loss =  0.4551521  Acc:  0.8649833 Val_loss =  0.4925669 Val_acc =  0.8491\n",
            "Iteration  101 : Loss =  0.45420468  Acc:  0.86468333 Val_loss =  0.49175334 Val_acc =  0.8498\n",
            "Iteration  102 : Loss =  0.45335987  Acc:  0.86543334 Val_loss =  0.49153075 Val_acc =  0.8495\n",
            "Iteration  103 : Loss =  0.4520778  Acc:  0.86578333 Val_loss =  0.48994175 Val_acc =  0.8514\n",
            "Iteration  104 : Loss =  0.45047474  Acc:  0.86621666 Val_loss =  0.48911056 Val_acc =  0.8504\n",
            "Iteration  105 : Loss =  0.44905782  Acc:  0.8670167 Val_loss =  0.48748052 Val_acc =  0.8509\n",
            "Iteration  106 : Loss =  0.44799942  Acc:  0.8663833 Val_loss =  0.48681223 Val_acc =  0.8517\n",
            "Iteration  107 : Loss =  0.44739535  Acc:  0.86731666 Val_loss =  0.4863864 Val_acc =  0.8511\n",
            "Iteration  108 : Loss =  0.44663227  Acc:  0.8668 Val_loss =  0.48572078 Val_acc =  0.8527\n",
            "Iteration  109 : Loss =  0.44536504  Acc:  0.8677 Val_loss =  0.48467922 Val_acc =  0.852\n",
            "Iteration  110 : Loss =  0.44338125  Acc:  0.86785 Val_loss =  0.4829266 Val_acc =  0.8531\n",
            "Iteration  111 : Loss =  0.44144428  Acc:  0.86953336 Val_loss =  0.48099148 Val_acc =  0.8527\n",
            "Iteration  112 : Loss =  0.44001982  Acc:  0.86976665 Val_loss =  0.48004472 Val_acc =  0.8538\n",
            "Iteration  113 : Loss =  0.43918636  Acc:  0.8701 Val_loss =  0.4791664 Val_acc =  0.8551\n",
            "Iteration  114 : Loss =  0.4385021  Acc:  0.87005 Val_loss =  0.4790004 Val_acc =  0.8544\n",
            "Iteration  115 : Loss =  0.43743095  Acc:  0.8704 Val_loss =  0.47785285 Val_acc =  0.8543\n",
            "Iteration  116 : Loss =  0.4360035  Acc:  0.8706833 Val_loss =  0.47687685 Val_acc =  0.8553\n",
            "Iteration  117 : Loss =  0.43427524  Acc:  0.8715 Val_loss =  0.4750828 Val_acc =  0.8562\n",
            "Iteration  118 : Loss =  0.43272245  Acc:  0.8715 Val_loss =  0.4739133 Val_acc =  0.8559\n",
            "Iteration  119 : Loss =  0.4315232  Acc:  0.87226665 Val_loss =  0.47282064 Val_acc =  0.8558\n",
            "Iteration  120 : Loss =  0.43062016  Acc:  0.8724667 Val_loss =  0.472179 Val_acc =  0.8568\n",
            "Iteration  121 : Loss =  0.42989528  Acc:  0.8725333 Val_loss =  0.4717552 Val_acc =  0.8557\n",
            "Iteration  122 : Loss =  0.42928225  Acc:  0.8728167 Val_loss =  0.47123212 Val_acc =  0.8571\n",
            "Iteration  123 : Loss =  0.42885694  Acc:  0.8729 Val_loss =  0.47117206 Val_acc =  0.8559\n",
            "Iteration  124 : Loss =  0.42869586  Acc:  0.87273335 Val_loss =  0.47106713 Val_acc =  0.8579\n",
            "Iteration  125 : Loss =  0.4294657  Acc:  0.87291664 Val_loss =  0.47208157 Val_acc =  0.8554\n",
            "Iteration  126 : Loss =  0.43007612  Acc:  0.87145 Val_loss =  0.47307652 Val_acc =  0.8565\n",
            "Iteration  127 : Loss =  0.4339668  Acc:  0.8710833 Val_loss =  0.4767075 Val_acc =  0.8552\n",
            "Iteration  128 : Loss =  0.42973658  Acc:  0.8714833 Val_loss =  0.47318405 Val_acc =  0.8539\n",
            "Iteration  129 : Loss =  0.42608672  Acc:  0.87395 Val_loss =  0.46888083 Val_acc =  0.8584\n",
            "Iteration  130 : Loss =  0.42319775  Acc:  0.87485 Val_loss =  0.46651587 Val_acc =  0.8583\n",
            "Iteration  131 : Loss =  0.4228775  Acc:  0.8742167 Val_loss =  0.4667067 Val_acc =  0.8584\n",
            "Iteration  132 : Loss =  0.42361277  Acc:  0.87521666 Val_loss =  0.4670838 Val_acc =  0.8587\n",
            "Iteration  133 : Loss =  0.42042434  Acc:  0.8757 Val_loss =  0.46453828 Val_acc =  0.8598\n",
            "Iteration  134 : Loss =  0.4189138  Acc:  0.8760333 Val_loss =  0.46317503 Val_acc =  0.86\n",
            "Iteration  135 : Loss =  0.4188114  Acc:  0.8764333 Val_loss =  0.4629426 Val_acc =  0.8595\n",
            "Iteration  136 : Loss =  0.4166289  Acc:  0.87645 Val_loss =  0.4613686 Val_acc =  0.8609\n",
            "Iteration  137 : Loss =  0.41517293  Acc:  0.8775333 Val_loss =  0.4599287 Val_acc =  0.8608\n",
            "Iteration  138 : Loss =  0.4152813  Acc:  0.8779167 Val_loss =  0.45988902 Val_acc =  0.8619\n",
            "Iteration  139 : Loss =  0.41417795  Acc:  0.8776 Val_loss =  0.4593773 Val_acc =  0.863\n",
            "Iteration  140 : Loss =  0.412427  Acc:  0.8789833 Val_loss =  0.4578405 Val_acc =  0.8621\n",
            "Iteration  141 : Loss =  0.41125742  Acc:  0.87908334 Val_loss =  0.45626092 Val_acc =  0.8623\n",
            "Iteration  142 : Loss =  0.41095063  Acc:  0.8789167 Val_loss =  0.4565425 Val_acc =  0.8622\n",
            "Iteration  143 : Loss =  0.4101803  Acc:  0.87885 Val_loss =  0.45614263 Val_acc =  0.8626\n",
            "Iteration  144 : Loss =  0.40848783  Acc:  0.87956667 Val_loss =  0.4540301 Val_acc =  0.8637\n",
            "Iteration  145 : Loss =  0.40711084  Acc:  0.8807 Val_loss =  0.4533081 Val_acc =  0.8636\n",
            "Iteration  146 : Loss =  0.40693074  Acc:  0.88021666 Val_loss =  0.45331597 Val_acc =  0.8642\n",
            "Iteration  147 : Loss =  0.4065505  Acc:  0.88015 Val_loss =  0.45268542 Val_acc =  0.8635\n",
            "Iteration  148 : Loss =  0.404836  Acc:  0.88088334 Val_loss =  0.45177007 Val_acc =  0.864\n",
            "Iteration  149 : Loss =  0.4033441  Acc:  0.88158333 Val_loss =  0.45016503 Val_acc =  0.8644\n",
            "Iteration  150 : Loss =  0.40317452  Acc:  0.88136667 Val_loss =  0.4498739 Val_acc =  0.8644\n",
            "Iteration  151 : Loss =  0.40287313  Acc:  0.8814333 Val_loss =  0.45036492 Val_acc =  0.8651\n",
            "Iteration  152 : Loss =  0.4015112  Acc:  0.8816 Val_loss =  0.44885665 Val_acc =  0.8655\n",
            "Iteration  153 : Loss =  0.40006185  Acc:  0.88255 Val_loss =  0.44740582 Val_acc =  0.8655\n",
            "Iteration  154 : Loss =  0.39930826  Acc:  0.88276666 Val_loss =  0.44719878 Val_acc =  0.8656\n",
            "Iteration  155 : Loss =  0.39883852  Acc:  0.88266665 Val_loss =  0.44666076 Val_acc =  0.8659\n",
            "Iteration  156 : Loss =  0.39832836  Acc:  0.8828 Val_loss =  0.44635963 Val_acc =  0.867\n",
            "Iteration  157 : Loss =  0.39723548  Acc:  0.8832667 Val_loss =  0.44567278 Val_acc =  0.8662\n",
            "Iteration  158 : Loss =  0.3959263  Acc:  0.88365 Val_loss =  0.44424665 Val_acc =  0.8666\n",
            "Iteration  159 : Loss =  0.3949886  Acc:  0.8843667 Val_loss =  0.4436372 Val_acc =  0.8667\n",
            "Iteration  160 : Loss =  0.39451554  Acc:  0.8840333 Val_loss =  0.44350487 Val_acc =  0.8669\n",
            "Iteration  161 : Loss =  0.39393526  Acc:  0.88416666 Val_loss =  0.44283402 Val_acc =  0.8675\n",
            "Iteration  162 : Loss =  0.39309138  Acc:  0.8846167 Val_loss =  0.44233122 Val_acc =  0.8675\n",
            "Iteration  163 : Loss =  0.39211655  Acc:  0.88445 Val_loss =  0.44163632 Val_acc =  0.8679\n",
            "Iteration  164 : Loss =  0.39118895  Acc:  0.88556665 Val_loss =  0.44065902 Val_acc =  0.8674\n",
            "Iteration  165 : Loss =  0.39032164  Acc:  0.8855 Val_loss =  0.44024885 Val_acc =  0.8673\n",
            "Iteration  166 : Loss =  0.38958418  Acc:  0.8861667 Val_loss =  0.4396047 Val_acc =  0.8685\n",
            "Iteration  167 : Loss =  0.38898945  Acc:  0.88591665 Val_loss =  0.4390878 Val_acc =  0.8682\n",
            "Iteration  168 : Loss =  0.38848752  Acc:  0.88633335 Val_loss =  0.43916136 Val_acc =  0.8682\n",
            "Iteration  169 : Loss =  0.3878355  Acc:  0.8861833 Val_loss =  0.43826306 Val_acc =  0.868\n",
            "Iteration  170 : Loss =  0.3872855  Acc:  0.8868667 Val_loss =  0.438251 Val_acc =  0.8686\n",
            "Iteration  171 : Loss =  0.38660794  Acc:  0.8865 Val_loss =  0.43761963 Val_acc =  0.8685\n",
            "Iteration  172 : Loss =  0.38634324  Acc:  0.88745 Val_loss =  0.43756914 Val_acc =  0.8685\n",
            "Iteration  173 : Loss =  0.38604632  Acc:  0.88596666 Val_loss =  0.43751067 Val_acc =  0.869\n",
            "Iteration  174 : Loss =  0.3865603  Acc:  0.8869 Val_loss =  0.43826547 Val_acc =  0.8677\n",
            "Iteration  175 : Loss =  0.38722304  Acc:  0.88558334 Val_loss =  0.4389432 Val_acc =  0.8671\n",
            "Iteration  176 : Loss =  0.3882764  Acc:  0.8858333 Val_loss =  0.44062927 Val_acc =  0.8663\n",
            "Iteration  177 : Loss =  0.38839054  Acc:  0.88488334 Val_loss =  0.44031096 Val_acc =  0.8655\n",
            "Iteration  178 : Loss =  0.3852688  Acc:  0.8869333 Val_loss =  0.43789822 Val_acc =  0.8674\n",
            "Iteration  179 : Loss =  0.38205782  Acc:  0.8879167 Val_loss =  0.4345621 Val_acc =  0.8685\n",
            "Iteration  180 : Loss =  0.3805375  Acc:  0.8883333 Val_loss =  0.43316627 Val_acc =  0.8706\n",
            "Iteration  181 : Loss =  0.38173994  Acc:  0.88871664 Val_loss =  0.43516907 Val_acc =  0.8691\n",
            "Iteration  182 : Loss =  0.38177297  Acc:  0.88736665 Val_loss =  0.43479377 Val_acc =  0.8691\n",
            "Iteration  183 : Loss =  0.38037598  Acc:  0.8887333 Val_loss =  0.43407047 Val_acc =  0.8691\n",
            "Iteration  184 : Loss =  0.37757245  Acc:  0.88953334 Val_loss =  0.4313713 Val_acc =  0.8699\n",
            "Iteration  185 : Loss =  0.37618294  Acc:  0.88991666 Val_loss =  0.4297662 Val_acc =  0.8721\n",
            "Iteration  186 : Loss =  0.37671578  Acc:  0.8904833 Val_loss =  0.43116584 Val_acc =  0.8704\n",
            "Iteration  187 : Loss =  0.37693813  Acc:  0.8892 Val_loss =  0.43107256 Val_acc =  0.8712\n",
            "Iteration  188 : Loss =  0.37662762  Acc:  0.8901167 Val_loss =  0.4311943 Val_acc =  0.8699\n",
            "Iteration  189 : Loss =  0.37447682  Acc:  0.8904333 Val_loss =  0.42932218 Val_acc =  0.872\n",
            "Iteration  190 : Loss =  0.37276706  Acc:  0.89065 Val_loss =  0.4272674 Val_acc =  0.872\n",
            "Iteration  191 : Loss =  0.3721126  Acc:  0.89135 Val_loss =  0.42736962 Val_acc =  0.8718\n",
            "Iteration  192 : Loss =  0.37224627  Acc:  0.8915333 Val_loss =  0.42741516 Val_acc =  0.8724\n",
            "Iteration  193 : Loss =  0.3723015  Acc:  0.89136666 Val_loss =  0.42770988 Val_acc =  0.8708\n",
            "Iteration  194 : Loss =  0.37122867  Acc:  0.8919167 Val_loss =  0.42714652 Val_acc =  0.8726\n",
            "Iteration  195 : Loss =  0.3698774  Acc:  0.8921833 Val_loss =  0.4255827 Val_acc =  0.8713\n",
            "Iteration  196 : Loss =  0.36893353  Acc:  0.8921667 Val_loss =  0.42513204 Val_acc =  0.8738\n",
            "Iteration  197 : Loss =  0.368836  Acc:  0.8925167 Val_loss =  0.42526478 Val_acc =  0.8714\n",
            "Iteration  198 : Loss =  0.36897498  Acc:  0.89235 Val_loss =  0.42533654 Val_acc =  0.8724\n",
            "Iteration  199 : Loss =  0.36883646  Acc:  0.89271665 Val_loss =  0.42577538 Val_acc =  0.8718\n",
            "Iteration  200 : Loss =  0.3679564  Acc:  0.8925833 Val_loss =  0.42478168 Val_acc =  0.8733\n",
            "Iteration  201 : Loss =  0.36719644  Acc:  0.8935 Val_loss =  0.42427722 Val_acc =  0.8723\n",
            "Iteration  202 : Loss =  0.36692512  Acc:  0.89308333 Val_loss =  0.42437726 Val_acc =  0.8738\n",
            "Iteration  203 : Loss =  0.3673548  Acc:  0.8923 Val_loss =  0.42479876 Val_acc =  0.8722\n",
            "Iteration  204 : Loss =  0.36762878  Acc:  0.8925833 Val_loss =  0.42552832 Val_acc =  0.8738\n",
            "Iteration  205 : Loss =  0.3679947  Acc:  0.89208335 Val_loss =  0.42619985 Val_acc =  0.8718\n",
            "Iteration  206 : Loss =  0.3662175  Acc:  0.8928667 Val_loss =  0.4242298 Val_acc =  0.8743\n",
            "Iteration  207 : Loss =  0.36434317  Acc:  0.8940333 Val_loss =  0.42299604 Val_acc =  0.8728\n",
            "Iteration  208 : Loss =  0.3623176  Acc:  0.8944333 Val_loss =  0.42053103 Val_acc =  0.8757\n",
            "Iteration  209 : Loss =  0.36141258  Acc:  0.89555 Val_loss =  0.420155 Val_acc =  0.8737\n",
            "Iteration  210 : Loss =  0.36153063  Acc:  0.8947 Val_loss =  0.42053944 Val_acc =  0.8739\n",
            "Iteration  211 : Loss =  0.36165458  Acc:  0.89415 Val_loss =  0.4205671 Val_acc =  0.8749\n",
            "Iteration  212 : Loss =  0.361688  Acc:  0.89481664 Val_loss =  0.4214805 Val_acc =  0.8739\n",
            "Iteration  213 : Loss =  0.36045593  Acc:  0.89525 Val_loss =  0.41983452 Val_acc =  0.876\n",
            "Iteration  214 : Loss =  0.35926875  Acc:  0.8957667 Val_loss =  0.4192887 Val_acc =  0.8729\n",
            "Iteration  215 : Loss =  0.35835528  Acc:  0.89606667 Val_loss =  0.41848305 Val_acc =  0.876\n",
            "Iteration  216 : Loss =  0.35812813  Acc:  0.89603335 Val_loss =  0.41813216 Val_acc =  0.8734\n",
            "Iteration  217 : Loss =  0.3582306  Acc:  0.89605 Val_loss =  0.4190728 Val_acc =  0.876\n",
            "Iteration  218 : Loss =  0.35835418  Acc:  0.8947 Val_loss =  0.41862884 Val_acc =  0.874\n",
            "Iteration  219 : Loss =  0.35792515  Acc:  0.89605 Val_loss =  0.41915175 Val_acc =  0.875\n",
            "Iteration  220 : Loss =  0.35697314  Acc:  0.89566666 Val_loss =  0.41790658 Val_acc =  0.8737\n",
            "Iteration  221 : Loss =  0.35582098  Acc:  0.89666665 Val_loss =  0.41702592 Val_acc =  0.8747\n",
            "Iteration  222 : Loss =  0.35489273  Acc:  0.89751667 Val_loss =  0.41661423 Val_acc =  0.8738\n",
            "Iteration  223 : Loss =  0.35413438  Acc:  0.8969333 Val_loss =  0.41546 Val_acc =  0.8762\n",
            "Iteration  224 : Loss =  0.35356495  Acc:  0.8976667 Val_loss =  0.41587853 Val_acc =  0.8746\n",
            "Iteration  225 : Loss =  0.35312587  Acc:  0.8972833 Val_loss =  0.4150772 Val_acc =  0.8765\n",
            "Iteration  226 : Loss =  0.35301238  Acc:  0.8976167 Val_loss =  0.41559926 Val_acc =  0.8746\n",
            "Iteration  227 : Loss =  0.3529423  Acc:  0.89751667 Val_loss =  0.41573936 Val_acc =  0.8764\n",
            "Iteration  228 : Loss =  0.35321847  Acc:  0.8971 Val_loss =  0.41591057 Val_acc =  0.8756\n",
            "Iteration  229 : Loss =  0.35298586  Acc:  0.89713335 Val_loss =  0.41637808 Val_acc =  0.8766\n",
            "Iteration  230 : Loss =  0.35271683  Acc:  0.89725 Val_loss =  0.41570666 Val_acc =  0.8748\n",
            "Iteration  231 : Loss =  0.35174507  Acc:  0.8979833 Val_loss =  0.41541505 Val_acc =  0.8761\n",
            "Iteration  232 : Loss =  0.35100442  Acc:  0.89753336 Val_loss =  0.41464382 Val_acc =  0.8757\n",
            "Iteration  233 : Loss =  0.35065094  Acc:  0.8983167 Val_loss =  0.4144761 Val_acc =  0.8767\n",
            "Iteration  234 : Loss =  0.35110366  Acc:  0.89811665 Val_loss =  0.41552404 Val_acc =  0.8738\n",
            "Iteration  235 : Loss =  0.35184917  Acc:  0.89795 Val_loss =  0.41606304 Val_acc =  0.8766\n",
            "Iteration  236 : Loss =  0.3534204  Acc:  0.8962167 Val_loss =  0.41820222 Val_acc =  0.8719\n",
            "Iteration  237 : Loss =  0.3531591  Acc:  0.8968833 Val_loss =  0.41834295 Val_acc =  0.8745\n",
            "Iteration  238 : Loss =  0.35275203  Acc:  0.89633334 Val_loss =  0.41729543 Val_acc =  0.8717\n",
            "Iteration  239 : Loss =  0.34950352  Acc:  0.89896667 Val_loss =  0.4154821 Val_acc =  0.8759\n",
            "Iteration  240 : Loss =  0.34636497  Acc:  0.89931667 Val_loss =  0.4111877 Val_acc =  0.8774\n",
            "Iteration  241 : Loss =  0.34548432  Acc:  0.89966667 Val_loss =  0.41139805 Val_acc =  0.8759\n",
            "Iteration  242 : Loss =  0.3467848  Acc:  0.89968336 Val_loss =  0.41300607 Val_acc =  0.8766\n",
            "Iteration  243 : Loss =  0.34831405  Acc:  0.89785 Val_loss =  0.4141215 Val_acc =  0.8737\n",
            "Iteration  244 : Loss =  0.34686106  Acc:  0.8993 Val_loss =  0.41388273 Val_acc =  0.8763\n",
            "Iteration  245 : Loss =  0.34442517  Acc:  0.90001667 Val_loss =  0.41074225 Val_acc =  0.877\n",
            "Iteration  246 : Loss =  0.34242803  Acc:  0.90141666 Val_loss =  0.40929374 Val_acc =  0.8771\n",
            "Iteration  247 : Loss =  0.34252828  Acc:  0.9006 Val_loss =  0.40972072 Val_acc =  0.878\n",
            "Iteration  248 : Loss =  0.34359145  Acc:  0.90003335 Val_loss =  0.41040987 Val_acc =  0.8768\n",
            "Iteration  249 : Loss =  0.34347272  Acc:  0.90078336 Val_loss =  0.4110852 Val_acc =  0.8771\n",
            "Iteration  250 : Loss =  0.34221274  Acc:  0.90106666 Val_loss =  0.40969482 Val_acc =  0.8768\n",
            "Iteration  251 : Loss =  0.3407758  Acc:  0.9015333 Val_loss =  0.40823022 Val_acc =  0.879\n",
            "Iteration  252 : Loss =  0.34016582  Acc:  0.90216666 Val_loss =  0.40856126 Val_acc =  0.8778\n",
            "Iteration  253 : Loss =  0.3401318  Acc:  0.9013 Val_loss =  0.40775582 Val_acc =  0.8782\n",
            "Iteration  254 : Loss =  0.33994967  Acc:  0.90176666 Val_loss =  0.40873414 Val_acc =  0.8782\n",
            "Iteration  255 : Loss =  0.33959475  Acc:  0.9012333 Val_loss =  0.40818354 Val_acc =  0.8774\n",
            "Iteration  256 : Loss =  0.3390648  Acc:  0.9023167 Val_loss =  0.40780276 Val_acc =  0.8783\n",
            "Iteration  257 : Loss =  0.3385676  Acc:  0.9017 Val_loss =  0.408091 Val_acc =  0.8769\n",
            "Iteration  258 : Loss =  0.33803448  Acc:  0.90316665 Val_loss =  0.4070831 Val_acc =  0.8786\n",
            "Iteration  259 : Loss =  0.3375199  Acc:  0.90258336 Val_loss =  0.40731424 Val_acc =  0.8779\n",
            "Iteration  260 : Loss =  0.33763695  Acc:  0.90235 Val_loss =  0.40760022 Val_acc =  0.8775\n",
            "Iteration  261 : Loss =  0.338446  Acc:  0.9018 Val_loss =  0.4081078 Val_acc =  0.8794\n",
            "Iteration  262 : Loss =  0.34044623  Acc:  0.90105 Val_loss =  0.41142705 Val_acc =  0.8774\n",
            "Iteration  263 : Loss =  0.34127045  Acc:  0.90038335 Val_loss =  0.41105768 Val_acc =  0.877\n",
            "Iteration  264 : Loss =  0.3431333  Acc:  0.9 Val_loss =  0.41453126 Val_acc =  0.8758\n",
            "Iteration  265 : Loss =  0.34007388  Acc:  0.90043336 Val_loss =  0.41065922 Val_acc =  0.8755\n",
            "Iteration  266 : Loss =  0.3386267  Acc:  0.90173334 Val_loss =  0.409402 Val_acc =  0.8781\n",
            "Iteration  267 : Loss =  0.3362597  Acc:  0.90236664 Val_loss =  0.4077136 Val_acc =  0.8771\n",
            "Iteration  268 : Loss =  0.334951  Acc:  0.9033 Val_loss =  0.40572214 Val_acc =  0.8801\n",
            "Iteration  269 : Loss =  0.33456573  Acc:  0.90335 Val_loss =  0.4061588 Val_acc =  0.8777\n",
            "Iteration  270 : Loss =  0.33478022  Acc:  0.90318334 Val_loss =  0.40670273 Val_acc =  0.8786\n",
            "Iteration  271 : Loss =  0.33514875  Acc:  0.9028 Val_loss =  0.40677392 Val_acc =  0.878\n",
            "Iteration  272 : Loss =  0.3340971  Acc:  0.90288335 Val_loss =  0.40660793 Val_acc =  0.8772\n",
            "Iteration  273 : Loss =  0.3330291  Acc:  0.904 Val_loss =  0.40553325 Val_acc =  0.8792\n",
            "Iteration  274 : Loss =  0.33261952  Acc:  0.90325 Val_loss =  0.4048565 Val_acc =  0.8777\n",
            "Iteration  275 : Loss =  0.33242702  Acc:  0.90433335 Val_loss =  0.4059107 Val_acc =  0.8795\n",
            "Iteration  276 : Loss =  0.3317483  Acc:  0.9045 Val_loss =  0.4041713 Val_acc =  0.8789\n",
            "Iteration  277 : Loss =  0.3301307  Acc:  0.90473336 Val_loss =  0.40355805 Val_acc =  0.8793\n",
            "Iteration  278 : Loss =  0.32933873  Acc:  0.90575 Val_loss =  0.40293574 Val_acc =  0.88\n",
            "Iteration  279 : Loss =  0.32954806  Acc:  0.90498334 Val_loss =  0.40247917 Val_acc =  0.8788\n",
            "Iteration  280 : Loss =  0.3297853  Acc:  0.9052167 Val_loss =  0.40418032 Val_acc =  0.8799\n",
            "Iteration  281 : Loss =  0.32969603  Acc:  0.9040667 Val_loss =  0.40327248 Val_acc =  0.8783\n",
            "Iteration  282 : Loss =  0.32923552  Acc:  0.9052333 Val_loss =  0.40352133 Val_acc =  0.8796\n",
            "Iteration  283 : Loss =  0.32883656  Acc:  0.90455 Val_loss =  0.40345305 Val_acc =  0.8789\n",
            "Iteration  284 : Loss =  0.32788715  Acc:  0.9058667 Val_loss =  0.4022206 Val_acc =  0.8798\n",
            "Iteration  285 : Loss =  0.3266884  Acc:  0.90613335 Val_loss =  0.40164244 Val_acc =  0.8789\n",
            "Iteration  286 : Loss =  0.3258847  Acc:  0.90681666 Val_loss =  0.40095058 Val_acc =  0.8801\n",
            "Iteration  287 : Loss =  0.325746  Acc:  0.90681666 Val_loss =  0.40073803 Val_acc =  0.8796\n",
            "Iteration  288 : Loss =  0.32580978  Acc:  0.90638334 Val_loss =  0.40161976 Val_acc =  0.8795\n",
            "Iteration  289 : Loss =  0.325572  Acc:  0.9066333 Val_loss =  0.401026 Val_acc =  0.8801\n",
            "Iteration  290 : Loss =  0.32514495  Acc:  0.9062333 Val_loss =  0.40113354 Val_acc =  0.8792\n",
            "Iteration  291 : Loss =  0.32470378  Acc:  0.9071 Val_loss =  0.40106204 Val_acc =  0.8797\n",
            "Iteration  292 : Loss =  0.32439342  Acc:  0.9061667 Val_loss =  0.40037018 Val_acc =  0.8791\n",
            "Iteration  293 : Loss =  0.3238831  Acc:  0.9075 Val_loss =  0.40086976 Val_acc =  0.88\n",
            "Iteration  294 : Loss =  0.32332718  Acc:  0.90671664 Val_loss =  0.39969465 Val_acc =  0.8793\n",
            "Iteration  295 : Loss =  0.32288224  Acc:  0.90775 Val_loss =  0.400011 Val_acc =  0.8811\n",
            "Iteration  296 : Loss =  0.3227976  Acc:  0.9073167 Val_loss =  0.39989024 Val_acc =  0.8805\n",
            "Iteration  297 : Loss =  0.3233013  Acc:  0.90746665 Val_loss =  0.40067104 Val_acc =  0.8807\n",
            "Iteration  298 : Loss =  0.32417455  Acc:  0.90681666 Val_loss =  0.40181112 Val_acc =  0.878\n",
            "Iteration  299 : Loss =  0.32687357  Acc:  0.9054833 Val_loss =  0.40505058 Val_acc =  0.8805\n",
            "Iteration  300 : Loss =  0.3291155  Acc:  0.90455 Val_loss =  0.4068783 Val_acc =  0.8746\n",
            "Iteration  301 : Loss =  0.33588588  Acc:  0.9020333 Val_loss =  0.41516244 Val_acc =  0.8771\n",
            "Iteration  302 : Loss =  0.33015066  Acc:  0.9043667 Val_loss =  0.40786925 Val_acc =  0.8738\n",
            "Iteration  303 : Loss =  0.32580063  Acc:  0.9061 Val_loss =  0.4047381 Val_acc =  0.8797\n",
            "Iteration  304 : Loss =  0.31984162  Acc:  0.90816665 Val_loss =  0.39811558 Val_acc =  0.8804\n",
            "Iteration  305 : Loss =  0.32097217  Acc:  0.9076833 Val_loss =  0.39936838 Val_acc =  0.8797\n",
            "Iteration  306 : Loss =  0.3261654  Acc:  0.90561664 Val_loss =  0.40594682 Val_acc =  0.8786\n",
            "Iteration  307 : Loss =  0.32464612  Acc:  0.90578336 Val_loss =  0.40368426 Val_acc =  0.8771\n",
            "Iteration  308 : Loss =  0.32164747  Acc:  0.90785 Val_loss =  0.4014827 Val_acc =  0.8802\n",
            "Iteration  309 : Loss =  0.31809664  Acc:  0.9090833 Val_loss =  0.39815226 Val_acc =  0.88\n",
            "Iteration  310 : Loss =  0.3185829  Acc:  0.90915 Val_loss =  0.3980993 Val_acc =  0.8803\n",
            "Iteration  311 : Loss =  0.32094967  Acc:  0.9076667 Val_loss =  0.4018074 Val_acc =  0.8804\n",
            "Iteration  312 : Loss =  0.32027492  Acc:  0.9077833 Val_loss =  0.40057212 Val_acc =  0.8791\n",
            "Iteration  313 : Loss =  0.31857914  Acc:  0.9094333 Val_loss =  0.3990226 Val_acc =  0.881\n",
            "Iteration  314 : Loss =  0.31679857  Acc:  0.9098 Val_loss =  0.398081 Val_acc =  0.8804\n",
            "Iteration  315 : Loss =  0.31657666  Acc:  0.9098833 Val_loss =  0.39697126 Val_acc =  0.8814\n",
            "Iteration  316 : Loss =  0.317142  Acc:  0.90965 Val_loss =  0.39889282 Val_acc =  0.8811\n",
            "Iteration  317 : Loss =  0.31699982  Acc:  0.90903336 Val_loss =  0.39848086 Val_acc =  0.8798\n",
            "Iteration  318 : Loss =  0.3166014  Acc:  0.9102167 Val_loss =  0.3979388 Val_acc =  0.8815\n",
            "Iteration  319 : Loss =  0.31556445  Acc:  0.91015 Val_loss =  0.39800364 Val_acc =  0.88\n",
            "Iteration  320 : Loss =  0.31455013  Acc:  0.91071665 Val_loss =  0.3960495 Val_acc =  0.8816\n",
            "Iteration  321 : Loss =  0.3140249  Acc:  0.91116667 Val_loss =  0.39646676 Val_acc =  0.8814\n",
            "Iteration  322 : Loss =  0.314176  Acc:  0.9105 Val_loss =  0.3967177 Val_acc =  0.8806\n",
            "Iteration  323 : Loss =  0.3145487  Acc:  0.91038334 Val_loss =  0.39673942 Val_acc =  0.8808\n",
            "Iteration  324 : Loss =  0.31417668  Acc:  0.9109833 Val_loss =  0.39752325 Val_acc =  0.88\n",
            "Iteration  325 : Loss =  0.31321958  Acc:  0.91083336 Val_loss =  0.3957447 Val_acc =  0.8811\n",
            "Iteration  326 : Loss =  0.31238025  Acc:  0.91146666 Val_loss =  0.39583147 Val_acc =  0.8815\n",
            "Iteration  327 : Loss =  0.31253633  Acc:  0.9111 Val_loss =  0.39594212 Val_acc =  0.8801\n",
            "Iteration  328 : Loss =  0.31359652  Acc:  0.91035 Val_loss =  0.39707693 Val_acc =  0.8816\n",
            "Iteration  329 : Loss =  0.31534022  Acc:  0.90938336 Val_loss =  0.3993187 Val_acc =  0.8776\n",
            "Iteration  330 : Loss =  0.31753123  Acc:  0.9084 Val_loss =  0.40185794 Val_acc =  0.88\n",
            "Iteration  331 : Loss =  0.3221988  Acc:  0.9058167 Val_loss =  0.40598983 Val_acc =  0.8731\n",
            "Iteration  332 : Loss =  0.3243663  Acc:  0.9062 Val_loss =  0.40992928 Val_acc =  0.8764\n",
            "Iteration  333 : Loss =  0.32751814  Acc:  0.9034 Val_loss =  0.4110288 Val_acc =  0.8715\n",
            "Iteration  334 : Loss =  0.31807774  Acc:  0.9093 Val_loss =  0.40343922 Val_acc =  0.8791\n",
            "Iteration  335 : Loss =  0.31117487  Acc:  0.91145 Val_loss =  0.3958268 Val_acc =  0.8808\n",
            "Iteration  336 : Loss =  0.3123656  Acc:  0.91015 Val_loss =  0.3964494 Val_acc =  0.8798\n",
            "Iteration  337 : Loss =  0.3162114  Acc:  0.90956664 Val_loss =  0.40243998 Val_acc =  0.8805\n",
            "Iteration  338 : Loss =  0.3161042  Acc:  0.9084167 Val_loss =  0.40126523 Val_acc =  0.8766\n",
            "Iteration  339 : Loss =  0.31082278  Acc:  0.912 Val_loss =  0.39643353 Val_acc =  0.882\n",
            "Iteration  340 : Loss =  0.30966976  Acc:  0.9123333 Val_loss =  0.39671206 Val_acc =  0.8821\n",
            "Iteration  341 : Loss =  0.3116578  Acc:  0.9102833 Val_loss =  0.3972085 Val_acc =  0.8797\n",
            "Iteration  342 : Loss =  0.31207934  Acc:  0.91113335 Val_loss =  0.39884606 Val_acc =  0.8812\n",
            "Iteration  343 : Loss =  0.31071582  Acc:  0.9112 Val_loss =  0.39767647 Val_acc =  0.8793\n",
            "Iteration  344 : Loss =  0.3084128  Acc:  0.91265 Val_loss =  0.3944748 Val_acc =  0.883\n",
            "Iteration  345 : Loss =  0.30742973  Acc:  0.9133833 Val_loss =  0.39495504 Val_acc =  0.8825\n",
            "Iteration  346 : Loss =  0.30862147  Acc:  0.91215 Val_loss =  0.39555806 Val_acc =  0.8791\n",
            "Iteration  347 : Loss =  0.30908227  Acc:  0.9119833 Val_loss =  0.39605507 Val_acc =  0.8822\n",
            "Iteration  348 : Loss =  0.30749422  Acc:  0.9127833 Val_loss =  0.395606 Val_acc =  0.8798\n",
            "Iteration  349 : Loss =  0.30535895  Acc:  0.9139 Val_loss =  0.3928237 Val_acc =  0.8822\n",
            "Iteration  350 : Loss =  0.30565396  Acc:  0.91375 Val_loss =  0.39355755 Val_acc =  0.882\n",
            "Iteration  351 : Loss =  0.30696994  Acc:  0.9130667 Val_loss =  0.39553243 Val_acc =  0.8787\n",
            "Iteration  352 : Loss =  0.3062275  Acc:  0.91333336 Val_loss =  0.39422098 Val_acc =  0.8822\n",
            "Iteration  353 : Loss =  0.3043721  Acc:  0.9147 Val_loss =  0.3929104 Val_acc =  0.8809\n",
            "Iteration  354 : Loss =  0.30396217  Acc:  0.91485 Val_loss =  0.39286652 Val_acc =  0.882\n",
            "Iteration  355 : Loss =  0.30460227  Acc:  0.91426665 Val_loss =  0.39294252 Val_acc =  0.8827\n",
            "Iteration  356 : Loss =  0.30445948  Acc:  0.91445 Val_loss =  0.39372927 Val_acc =  0.8808\n",
            "Iteration  357 : Loss =  0.30350065  Acc:  0.91508335 Val_loss =  0.39289948 Val_acc =  0.8821\n",
            "Iteration  358 : Loss =  0.30291742  Acc:  0.91468334 Val_loss =  0.39187565 Val_acc =  0.8818\n",
            "Iteration  359 : Loss =  0.30260104  Acc:  0.9156833 Val_loss =  0.3927106 Val_acc =  0.8819\n",
            "Iteration  360 : Loss =  0.30223927  Acc:  0.9153 Val_loss =  0.39189416 Val_acc =  0.8835\n",
            "Iteration  361 : Loss =  0.3020866  Acc:  0.9151667 Val_loss =  0.39191306 Val_acc =  0.8815\n",
            "Iteration  362 : Loss =  0.30204737  Acc:  0.91575 Val_loss =  0.39271653 Val_acc =  0.8826\n",
            "Iteration  363 : Loss =  0.30172515  Acc:  0.9149167 Val_loss =  0.39168936 Val_acc =  0.8826\n",
            "Iteration  364 : Loss =  0.30090308  Acc:  0.9162167 Val_loss =  0.39186475 Val_acc =  0.883\n",
            "Iteration  365 : Loss =  0.30036962  Acc:  0.9163833 Val_loss =  0.39120358 Val_acc =  0.8824\n",
            "Iteration  366 : Loss =  0.30034956  Acc:  0.91601664 Val_loss =  0.39103088 Val_acc =  0.8814\n",
            "Iteration  367 : Loss =  0.30043224  Acc:  0.91611665 Val_loss =  0.39213678 Val_acc =  0.8828\n",
            "Iteration  368 : Loss =  0.3002535  Acc:  0.9159167 Val_loss =  0.39121807 Val_acc =  0.8814\n",
            "Iteration  369 : Loss =  0.299989  Acc:  0.91645 Val_loss =  0.391792 Val_acc =  0.8828\n",
            "Iteration  370 : Loss =  0.29989013  Acc:  0.91648334 Val_loss =  0.39170626 Val_acc =  0.8801\n",
            "Iteration  371 : Loss =  0.30021408  Acc:  0.91588336 Val_loss =  0.39209974 Val_acc =  0.8832\n",
            "Iteration  372 : Loss =  0.30070066  Acc:  0.9158667 Val_loss =  0.3931288 Val_acc =  0.8806\n",
            "Iteration  373 : Loss =  0.3023354  Acc:  0.91465 Val_loss =  0.3947938 Val_acc =  0.8831\n",
            "Iteration  374 : Loss =  0.30424994  Acc:  0.91398335 Val_loss =  0.39694175 Val_acc =  0.8815\n",
            "Iteration  375 : Loss =  0.3086706  Acc:  0.91141665 Val_loss =  0.40188172 Val_acc =  0.8808\n",
            "Iteration  376 : Loss =  0.3080502  Acc:  0.9124 Val_loss =  0.40088338 Val_acc =  0.8798\n",
            "Iteration  377 : Loss =  0.30685753  Acc:  0.9123 Val_loss =  0.40008342 Val_acc =  0.8814\n",
            "Iteration  378 : Loss =  0.2998691  Acc:  0.9159667 Val_loss =  0.39289582 Val_acc =  0.8813\n",
            "Iteration  379 : Loss =  0.29680216  Acc:  0.91753334 Val_loss =  0.38963395 Val_acc =  0.8847\n",
            "Iteration  380 : Loss =  0.29852307  Acc:  0.9161 Val_loss =  0.39212754 Val_acc =  0.8817\n",
            "Iteration  381 : Loss =  0.30120012  Acc:  0.91578335 Val_loss =  0.39514488 Val_acc =  0.8815\n",
            "Iteration  382 : Loss =  0.3022849  Acc:  0.91386664 Val_loss =  0.39629066 Val_acc =  0.8823\n",
            "Iteration  383 : Loss =  0.29934236  Acc:  0.9162 Val_loss =  0.39389604 Val_acc =  0.8812\n",
            "Iteration  384 : Loss =  0.29710075  Acc:  0.91705 Val_loss =  0.39161512 Val_acc =  0.8841\n",
            "Iteration  385 : Loss =  0.2965705  Acc:  0.91675 Val_loss =  0.39125884 Val_acc =  0.8812\n",
            "Iteration  386 : Loss =  0.29758695  Acc:  0.91728336 Val_loss =  0.392709 Val_acc =  0.8833\n",
            "Iteration  387 : Loss =  0.29809415  Acc:  0.9159167 Val_loss =  0.39277324 Val_acc =  0.8821\n",
            "Iteration  388 : Loss =  0.29664063  Acc:  0.91785 Val_loss =  0.39243877 Val_acc =  0.8828\n",
            "Iteration  389 : Loss =  0.29438442  Acc:  0.91815 Val_loss =  0.38948017 Val_acc =  0.8829\n",
            "Iteration  390 : Loss =  0.29354376  Acc:  0.9184333 Val_loss =  0.38927725 Val_acc =  0.8832\n",
            "Iteration  391 : Loss =  0.29435495  Acc:  0.91825 Val_loss =  0.39061505 Val_acc =  0.8831\n",
            "Iteration  392 : Loss =  0.29560235  Acc:  0.9169833 Val_loss =  0.39124903 Val_acc =  0.882\n",
            "Iteration  393 : Loss =  0.29536688  Acc:  0.91805 Val_loss =  0.3922242 Val_acc =  0.8834\n",
            "Iteration  394 : Loss =  0.29426712  Acc:  0.91781664 Val_loss =  0.39009136 Val_acc =  0.8819\n",
            "Iteration  395 : Loss =  0.29301202  Acc:  0.9191167 Val_loss =  0.3896675 Val_acc =  0.8844\n",
            "Iteration  396 : Loss =  0.29283196  Acc:  0.9188167 Val_loss =  0.3896074 Val_acc =  0.8819\n",
            "Iteration  397 : Loss =  0.29347473  Acc:  0.91798335 Val_loss =  0.39011353 Val_acc =  0.8839\n",
            "Iteration  398 : Loss =  0.2935429  Acc:  0.9185333 Val_loss =  0.3907842 Val_acc =  0.8822\n",
            "Iteration  399 : Loss =  0.29312655  Acc:  0.91788334 Val_loss =  0.39044392 Val_acc =  0.8836\n",
            "Iteration  400 : Loss =  0.29187077  Acc:  0.9191667 Val_loss =  0.38930818 Val_acc =  0.8823\n",
            "Iteration  401 : Loss =  0.29103473  Acc:  0.9195333 Val_loss =  0.38892874 Val_acc =  0.8836\n",
            "Iteration  402 : Loss =  0.29062152  Acc:  0.9196 Val_loss =  0.38821757 Val_acc =  0.8834\n",
            "Iteration  403 : Loss =  0.2906502  Acc:  0.91983336 Val_loss =  0.38911942 Val_acc =  0.8836\n",
            "Iteration  404 : Loss =  0.2908107  Acc:  0.91928333 Val_loss =  0.38868433 Val_acc =  0.8823\n",
            "Iteration  405 : Loss =  0.2906834  Acc:  0.92005 Val_loss =  0.38917145 Val_acc =  0.8841\n",
            "Iteration  406 : Loss =  0.29035422  Acc:  0.91973335 Val_loss =  0.38875267 Val_acc =  0.8825\n",
            "Iteration  407 : Loss =  0.2898383  Acc:  0.92053336 Val_loss =  0.38853252 Val_acc =  0.8847\n",
            "Iteration  408 : Loss =  0.28955442  Acc:  0.9199333 Val_loss =  0.3884513 Val_acc =  0.8827\n",
            "Iteration  409 : Loss =  0.28973323  Acc:  0.91995 Val_loss =  0.38892448 Val_acc =  0.8855\n",
            "Iteration  410 : Loss =  0.2904554  Acc:  0.9188667 Val_loss =  0.38967296 Val_acc =  0.8823\n",
            "Iteration  411 : Loss =  0.29259565  Acc:  0.91816664 Val_loss =  0.3925729 Val_acc =  0.8848\n",
            "Iteration  412 : Loss =  0.29400972  Acc:  0.9173333 Val_loss =  0.39331883 Val_acc =  0.882\n",
            "Iteration  413 : Loss =  0.2977071  Acc:  0.91655 Val_loss =  0.39824894 Val_acc =  0.8839\n",
            "Iteration  414 : Loss =  0.29555154  Acc:  0.916 Val_loss =  0.39523554 Val_acc =  0.8807\n",
            "Iteration  415 : Loss =  0.29392156  Acc:  0.91765 Val_loss =  0.3941831 Val_acc =  0.8842\n",
            "Iteration  416 : Loss =  0.28957736  Acc:  0.9191833 Val_loss =  0.389615 Val_acc =  0.882\n",
            "Iteration  417 : Loss =  0.2873714  Acc:  0.9216833 Val_loss =  0.38769513 Val_acc =  0.8849\n",
            "Iteration  418 : Loss =  0.28787413  Acc:  0.9201 Val_loss =  0.3887425 Val_acc =  0.8831\n",
            "Iteration  419 : Loss =  0.2894565  Acc:  0.92025 Val_loss =  0.39045772 Val_acc =  0.8835\n",
            "Iteration  420 : Loss =  0.29173928  Acc:  0.9184333 Val_loss =  0.39321613 Val_acc =  0.8829\n",
            "Iteration  421 : Loss =  0.290564  Acc:  0.91863334 Val_loss =  0.39201665 Val_acc =  0.8826\n",
            "Iteration  422 : Loss =  0.28928894  Acc:  0.92 Val_loss =  0.39085203 Val_acc =  0.8846\n",
            "Iteration  423 : Loss =  0.2866943  Acc:  0.92053336 Val_loss =  0.38829756 Val_acc =  0.882\n",
            "Iteration  424 : Loss =  0.2854372  Acc:  0.92215 Val_loss =  0.3872432 Val_acc =  0.8848\n",
            "Iteration  425 : Loss =  0.28588125  Acc:  0.92071664 Val_loss =  0.38756233 Val_acc =  0.8832\n",
            "Iteration  426 : Loss =  0.2870775  Acc:  0.9214 Val_loss =  0.38964343 Val_acc =  0.8838\n",
            "Iteration  427 : Loss =  0.28766963  Acc:  0.92001665 Val_loss =  0.38963047 Val_acc =  0.8845\n",
            "Iteration  428 : Loss =  0.28654563  Acc:  0.9216 Val_loss =  0.38961112 Val_acc =  0.8832\n",
            "Iteration  429 : Loss =  0.28473103  Acc:  0.92115 Val_loss =  0.38732466 Val_acc =  0.8835\n",
            "Iteration  430 : Loss =  0.28315896  Acc:  0.92315 Val_loss =  0.38582802 Val_acc =  0.8843\n",
            "Iteration  431 : Loss =  0.28275728  Acc:  0.92321664 Val_loss =  0.38609925 Val_acc =  0.8848\n",
            "Iteration  432 : Loss =  0.28337443  Acc:  0.9223167 Val_loss =  0.38634717 Val_acc =  0.8841\n",
            "Iteration  433 : Loss =  0.2842508  Acc:  0.9227333 Val_loss =  0.38812408 Val_acc =  0.8841\n",
            "Iteration  434 : Loss =  0.28466797  Acc:  0.92115 Val_loss =  0.3880382 Val_acc =  0.8847\n",
            "Iteration  435 : Loss =  0.28440076  Acc:  0.9224667 Val_loss =  0.3885665 Val_acc =  0.8836\n",
            "Iteration  436 : Loss =  0.28353348  Acc:  0.9217333 Val_loss =  0.38729978 Val_acc =  0.8833\n",
            "Iteration  437 : Loss =  0.2825144  Acc:  0.9234 Val_loss =  0.38674417 Val_acc =  0.8858\n",
            "Iteration  438 : Loss =  0.28193435  Acc:  0.9231833 Val_loss =  0.38629282 Val_acc =  0.884\n",
            "Iteration  439 : Loss =  0.28195855  Acc:  0.92356664 Val_loss =  0.3864264 Val_acc =  0.8854\n",
            "Iteration  440 : Loss =  0.2824493  Acc:  0.92245 Val_loss =  0.38729355 Val_acc =  0.8837\n",
            "Iteration  441 : Loss =  0.2834707  Acc:  0.9223167 Val_loss =  0.3888418 Val_acc =  0.8848\n",
            "Iteration  442 : Loss =  0.28379592  Acc:  0.9213167 Val_loss =  0.3887518 Val_acc =  0.8837\n",
            "Iteration  443 : Loss =  0.28467023  Acc:  0.92151666 Val_loss =  0.39062664 Val_acc =  0.8844\n",
            "Iteration  444 : Loss =  0.283927  Acc:  0.92118335 Val_loss =  0.38875803 Val_acc =  0.8837\n",
            "Iteration  445 : Loss =  0.2832806  Acc:  0.92226666 Val_loss =  0.38932014 Val_acc =  0.8844\n",
            "Iteration  446 : Loss =  0.28245834  Acc:  0.9224 Val_loss =  0.38763738 Val_acc =  0.8839\n",
            "Iteration  447 : Loss =  0.28170422  Acc:  0.92368335 Val_loss =  0.388202 Val_acc =  0.8847\n",
            "Iteration  448 : Loss =  0.28188726  Acc:  0.9228 Val_loss =  0.38777578 Val_acc =  0.8842\n",
            "Iteration  449 : Loss =  0.2820402  Acc:  0.9234833 Val_loss =  0.38865224 Val_acc =  0.8853\n",
            "Iteration  450 : Loss =  0.28202945  Acc:  0.9227333 Val_loss =  0.3885687 Val_acc =  0.8839\n",
            "Iteration  451 : Loss =  0.28110158  Acc:  0.9237 Val_loss =  0.38762257 Val_acc =  0.8853\n",
            "Iteration  452 : Loss =  0.27969435  Acc:  0.92385 Val_loss =  0.38660255 Val_acc =  0.8852\n",
            "Iteration  453 : Loss =  0.2781071  Acc:  0.92506665 Val_loss =  0.38501173 Val_acc =  0.8849\n",
            "Iteration  454 : Loss =  0.27733976  Acc:  0.92506665 Val_loss =  0.38446033 Val_acc =  0.8859\n",
            "Iteration  455 : Loss =  0.27748522  Acc:  0.9254 Val_loss =  0.3847975 Val_acc =  0.8847\n",
            "Iteration  456 : Loss =  0.2782776  Acc:  0.9249833 Val_loss =  0.38577384 Val_acc =  0.8861\n",
            "Iteration  457 : Loss =  0.27939135  Acc:  0.9237 Val_loss =  0.38704225 Val_acc =  0.8852\n",
            "Iteration  458 : Loss =  0.28014863  Acc:  0.92435 Val_loss =  0.38842002 Val_acc =  0.8861\n",
            "Iteration  459 : Loss =  0.28085425  Acc:  0.92251664 Val_loss =  0.38869768 Val_acc =  0.884\n",
            "Iteration  460 : Loss =  0.28039998  Acc:  0.9242 Val_loss =  0.38928002 Val_acc =  0.8848\n",
            "Iteration  461 : Loss =  0.28010854  Acc:  0.92263335 Val_loss =  0.38785547 Val_acc =  0.8838\n",
            "Iteration  462 : Loss =  0.2788108  Acc:  0.9245167 Val_loss =  0.3880893 Val_acc =  0.8848\n",
            "Iteration  463 : Loss =  0.27816972  Acc:  0.9238333 Val_loss =  0.38616794 Val_acc =  0.8846\n",
            "Iteration  464 : Loss =  0.27813524  Acc:  0.92425 Val_loss =  0.38766602 Val_acc =  0.8849\n",
            "Iteration  465 : Loss =  0.27778405  Acc:  0.9239 Val_loss =  0.38678712 Val_acc =  0.8844\n",
            "Iteration  466 : Loss =  0.27844524  Acc:  0.92378336 Val_loss =  0.3876959 Val_acc =  0.8856\n",
            "Iteration  467 : Loss =  0.27810672  Acc:  0.92455 Val_loss =  0.38805798 Val_acc =  0.8846\n",
            "Iteration  468 : Loss =  0.27787024  Acc:  0.9242 Val_loss =  0.386951 Val_acc =  0.886\n",
            "Iteration  469 : Loss =  0.27684495  Acc:  0.9253833 Val_loss =  0.38744044 Val_acc =  0.8844\n",
            "Iteration  470 : Loss =  0.2753727  Acc:  0.92536664 Val_loss =  0.38468364 Val_acc =  0.8855\n",
            "Iteration  471 : Loss =  0.2738052  Acc:  0.9271 Val_loss =  0.3843616 Val_acc =  0.8843\n",
            "Iteration  472 : Loss =  0.27304706  Acc:  0.92693335 Val_loss =  0.38316777 Val_acc =  0.8852\n",
            "Iteration  473 : Loss =  0.27332738  Acc:  0.92653334 Val_loss =  0.3836631 Val_acc =  0.8856\n",
            "Iteration  474 : Loss =  0.27421135  Acc:  0.9270833 Val_loss =  0.38534576 Val_acc =  0.885\n",
            "Iteration  475 : Loss =  0.27514172  Acc:  0.9252833 Val_loss =  0.38553408 Val_acc =  0.8858\n",
            "Iteration  476 : Loss =  0.2754302  Acc:  0.92583334 Val_loss =  0.38716304 Val_acc =  0.8842\n",
            "Iteration  477 : Loss =  0.27521488  Acc:  0.9251 Val_loss =  0.38604224 Val_acc =  0.8855\n",
            "Iteration  478 : Loss =  0.27417383  Acc:  0.9261 Val_loss =  0.3858018 Val_acc =  0.8845\n",
            "Iteration  479 : Loss =  0.2737984  Acc:  0.92576665 Val_loss =  0.3854542 Val_acc =  0.8853\n",
            "Iteration  480 : Loss =  0.2736405  Acc:  0.9259833 Val_loss =  0.3847261 Val_acc =  0.8851\n",
            "Iteration  481 : Loss =  0.27447456  Acc:  0.926 Val_loss =  0.3871415 Val_acc =  0.8848\n",
            "Iteration  482 : Loss =  0.27576178  Acc:  0.92481667 Val_loss =  0.38694823 Val_acc =  0.8839\n",
            "Iteration  483 : Loss =  0.2765467  Acc:  0.9252667 Val_loss =  0.3898117 Val_acc =  0.8858\n",
            "Iteration  484 : Loss =  0.2781125  Acc:  0.9232 Val_loss =  0.3899317 Val_acc =  0.8834\n",
            "Iteration  485 : Loss =  0.27766263  Acc:  0.92476666 Val_loss =  0.39082032 Val_acc =  0.8854\n",
            "Iteration  486 : Loss =  0.27751154  Acc:  0.9234 Val_loss =  0.3899122 Val_acc =  0.8839\n",
            "Iteration  487 : Loss =  0.27505577  Acc:  0.92623335 Val_loss =  0.38770327 Val_acc =  0.8848\n",
            "Iteration  488 : Loss =  0.27237907  Acc:  0.92693335 Val_loss =  0.38585368 Val_acc =  0.8845\n",
            "Iteration  489 : Loss =  0.27010658  Acc:  0.9281167 Val_loss =  0.3829646 Val_acc =  0.8853\n",
            "Iteration  490 : Loss =  0.26979786  Acc:  0.9288 Val_loss =  0.38386965 Val_acc =  0.8866\n",
            "Iteration  491 : Loss =  0.27149174  Acc:  0.92696667 Val_loss =  0.3853236 Val_acc =  0.8848\n",
            "Iteration  492 : Loss =  0.27325976  Acc:  0.9266 Val_loss =  0.387223 Val_acc =  0.8864\n",
            "Iteration  493 : Loss =  0.27415124  Acc:  0.9253 Val_loss =  0.38831598 Val_acc =  0.8844\n",
            "Iteration  494 : Loss =  0.27260768  Acc:  0.9271167 Val_loss =  0.38679922 Val_acc =  0.8865\n",
            "Iteration  495 : Loss =  0.2707397  Acc:  0.9270667 Val_loss =  0.3844486 Val_acc =  0.8852\n",
            "Iteration  496 : Loss =  0.2694508  Acc:  0.92828333 Val_loss =  0.3840565 Val_acc =  0.8856\n",
            "Iteration  497 : Loss =  0.26942837  Acc:  0.9282 Val_loss =  0.38324344 Val_acc =  0.8858\n",
            "Iteration  498 : Loss =  0.27033037  Acc:  0.92765 Val_loss =  0.3853363 Val_acc =  0.8858\n",
            "Iteration  499 : Loss =  0.2708472  Acc:  0.92763335 Val_loss =  0.38571748 Val_acc =  0.8862\n",
            "Iteration  500 : Loss =  0.27139556  Acc:  0.92658335 Val_loss =  0.38636762 Val_acc =  0.8863\n",
            "Iteration  501 : Loss =  0.27176592  Acc:  0.9277667 Val_loss =  0.38766235 Val_acc =  0.8864\n",
            "Iteration  502 : Loss =  0.2718139  Acc:  0.9262667 Val_loss =  0.3865364 Val_acc =  0.8859\n",
            "Iteration  503 : Loss =  0.27071708  Acc:  0.928 Val_loss =  0.38678584 Val_acc =  0.8854\n",
            "Iteration  504 : Loss =  0.26912057  Acc:  0.92773336 Val_loss =  0.38415235 Val_acc =  0.8865\n",
            "Iteration  505 : Loss =  0.26794794  Acc:  0.92835 Val_loss =  0.38349947 Val_acc =  0.8853\n",
            "Iteration  506 : Loss =  0.26864335  Acc:  0.92795 Val_loss =  0.38508737 Val_acc =  0.886\n",
            "Iteration  507 : Loss =  0.27009752  Acc:  0.9267667 Val_loss =  0.38545653 Val_acc =  0.8849\n",
            "Iteration  508 : Loss =  0.2702711  Acc:  0.92758334 Val_loss =  0.38784823 Val_acc =  0.8855\n",
            "Iteration  509 : Loss =  0.26947734  Acc:  0.92721665 Val_loss =  0.38551337 Val_acc =  0.8851\n",
            "Iteration  510 : Loss =  0.26713672  Acc:  0.9291667 Val_loss =  0.38445663 Val_acc =  0.8861\n",
            "Iteration  511 : Loss =  0.26556647  Acc:  0.9296167 Val_loss =  0.38244832 Val_acc =  0.8862\n",
            "Iteration  512 : Loss =  0.26531968  Acc:  0.92971665 Val_loss =  0.3820318 Val_acc =  0.8857\n",
            "Iteration  513 : Loss =  0.2657972  Acc:  0.93058336 Val_loss =  0.38357687 Val_acc =  0.8855\n",
            "Iteration  514 : Loss =  0.26617473  Acc:  0.9291 Val_loss =  0.38319123 Val_acc =  0.8867\n",
            "Iteration  515 : Loss =  0.2660111  Acc:  0.9301 Val_loss =  0.3841265 Val_acc =  0.887\n",
            "Iteration  516 : Loss =  0.26560533  Acc:  0.9296333 Val_loss =  0.38322943 Val_acc =  0.8856\n",
            "Iteration  517 : Loss =  0.26507074  Acc:  0.93065 Val_loss =  0.38295385 Val_acc =  0.8873\n",
            "Iteration  518 : Loss =  0.26450437  Acc:  0.9303333 Val_loss =  0.38265052 Val_acc =  0.8851\n",
            "Iteration  519 : Loss =  0.26390254  Acc:  0.9310333 Val_loss =  0.38171402 Val_acc =  0.8873\n",
            "Iteration  520 : Loss =  0.26355192  Acc:  0.9310833 Val_loss =  0.38220125 Val_acc =  0.8866\n",
            "Iteration  521 : Loss =  0.2636457  Acc:  0.9304 Val_loss =  0.38200876 Val_acc =  0.8866\n",
            "Iteration  522 : Loss =  0.2644269  Acc:  0.93023336 Val_loss =  0.38343567 Val_acc =  0.886\n",
            "Iteration  523 : Loss =  0.26568487  Acc:  0.92868334 Val_loss =  0.38468117 Val_acc =  0.8872\n",
            "Iteration  524 : Loss =  0.2677782  Acc:  0.92828333 Val_loss =  0.3871674 Val_acc =  0.8854\n",
            "Iteration  525 : Loss =  0.26960504  Acc:  0.9262 Val_loss =  0.388942 Val_acc =  0.8854\n",
            "Iteration  526 : Loss =  0.27386025  Acc:  0.9245333 Val_loss =  0.3937458 Val_acc =  0.8853\n",
            "Iteration  527 : Loss =  0.27321282  Acc:  0.92455 Val_loss =  0.39271328 Val_acc =  0.883\n",
            "Iteration  528 : Loss =  0.276365  Acc:  0.92373335 Val_loss =  0.39654538 Val_acc =  0.8851\n",
            "Iteration  529 : Loss =  0.2700615  Acc:  0.9270667 Val_loss =  0.38966683 Val_acc =  0.8861\n",
            "Iteration  530 : Loss =  0.26663062  Acc:  0.9284 Val_loss =  0.38623005 Val_acc =  0.8871\n",
            "Iteration  531 : Loss =  0.26497203  Acc:  0.9301 Val_loss =  0.3851212 Val_acc =  0.8875\n",
            "Iteration  532 : Loss =  0.2664523  Acc:  0.92775 Val_loss =  0.38606408 Val_acc =  0.8858\n",
            "Iteration  533 : Loss =  0.26863146  Acc:  0.9274167 Val_loss =  0.3897794 Val_acc =  0.887\n",
            "Iteration  534 : Loss =  0.2672013  Acc:  0.92761666 Val_loss =  0.38748854 Val_acc =  0.8858\n",
            "Iteration  535 : Loss =  0.2667103  Acc:  0.92793334 Val_loss =  0.38804364 Val_acc =  0.8864\n",
            "Iteration  536 : Loss =  0.2640587  Acc:  0.9305 Val_loss =  0.38531682 Val_acc =  0.8862\n",
            "Iteration  537 : Loss =  0.2632228  Acc:  0.93013334 Val_loss =  0.38401386 Val_acc =  0.8864\n",
            "Iteration  538 : Loss =  0.26324242  Acc:  0.93075 Val_loss =  0.3851311 Val_acc =  0.888\n",
            "Iteration  539 : Loss =  0.26361176  Acc:  0.92925 Val_loss =  0.38451794 Val_acc =  0.8865\n",
            "Iteration  540 : Loss =  0.26283467  Acc:  0.93095 Val_loss =  0.3848875 Val_acc =  0.887\n",
            "Iteration  541 : Loss =  0.26127312  Acc:  0.9311 Val_loss =  0.38295066 Val_acc =  0.8871\n",
            "Iteration  542 : Loss =  0.2608449  Acc:  0.9313667 Val_loss =  0.3827169 Val_acc =  0.8873\n",
            "Iteration  543 : Loss =  0.2610252  Acc:  0.9324833 Val_loss =  0.38384756 Val_acc =  0.8867\n",
            "Iteration  544 : Loss =  0.26139632  Acc:  0.93058336 Val_loss =  0.38327074 Val_acc =  0.8857\n",
            "Iteration  545 : Loss =  0.26091105  Acc:  0.93195 Val_loss =  0.38393274 Val_acc =  0.8883\n",
            "Iteration  546 : Loss =  0.2601521  Acc:  0.9313667 Val_loss =  0.3823322 Val_acc =  0.8862\n",
            "Iteration  547 : Loss =  0.25923887  Acc:  0.93245 Val_loss =  0.3818826 Val_acc =  0.8879\n",
            "Iteration  548 : Loss =  0.25879434  Acc:  0.93268335 Val_loss =  0.38157547 Val_acc =  0.8869\n",
            "Iteration  549 : Loss =  0.25892633  Acc:  0.9324833 Val_loss =  0.3816879 Val_acc =  0.8861\n",
            "Iteration  550 : Loss =  0.25909495  Acc:  0.9332333 Val_loss =  0.38289794 Val_acc =  0.8875\n",
            "Iteration  551 : Loss =  0.25898623  Acc:  0.9324333 Val_loss =  0.38220397 Val_acc =  0.8865\n",
            "Iteration  552 : Loss =  0.2585085  Acc:  0.93335 Val_loss =  0.382509 Val_acc =  0.8887\n",
            "Iteration  553 : Loss =  0.25794312  Acc:  0.9326 Val_loss =  0.3814689 Val_acc =  0.8856\n",
            "Iteration  554 : Loss =  0.25739136  Acc:  0.93401664 Val_loss =  0.38105693 Val_acc =  0.8881\n",
            "Iteration  555 : Loss =  0.25696367  Acc:  0.93315 Val_loss =  0.3810912 Val_acc =  0.8858\n",
            "Iteration  556 : Loss =  0.25668967  Acc:  0.93418336 Val_loss =  0.380718 Val_acc =  0.8868\n",
            "Iteration  557 : Loss =  0.2565353  Acc:  0.93378335 Val_loss =  0.38111806 Val_acc =  0.887\n",
            "Iteration  558 : Loss =  0.2565054  Acc:  0.9338 Val_loss =  0.38091135 Val_acc =  0.8861\n",
            "Iteration  559 : Loss =  0.25663126  Acc:  0.9338833 Val_loss =  0.38136664 Val_acc =  0.8878\n",
            "Iteration  560 : Loss =  0.2569058  Acc:  0.9335833 Val_loss =  0.3816191 Val_acc =  0.8871\n",
            "Iteration  561 : Loss =  0.2571593  Acc:  0.9335167 Val_loss =  0.38213268 Val_acc =  0.8871\n",
            "Iteration  562 : Loss =  0.25738272  Acc:  0.9328667 Val_loss =  0.38266572 Val_acc =  0.8875\n",
            "Iteration  563 : Loss =  0.25729442  Acc:  0.93348336 Val_loss =  0.38254964 Val_acc =  0.8867\n",
            "Iteration  564 : Loss =  0.25712347  Acc:  0.93291664 Val_loss =  0.382595 Val_acc =  0.8874\n",
            "Iteration  565 : Loss =  0.2566897  Acc:  0.93331665 Val_loss =  0.3822986 Val_acc =  0.8867\n",
            "Iteration  566 : Loss =  0.25646794  Acc:  0.9335 Val_loss =  0.38214174 Val_acc =  0.8865\n",
            "Iteration  567 : Loss =  0.25619143  Acc:  0.93365 Val_loss =  0.38226274 Val_acc =  0.8875\n",
            "Iteration  568 : Loss =  0.2563953  Acc:  0.9332167 Val_loss =  0.38242894 Val_acc =  0.8867\n",
            "Iteration  569 : Loss =  0.25660452  Acc:  0.9331 Val_loss =  0.38310167 Val_acc =  0.8873\n",
            "Iteration  570 : Loss =  0.2575585  Acc:  0.9325167 Val_loss =  0.38405448 Val_acc =  0.8865\n",
            "Iteration  571 : Loss =  0.2582059  Acc:  0.93126667 Val_loss =  0.3849081 Val_acc =  0.8867\n",
            "Iteration  572 : Loss =  0.25932884  Acc:  0.93165 Val_loss =  0.38641006 Val_acc =  0.8872\n",
            "Iteration  573 : Loss =  0.25938013  Acc:  0.93053335 Val_loss =  0.38624576 Val_acc =  0.8865\n",
            "Iteration  574 : Loss =  0.25917077  Acc:  0.93165 Val_loss =  0.38681203 Val_acc =  0.8879\n",
            "Iteration  575 : Loss =  0.25780866  Acc:  0.93161666 Val_loss =  0.3845408 Val_acc =  0.8874\n",
            "Iteration  576 : Loss =  0.256047  Acc:  0.93343335 Val_loss =  0.38406596 Val_acc =  0.8882\n",
            "Iteration  577 : Loss =  0.25452197  Acc:  0.93375 Val_loss =  0.38126957 Val_acc =  0.8872\n",
            "Iteration  578 : Loss =  0.25336322  Acc:  0.9353833 Val_loss =  0.38148466 Val_acc =  0.8886\n",
            "Iteration  579 : Loss =  0.25312138  Acc:  0.9346833 Val_loss =  0.38066423 Val_acc =  0.8875\n",
            "Iteration  580 : Loss =  0.25370395  Acc:  0.93451667 Val_loss =  0.38165855 Val_acc =  0.888\n",
            "Iteration  581 : Loss =  0.25527346  Acc:  0.93348336 Val_loss =  0.38370058 Val_acc =  0.8865\n",
            "Iteration  582 : Loss =  0.2564811  Acc:  0.9324 Val_loss =  0.38448092 Val_acc =  0.8871\n",
            "Iteration  583 : Loss =  0.25898457  Acc:  0.93165 Val_loss =  0.38799945 Val_acc =  0.8875\n",
            "Iteration  584 : Loss =  0.25792754  Acc:  0.93121666 Val_loss =  0.3859775 Val_acc =  0.8869\n",
            "Iteration  585 : Loss =  0.25747114  Acc:  0.93226665 Val_loss =  0.3866042 Val_acc =  0.8879\n",
            "Iteration  586 : Loss =  0.25384787  Acc:  0.9338667 Val_loss =  0.38215184 Val_acc =  0.8872\n",
            "Iteration  587 : Loss =  0.25156265  Acc:  0.93551666 Val_loss =  0.38046217 Val_acc =  0.8873\n",
            "Iteration  588 : Loss =  0.25086212  Acc:  0.93615 Val_loss =  0.3802107 Val_acc =  0.8887\n",
            "Iteration  589 : Loss =  0.2517975  Acc:  0.93483335 Val_loss =  0.38081497 Val_acc =  0.8872\n",
            "Iteration  590 : Loss =  0.25335848  Acc:  0.9344 Val_loss =  0.38363177 Val_acc =  0.8882\n",
            "Iteration  591 : Loss =  0.25462592  Acc:  0.9328833 Val_loss =  0.38388866 Val_acc =  0.8875\n",
            "Iteration  592 : Loss =  0.25517115  Acc:  0.93305 Val_loss =  0.38556242 Val_acc =  0.8886\n",
            "Iteration  593 : Loss =  0.2547147  Acc:  0.9328 Val_loss =  0.38420418 Val_acc =  0.888\n",
            "Iteration  594 : Loss =  0.25356779  Acc:  0.93408334 Val_loss =  0.38371578 Val_acc =  0.8881\n",
            "Iteration  595 : Loss =  0.25258335  Acc:  0.9344 Val_loss =  0.38270357 Val_acc =  0.8863\n",
            "Iteration  596 : Loss =  0.25186124  Acc:  0.93541664 Val_loss =  0.38199174 Val_acc =  0.8885\n",
            "Iteration  597 : Loss =  0.25191677  Acc:  0.93486667 Val_loss =  0.38289523 Val_acc =  0.8867\n",
            "Iteration  598 : Loss =  0.25261185  Acc:  0.93511665 Val_loss =  0.3830667 Val_acc =  0.8885\n",
            "Iteration  599 : Loss =  0.25445843  Acc:  0.93303335 Val_loss =  0.38579124 Val_acc =  0.8879\n",
            "Iteration  600 : Loss =  0.25638407  Acc:  0.9327833 Val_loss =  0.3876055 Val_acc =  0.8884\n",
            "Iteration  601 : Loss =  0.26087674  Acc:  0.92983335 Val_loss =  0.39179802 Val_acc =  0.8847\n",
            "Iteration  602 : Loss =  0.26384902  Acc:  0.9288833 Val_loss =  0.3959248 Val_acc =  0.8845\n",
            "Iteration  603 : Loss =  0.26407877  Acc:  0.9284833 Val_loss =  0.39410704 Val_acc =  0.8838\n",
            "Iteration  604 : Loss =  0.25780717  Acc:  0.9328333 Val_loss =  0.3896647 Val_acc =  0.8852\n",
            "Iteration  605 : Loss =  0.25062093  Acc:  0.93495 Val_loss =  0.38029218 Val_acc =  0.8874\n",
            "Iteration  606 : Loss =  0.25128645  Acc:  0.9350167 Val_loss =  0.38198578 Val_acc =  0.8883\n",
            "Iteration  607 : Loss =  0.25662416  Acc:  0.93235 Val_loss =  0.38880563 Val_acc =  0.8875\n",
            "Iteration  608 : Loss =  0.25797185  Acc:  0.93088335 Val_loss =  0.3891035 Val_acc =  0.8863\n",
            "Iteration  609 : Loss =  0.25212333  Acc:  0.93483335 Val_loss =  0.38505286 Val_acc =  0.8886\n",
            "Iteration  610 : Loss =  0.24778649  Acc:  0.93695 Val_loss =  0.3802996 Val_acc =  0.8879\n",
            "Iteration  611 : Loss =  0.24915415  Acc:  0.93626666 Val_loss =  0.3811007 Val_acc =  0.8874\n",
            "Iteration  612 : Loss =  0.25193107  Acc:  0.93515 Val_loss =  0.38521564 Val_acc =  0.8878\n",
            "Iteration  613 : Loss =  0.2517334  Acc:  0.9346667 Val_loss =  0.383818 Val_acc =  0.8886\n",
            "Iteration  614 : Loss =  0.24872911  Acc:  0.93633336 Val_loss =  0.38121697 Val_acc =  0.8891\n",
            "Iteration  615 : Loss =  0.24765389  Acc:  0.93738335 Val_loss =  0.38102472 Val_acc =  0.8886\n",
            "Iteration  616 : Loss =  0.24864048  Acc:  0.9357833 Val_loss =  0.38111 Val_acc =  0.8878\n",
            "Iteration  617 : Loss =  0.24899158  Acc:  0.93688333 Val_loss =  0.38299173 Val_acc =  0.8884\n",
            "Iteration  618 : Loss =  0.24784084  Acc:  0.93655 Val_loss =  0.38114244 Val_acc =  0.888\n",
            "Iteration  619 : Loss =  0.24678995  Acc:  0.9371 Val_loss =  0.3802909 Val_acc =  0.8894\n",
            "Iteration  620 : Loss =  0.24728708  Acc:  0.9370833 Val_loss =  0.3810634 Val_acc =  0.8888\n",
            "Iteration  621 : Loss =  0.24785838  Acc:  0.9368 Val_loss =  0.38074416 Val_acc =  0.8888\n",
            "Iteration  622 : Loss =  0.2470388  Acc:  0.93761665 Val_loss =  0.38152382 Val_acc =  0.8882\n",
            "Iteration  623 : Loss =  0.24551174  Acc:  0.9378167 Val_loss =  0.37909776 Val_acc =  0.8879\n",
            "Iteration  624 : Loss =  0.24486999  Acc:  0.9384 Val_loss =  0.3790602 Val_acc =  0.8898\n",
            "Iteration  625 : Loss =  0.24558747  Acc:  0.93813336 Val_loss =  0.3803042 Val_acc =  0.8881\n",
            "Iteration  626 : Loss =  0.24640179  Acc:  0.93768334 Val_loss =  0.38027287 Val_acc =  0.889\n",
            "Iteration  627 : Loss =  0.24612278  Acc:  0.93775 Val_loss =  0.38106766 Val_acc =  0.8875\n",
            "Iteration  628 : Loss =  0.24521899  Acc:  0.93808335 Val_loss =  0.37979093 Val_acc =  0.8897\n",
            "Iteration  629 : Loss =  0.2446726  Acc:  0.93815 Val_loss =  0.37945426 Val_acc =  0.8881\n",
            "Iteration  630 : Loss =  0.24483189  Acc:  0.93868333 Val_loss =  0.3800731 Val_acc =  0.8895\n",
            "Iteration  631 : Loss =  0.24521276  Acc:  0.93743336 Val_loss =  0.37985054 Val_acc =  0.8881\n",
            "Iteration  632 : Loss =  0.24507439  Acc:  0.93836665 Val_loss =  0.38058192 Val_acc =  0.8902\n",
            "Iteration  633 : Loss =  0.24494657  Acc:  0.9377667 Val_loss =  0.3797744 Val_acc =  0.8889\n",
            "Iteration  634 : Loss =  0.24532649  Acc:  0.9378833 Val_loss =  0.38113657 Val_acc =  0.8899\n",
            "Iteration  635 : Loss =  0.24701998  Acc:  0.9364 Val_loss =  0.3824484 Val_acc =  0.8885\n",
            "Iteration  636 : Loss =  0.24957108  Acc:  0.9352667 Val_loss =  0.38554212 Val_acc =  0.8884\n",
            "Iteration  637 : Loss =  0.25303462  Acc:  0.9329 Val_loss =  0.38888982 Val_acc =  0.8862\n",
            "Iteration  638 : Loss =  0.25553054  Acc:  0.93186665 Val_loss =  0.39218158 Val_acc =  0.8869\n",
            "Iteration  639 : Loss =  0.25760502  Acc:  0.93015 Val_loss =  0.39335787 Val_acc =  0.8831\n",
            "Iteration  640 : Loss =  0.25353166  Acc:  0.9333 Val_loss =  0.39068234 Val_acc =  0.8872\n",
            "Iteration  641 : Loss =  0.24860677  Acc:  0.93515 Val_loss =  0.38405383 Val_acc =  0.8873\n",
            "Iteration  642 : Loss =  0.24364926  Acc:  0.939 Val_loss =  0.38026178 Val_acc =  0.89\n",
            "Iteration  643 : Loss =  0.2443017  Acc:  0.93773335 Val_loss =  0.3808563 Val_acc =  0.889\n",
            "Iteration  644 : Loss =  0.24850339  Acc:  0.9349167 Val_loss =  0.38470295 Val_acc =  0.8878\n",
            "Iteration  645 : Loss =  0.2507401  Acc:  0.9346 Val_loss =  0.38884965 Val_acc =  0.8886\n",
            "Iteration  646 : Loss =  0.24903958  Acc:  0.9346833 Val_loss =  0.38571852 Val_acc =  0.8866\n",
            "Iteration  647 : Loss =  0.24426302  Acc:  0.93798333 Val_loss =  0.38211924 Val_acc =  0.8897\n",
            "Iteration  648 : Loss =  0.24150015  Acc:  0.93955 Val_loss =  0.3790512 Val_acc =  0.8895\n",
            "Iteration  649 : Loss =  0.24204585  Acc:  0.93905 Val_loss =  0.37908587 Val_acc =  0.8893\n",
            "Iteration  650 : Loss =  0.24410489  Acc:  0.9378667 Val_loss =  0.38223857 Val_acc =  0.8896\n",
            "Iteration  651 : Loss =  0.24545825  Acc:  0.9363833 Val_loss =  0.3828208 Val_acc =  0.8879\n",
            "Iteration  652 : Loss =  0.24496019  Acc:  0.9374667 Val_loss =  0.3834185 Val_acc =  0.8889\n",
            "Iteration  653 : Loss =  0.24328801  Acc:  0.9378 Val_loss =  0.38138413 Val_acc =  0.89\n",
            "Iteration  654 : Loss =  0.24171591  Acc:  0.9396667 Val_loss =  0.37985694 Val_acc =  0.8889\n",
            "Iteration  655 : Loss =  0.2407755  Acc:  0.94026667 Val_loss =  0.37940875 Val_acc =  0.8893\n",
            "Iteration  656 : Loss =  0.24075086  Acc:  0.9393167 Val_loss =  0.37892896 Val_acc =  0.8888\n",
            "Iteration  657 : Loss =  0.24130604  Acc:  0.93981665 Val_loss =  0.38029322 Val_acc =  0.8899\n",
            "Iteration  658 : Loss =  0.24213883  Acc:  0.9385167 Val_loss =  0.38108268 Val_acc =  0.8895\n",
            "Iteration  659 : Loss =  0.24229835  Acc:  0.9392667 Val_loss =  0.38132194 Val_acc =  0.8894\n",
            "Iteration  660 : Loss =  0.2416412  Acc:  0.9388667 Val_loss =  0.3809111 Val_acc =  0.8889\n",
            "Iteration  661 : Loss =  0.2402955  Acc:  0.9404167 Val_loss =  0.37934822 Val_acc =  0.8896\n",
            "Iteration  662 : Loss =  0.23918083  Acc:  0.9406667 Val_loss =  0.37840316 Val_acc =  0.8904\n",
            "Iteration  663 : Loss =  0.23887917  Acc:  0.9409 Val_loss =  0.3784467 Val_acc =  0.8904\n",
            "Iteration  664 : Loss =  0.23925501  Acc:  0.94065 Val_loss =  0.3789147 Val_acc =  0.8897\n",
            "Iteration  665 : Loss =  0.2397145  Acc:  0.94035 Val_loss =  0.37967464 Val_acc =  0.8898\n",
            "Iteration  666 : Loss =  0.2399148  Acc:  0.94013333 Val_loss =  0.37984693 Val_acc =  0.8897\n",
            "Iteration  667 : Loss =  0.23979762  Acc:  0.93955 Val_loss =  0.3797217 Val_acc =  0.8903\n",
            "Iteration  668 : Loss =  0.2395738  Acc:  0.94025 Val_loss =  0.37990054 Val_acc =  0.8895\n",
            "Iteration  669 : Loss =  0.23930325  Acc:  0.93975 Val_loss =  0.37934086 Val_acc =  0.8896\n",
            "Iteration  670 : Loss =  0.23921382  Acc:  0.94026667 Val_loss =  0.3800931 Val_acc =  0.89\n",
            "Iteration  671 : Loss =  0.23918155  Acc:  0.9404333 Val_loss =  0.37952405 Val_acc =  0.8899\n",
            "Iteration  672 : Loss =  0.23990324  Acc:  0.9396167 Val_loss =  0.3809876 Val_acc =  0.8898\n",
            "Iteration  673 : Loss =  0.24163017  Acc:  0.93925 Val_loss =  0.3824541 Val_acc =  0.8897\n",
            "Iteration  674 : Loss =  0.24636266  Acc:  0.9360667 Val_loss =  0.38763106 Val_acc =  0.8876\n",
            "Iteration  675 : Loss =  0.25413945  Acc:  0.93295 Val_loss =  0.39606613 Val_acc =  0.8835\n",
            "Iteration  676 : Loss =  0.26574326  Acc:  0.92728335 Val_loss =  0.40751028 Val_acc =  0.8789\n",
            "Iteration  677 : Loss =  0.26872948  Acc:  0.9253167 Val_loss =  0.4108917 Val_acc =  0.8793\n",
            "Iteration  678 : Loss =  0.256562  Acc:  0.93158334 Val_loss =  0.39659747 Val_acc =  0.8838\n",
            "Iteration  679 : Loss =  0.23950723  Acc:  0.9400167 Val_loss =  0.3784131 Val_acc =  0.8899\n",
            "Iteration  680 : Loss =  0.24565521  Acc:  0.9375 Val_loss =  0.38606122 Val_acc =  0.8882\n",
            "Iteration  681 : Loss =  0.25425726  Acc:  0.93266666 Val_loss =  0.39439592 Val_acc =  0.8841\n",
            "Iteration  682 : Loss =  0.24385172  Acc:  0.93798333 Val_loss =  0.38469082 Val_acc =  0.8893\n",
            "Iteration  683 : Loss =  0.23888952  Acc:  0.9407167 Val_loss =  0.3810932 Val_acc =  0.89\n",
            "Iteration  684 : Loss =  0.24667066  Acc:  0.93598336 Val_loss =  0.38855326 Val_acc =  0.8858\n",
            "Iteration  685 : Loss =  0.24799415  Acc:  0.93705 Val_loss =  0.39129132 Val_acc =  0.887\n",
            "Iteration  686 : Loss =  0.2398223  Acc:  0.93948334 Val_loss =  0.3824982 Val_acc =  0.8894\n",
            "Iteration  687 : Loss =  0.23828354  Acc:  0.94023335 Val_loss =  0.38048252 Val_acc =  0.8886\n",
            "Iteration  688 : Loss =  0.24340771  Acc:  0.93871665 Val_loss =  0.3873127 Val_acc =  0.8884\n",
            "Iteration  689 : Loss =  0.24181855  Acc:  0.93838334 Val_loss =  0.3842124 Val_acc =  0.8882\n",
            "Iteration  690 : Loss =  0.23641187  Acc:  0.94156665 Val_loss =  0.3791073 Val_acc =  0.8911\n",
            "Iteration  691 : Loss =  0.23840527  Acc:  0.94091666 Val_loss =  0.38246387 Val_acc =  0.8897\n",
            "Iteration  692 : Loss =  0.24138871  Acc:  0.9381833 Val_loss =  0.38440794 Val_acc =  0.8877\n",
            "Iteration  693 : Loss =  0.23805016  Acc:  0.94095 Val_loss =  0.38173383 Val_acc =  0.8912\n",
            "Iteration  694 : Loss =  0.23507427  Acc:  0.94266665 Val_loss =  0.3787387 Val_acc =  0.8906\n",
            "Iteration  695 : Loss =  0.2375406  Acc:  0.94053334 Val_loss =  0.38097453 Val_acc =  0.889\n",
            "Iteration  696 : Loss =  0.23898113  Acc:  0.94045 Val_loss =  0.38335204 Val_acc =  0.8899\n",
            "Iteration  697 : Loss =  0.23621917  Acc:  0.9412 Val_loss =  0.37962744 Val_acc =  0.8894\n",
            "Iteration  698 : Loss =  0.23435047  Acc:  0.94295 Val_loss =  0.3782869 Val_acc =  0.8911\n",
            "Iteration  699 : Loss =  0.23594388  Acc:  0.9419 Val_loss =  0.3804074 Val_acc =  0.8909\n",
            "Iteration  700 : Loss =  0.23705485  Acc:  0.94086665 Val_loss =  0.3808161 Val_acc =  0.8888\n",
            "Iteration  701 : Loss =  0.23528665  Acc:  0.9422167 Val_loss =  0.38012922 Val_acc =  0.8916\n",
            "Iteration  702 : Loss =  0.23365146  Acc:  0.94346666 Val_loss =  0.3782049 Val_acc =  0.8908\n",
            "Iteration  703 : Loss =  0.23437783  Acc:  0.94243336 Val_loss =  0.37899545 Val_acc =  0.8896\n",
            "Iteration  704 : Loss =  0.2353747  Acc:  0.9421167 Val_loss =  0.38090664 Val_acc =  0.8913\n",
            "Iteration  705 : Loss =  0.23468533  Acc:  0.9418667 Val_loss =  0.37962186 Val_acc =  0.8895\n",
            "Iteration  706 : Loss =  0.23326458  Acc:  0.94311666 Val_loss =  0.37834322 Val_acc =  0.8906\n",
            "Iteration  707 : Loss =  0.23314388  Acc:  0.94296664 Val_loss =  0.37842387 Val_acc =  0.8904\n",
            "Iteration  708 : Loss =  0.23396888  Acc:  0.94261664 Val_loss =  0.37897515 Val_acc =  0.8894\n",
            "Iteration  709 : Loss =  0.23412667  Acc:  0.9424667 Val_loss =  0.3797778 Val_acc =  0.8913\n",
            "Iteration  710 : Loss =  0.2335499  Acc:  0.94238335 Val_loss =  0.37881353 Val_acc =  0.8898\n",
            "Iteration  711 : Loss =  0.2332021  Acc:  0.94285 Val_loss =  0.37926427 Val_acc =  0.8901\n",
            "Iteration  712 : Loss =  0.23410271  Acc:  0.9421167 Val_loss =  0.38010234 Val_acc =  0.8902\n",
            "Iteration  713 : Loss =  0.23648545  Acc:  0.94053334 Val_loss =  0.38266328 Val_acc =  0.8891\n",
            "Iteration  714 : Loss =  0.2388562  Acc:  0.93916667 Val_loss =  0.3851102 Val_acc =  0.8904\n",
            "Iteration  715 : Loss =  0.24434516  Acc:  0.93556666 Val_loss =  0.39139515 Val_acc =  0.8881\n",
            "Iteration  716 : Loss =  0.24742001  Acc:  0.93376666 Val_loss =  0.39370477 Val_acc =  0.885\n",
            "Iteration  717 : Loss =  0.2522186  Acc:  0.9318 Val_loss =  0.3997308 Val_acc =  0.8868\n",
            "Iteration  718 : Loss =  0.24704581  Acc:  0.9343333 Val_loss =  0.39300212 Val_acc =  0.884\n",
            "Iteration  719 : Loss =  0.2368288  Acc:  0.94085 Val_loss =  0.38357535 Val_acc =  0.8903\n",
            "Iteration  720 : Loss =  0.23195547  Acc:  0.94335 Val_loss =  0.37806475 Val_acc =  0.8908\n",
            "Iteration  721 : Loss =  0.23585835  Acc:  0.9406667 Val_loss =  0.38230705 Val_acc =  0.8891\n",
            "Iteration  722 : Loss =  0.24161026  Acc:  0.93765 Val_loss =  0.38962072 Val_acc =  0.8898\n",
            "Iteration  723 : Loss =  0.24066174  Acc:  0.9385167 Val_loss =  0.38774484 Val_acc =  0.8866\n",
            "Iteration  724 : Loss =  0.23551407  Acc:  0.94081664 Val_loss =  0.38332412 Val_acc =  0.8898\n",
            "Iteration  725 : Loss =  0.23147121  Acc:  0.94366664 Val_loss =  0.3790146 Val_acc =  0.8911\n",
            "Iteration  726 : Loss =  0.23214519  Acc:  0.94241667 Val_loss =  0.37933964 Val_acc =  0.8894\n",
            "Iteration  727 : Loss =  0.23486981  Acc:  0.9418333 Val_loss =  0.38301578 Val_acc =  0.89\n",
            "Iteration  728 : Loss =  0.23670846  Acc:  0.9400667 Val_loss =  0.38441753 Val_acc =  0.8881\n",
            "Iteration  729 : Loss =  0.23622449  Acc:  0.94016665 Val_loss =  0.3845382 Val_acc =  0.8902\n",
            "Iteration  730 : Loss =  0.23336665  Acc:  0.9417167 Val_loss =  0.3815477 Val_acc =  0.8916\n",
            "Iteration  731 : Loss =  0.23145044  Acc:  0.94353336 Val_loss =  0.379624 Val_acc =  0.8911\n",
            "Iteration  732 : Loss =  0.23058456  Acc:  0.94376665 Val_loss =  0.37910804 Val_acc =  0.8917\n",
            "Iteration  733 : Loss =  0.23101616  Acc:  0.9432 Val_loss =  0.37946683 Val_acc =  0.8892\n",
            "Iteration  734 : Loss =  0.23215917  Acc:  0.9429167 Val_loss =  0.38116688 Val_acc =  0.89\n",
            "Iteration  735 : Loss =  0.23287056  Acc:  0.9425167 Val_loss =  0.38192293 Val_acc =  0.8909\n",
            "Iteration  736 : Loss =  0.2331657  Acc:  0.9421667 Val_loss =  0.3822813 Val_acc =  0.889\n",
            "Iteration  737 : Loss =  0.231668  Acc:  0.94311666 Val_loss =  0.38083085 Val_acc =  0.8911\n",
            "Iteration  738 : Loss =  0.23028117  Acc:  0.94408333 Val_loss =  0.37956938 Val_acc =  0.8907\n",
            "Iteration  739 : Loss =  0.2292498  Acc:  0.9447333 Val_loss =  0.37814108 Val_acc =  0.8917\n",
            "Iteration  740 : Loss =  0.22916295  Acc:  0.94451666 Val_loss =  0.37879068 Val_acc =  0.8916\n",
            "Iteration  741 : Loss =  0.22954172  Acc:  0.9442 Val_loss =  0.3789478 Val_acc =  0.8902\n",
            "Iteration  742 : Loss =  0.2297827  Acc:  0.9439833 Val_loss =  0.3796863 Val_acc =  0.8911\n",
            "Iteration  743 : Loss =  0.2298402  Acc:  0.9443667 Val_loss =  0.37995178 Val_acc =  0.891\n",
            "Iteration  744 : Loss =  0.22962582  Acc:  0.9435833 Val_loss =  0.3797571 Val_acc =  0.891\n",
            "Iteration  745 : Loss =  0.22967191  Acc:  0.94425 Val_loss =  0.38025004 Val_acc =  0.8909\n",
            "Iteration  746 : Loss =  0.22939703  Acc:  0.94425 Val_loss =  0.37936985 Val_acc =  0.8901\n",
            "Iteration  747 : Loss =  0.22901666  Acc:  0.94465 Val_loss =  0.3799745 Val_acc =  0.8909\n",
            "Iteration  748 : Loss =  0.22835109  Acc:  0.9450167 Val_loss =  0.37858582 Val_acc =  0.8905\n",
            "Iteration  749 : Loss =  0.22771981  Acc:  0.94525 Val_loss =  0.37861222 Val_acc =  0.8915\n",
            "Iteration  750 : Loss =  0.22734937  Acc:  0.94556665 Val_loss =  0.37828156 Val_acc =  0.8906\n",
            "Iteration  751 : Loss =  0.22727899  Acc:  0.94556665 Val_loss =  0.37814894 Val_acc =  0.8914\n",
            "Iteration  752 : Loss =  0.2273784  Acc:  0.9454 Val_loss =  0.37863138 Val_acc =  0.8911\n",
            "Iteration  753 : Loss =  0.22750887  Acc:  0.94535 Val_loss =  0.37859184 Val_acc =  0.89\n",
            "Iteration  754 : Loss =  0.22756594  Acc:  0.94525 Val_loss =  0.3792938 Val_acc =  0.8915\n",
            "Iteration  755 : Loss =  0.2276052  Acc:  0.9449833 Val_loss =  0.37888226 Val_acc =  0.8896\n",
            "Iteration  756 : Loss =  0.22779332  Acc:  0.94516665 Val_loss =  0.37979317 Val_acc =  0.8911\n",
            "Iteration  757 : Loss =  0.22796927  Acc:  0.9443833 Val_loss =  0.37963653 Val_acc =  0.8908\n",
            "Iteration  758 : Loss =  0.22866032  Acc:  0.94453335 Val_loss =  0.3807839 Val_acc =  0.8903\n",
            "Iteration  759 : Loss =  0.22899982  Acc:  0.94395 Val_loss =  0.38105857 Val_acc =  0.8919\n",
            "Iteration  760 : Loss =  0.22997278  Acc:  0.9436333 Val_loss =  0.3825043 Val_acc =  0.89\n",
            "Iteration  761 : Loss =  0.23003212  Acc:  0.94345 Val_loss =  0.38235208 Val_acc =  0.8915\n",
            "Iteration  762 : Loss =  0.230815  Acc:  0.94266665 Val_loss =  0.38367602 Val_acc =  0.8904\n",
            "Iteration  763 : Loss =  0.23003614  Acc:  0.94336665 Val_loss =  0.38247186 Val_acc =  0.8911\n",
            "Iteration  764 : Loss =  0.22995928  Acc:  0.9433 Val_loss =  0.3830024 Val_acc =  0.8914\n",
            "Iteration  765 : Loss =  0.22871013  Acc:  0.94378334 Val_loss =  0.3812589 Val_acc =  0.8905\n",
            "Iteration  766 : Loss =  0.227857  Acc:  0.94451666 Val_loss =  0.38125506 Val_acc =  0.8908\n",
            "Iteration  767 : Loss =  0.22692113  Acc:  0.94458336 Val_loss =  0.3798494 Val_acc =  0.8902\n",
            "Iteration  768 : Loss =  0.2261081  Acc:  0.94593334 Val_loss =  0.37972313 Val_acc =  0.8928\n",
            "Iteration  769 : Loss =  0.22561759  Acc:  0.9457333 Val_loss =  0.3790231 Val_acc =  0.891\n",
            "Iteration  770 : Loss =  0.22535273  Acc:  0.94591665 Val_loss =  0.37908518 Val_acc =  0.8914\n",
            "Iteration  771 : Loss =  0.22544149  Acc:  0.94586664 Val_loss =  0.37946883 Val_acc =  0.8913\n",
            "Iteration  772 : Loss =  0.22581989  Acc:  0.9453 Val_loss =  0.37967637 Val_acc =  0.8908\n",
            "Iteration  773 : Loss =  0.22668487  Acc:  0.94555 Val_loss =  0.3812479 Val_acc =  0.8903\n",
            "Iteration  774 : Loss =  0.22747514  Acc:  0.94453335 Val_loss =  0.38142437 Val_acc =  0.8905\n",
            "Iteration  775 : Loss =  0.22914182  Acc:  0.94376665 Val_loss =  0.38413638 Val_acc =  0.8898\n",
            "Iteration  776 : Loss =  0.22960556  Acc:  0.9429333 Val_loss =  0.38369507 Val_acc =  0.8897\n",
            "Iteration  777 : Loss =  0.23138437  Acc:  0.94198334 Val_loss =  0.3867051 Val_acc =  0.8887\n",
            "Iteration  778 : Loss =  0.23005092  Acc:  0.94276667 Val_loss =  0.38439053 Val_acc =  0.8902\n",
            "Iteration  779 : Loss =  0.23006363  Acc:  0.94268334 Val_loss =  0.38533306 Val_acc =  0.8901\n",
            "Iteration  780 : Loss =  0.22775102  Acc:  0.94416666 Val_loss =  0.38241124 Val_acc =  0.8914\n",
            "Iteration  781 : Loss =  0.22702377  Acc:  0.9447333 Val_loss =  0.38213515 Val_acc =  0.8901\n",
            "Iteration  782 : Loss =  0.2270239  Acc:  0.9450167 Val_loss =  0.38256228 Val_acc =  0.8902\n",
            "Iteration  783 : Loss =  0.22805054  Acc:  0.9439167 Val_loss =  0.3833146 Val_acc =  0.8894\n",
            "Iteration  784 : Loss =  0.22949007  Acc:  0.9429 Val_loss =  0.38596728 Val_acc =  0.8891\n",
            "Iteration  785 : Loss =  0.23010075  Acc:  0.94325 Val_loss =  0.38575673 Val_acc =  0.8902\n",
            "Iteration  786 : Loss =  0.22993393  Acc:  0.94243336 Val_loss =  0.38651907 Val_acc =  0.8883\n",
            "Iteration  787 : Loss =  0.22890773  Acc:  0.94441664 Val_loss =  0.38520873 Val_acc =  0.8901\n",
            "Iteration  788 : Loss =  0.22860362  Acc:  0.94343334 Val_loss =  0.38448706 Val_acc =  0.8883\n",
            "Iteration  789 : Loss =  0.2285639  Acc:  0.94408333 Val_loss =  0.3853426 Val_acc =  0.8918\n",
            "Iteration  790 : Loss =  0.22789419  Acc:  0.94388336 Val_loss =  0.38321555 Val_acc =  0.8888\n",
            "Iteration  791 : Loss =  0.22551161  Acc:  0.9458 Val_loss =  0.38221422 Val_acc =  0.8913\n",
            "Iteration  792 : Loss =  0.22354135  Acc:  0.94668335 Val_loss =  0.37949434 Val_acc =  0.8907\n",
            "Iteration  793 : Loss =  0.2234122  Acc:  0.9461167 Val_loss =  0.3800764 Val_acc =  0.8912\n",
            "Iteration  794 : Loss =  0.22508621  Acc:  0.9461333 Val_loss =  0.38231072 Val_acc =  0.8919\n",
            "Iteration  795 : Loss =  0.22722769  Acc:  0.94416666 Val_loss =  0.38389534 Val_acc =  0.8878\n",
            "Iteration  796 : Loss =  0.22778559  Acc:  0.94515 Val_loss =  0.38567615 Val_acc =  0.8914\n",
            "Iteration  797 : Loss =  0.22688273  Acc:  0.9443333 Val_loss =  0.38349172 Val_acc =  0.8884\n",
            "Iteration  798 : Loss =  0.22415331  Acc:  0.9468 Val_loss =  0.38166505 Val_acc =  0.8928\n",
            "Iteration  799 : Loss =  0.22269079  Acc:  0.9467 Val_loss =  0.37962368 Val_acc =  0.8902\n",
            "Iteration  800 : Loss =  0.22360256  Acc:  0.9461333 Val_loss =  0.38093883 Val_acc =  0.8906\n",
            "Iteration  801 : Loss =  0.22601908  Acc:  0.945 Val_loss =  0.3840224 Val_acc =  0.8916\n",
            "Iteration  802 : Loss =  0.22883567  Acc:  0.9424 Val_loss =  0.38704047 Val_acc =  0.8887\n",
            "Iteration  803 : Loss =  0.2294504  Acc:  0.9432 Val_loss =  0.38811067 Val_acc =  0.8908\n",
            "Iteration  804 : Loss =  0.23105094  Acc:  0.9411833 Val_loss =  0.38987973 Val_acc =  0.8872\n",
            "Iteration  805 : Loss =  0.2291587  Acc:  0.94263333 Val_loss =  0.38722843 Val_acc =  0.8886\n",
            "Iteration  806 : Loss =  0.23074007  Acc:  0.94116664 Val_loss =  0.3898384 Val_acc =  0.8899\n",
            "Iteration  807 : Loss =  0.2298388  Acc:  0.94205 Val_loss =  0.38739944 Val_acc =  0.8872\n",
            "Iteration  808 : Loss =  0.22726628  Acc:  0.9446 Val_loss =  0.3865976 Val_acc =  0.8913\n",
            "Iteration  809 : Loss =  0.22440857  Acc:  0.9450167 Val_loss =  0.38265747 Val_acc =  0.888\n",
            "Iteration  810 : Loss =  0.22229333  Acc:  0.9471833 Val_loss =  0.38111913 Val_acc =  0.8911\n",
            "Iteration  811 : Loss =  0.22339034  Acc:  0.9461833 Val_loss =  0.38287565 Val_acc =  0.8903\n",
            "Iteration  812 : Loss =  0.2252676  Acc:  0.9443167 Val_loss =  0.38377398 Val_acc =  0.8895\n",
            "Iteration  813 : Loss =  0.22603182  Acc:  0.9447333 Val_loss =  0.38610062 Val_acc =  0.8912\n",
            "Iteration  814 : Loss =  0.22442164  Acc:  0.9450667 Val_loss =  0.38351026 Val_acc =  0.8897\n",
            "Iteration  815 : Loss =  0.22355811  Acc:  0.946 Val_loss =  0.3834315 Val_acc =  0.8911\n",
            "Iteration  816 : Loss =  0.22433323  Acc:  0.94526666 Val_loss =  0.38428217 Val_acc =  0.8894\n",
            "Iteration  817 : Loss =  0.22518927  Acc:  0.9446167 Val_loss =  0.38497236 Val_acc =  0.8888\n",
            "Iteration  818 : Loss =  0.22445609  Acc:  0.94535 Val_loss =  0.3849722 Val_acc =  0.8899\n",
            "Iteration  819 : Loss =  0.22180277  Acc:  0.94706666 Val_loss =  0.38187692 Val_acc =  0.891\n",
            "Iteration  820 : Loss =  0.21957341  Acc:  0.9477 Val_loss =  0.37996563 Val_acc =  0.8906\n",
            "Iteration  821 : Loss =  0.2194252  Acc:  0.94805 Val_loss =  0.38006747 Val_acc =  0.8919\n",
            "Iteration  822 : Loss =  0.22051385  Acc:  0.94773334 Val_loss =  0.38052577 Val_acc =  0.8898\n",
            "Iteration  823 : Loss =  0.22084568  Acc:  0.9475667 Val_loss =  0.38170666 Val_acc =  0.8912\n",
            "Iteration  824 : Loss =  0.22024673  Acc:  0.94768333 Val_loss =  0.3804795 Val_acc =  0.8907\n",
            "Iteration  825 : Loss =  0.21987107  Acc:  0.9475333 Val_loss =  0.38020223 Val_acc =  0.8913\n",
            "Iteration  826 : Loss =  0.22062147  Acc:  0.9477 Val_loss =  0.3817041 Val_acc =  0.8915\n",
            "Iteration  827 : Loss =  0.2221486  Acc:  0.9458167 Val_loss =  0.38257128 Val_acc =  0.8891\n",
            "Iteration  828 : Loss =  0.2230025  Acc:  0.94633335 Val_loss =  0.3845918 Val_acc =  0.8921\n",
            "Iteration  829 : Loss =  0.2228468  Acc:  0.94551665 Val_loss =  0.38367593 Val_acc =  0.8888\n",
            "Iteration  830 : Loss =  0.22215798  Acc:  0.94633335 Val_loss =  0.38405988 Val_acc =  0.8921\n",
            "Iteration  831 : Loss =  0.22132006  Acc:  0.9469 Val_loss =  0.38265416 Val_acc =  0.8907\n",
            "Iteration  832 : Loss =  0.22129059  Acc:  0.94665 Val_loss =  0.3829303 Val_acc =  0.8894\n",
            "Iteration  833 : Loss =  0.2206811  Acc:  0.94733334 Val_loss =  0.38284206 Val_acc =  0.8921\n",
            "Iteration  834 : Loss =  0.21973298  Acc:  0.9478 Val_loss =  0.38159287 Val_acc =  0.8894\n",
            "Iteration  835 : Loss =  0.21863645  Acc:  0.9482 Val_loss =  0.38091803 Val_acc =  0.8919\n",
            "Iteration  836 : Loss =  0.21847163  Acc:  0.94818336 Val_loss =  0.3809688 Val_acc =  0.8914\n",
            "Iteration  837 : Loss =  0.21909556  Acc:  0.9486333 Val_loss =  0.38137192 Val_acc =  0.8914\n",
            "Iteration  838 : Loss =  0.22001737  Acc:  0.94673336 Val_loss =  0.38267988 Val_acc =  0.8905\n",
            "Iteration  839 : Loss =  0.22045262  Acc:  0.9482333 Val_loss =  0.38297945 Val_acc =  0.8917\n",
            "Iteration  840 : Loss =  0.22055154  Acc:  0.94671667 Val_loss =  0.3832937 Val_acc =  0.8891\n",
            "Iteration  841 : Loss =  0.22025174  Acc:  0.9483333 Val_loss =  0.3832045 Val_acc =  0.8918\n",
            "Iteration  842 : Loss =  0.2201432  Acc:  0.94706666 Val_loss =  0.3829942 Val_acc =  0.8896\n",
            "Iteration  843 : Loss =  0.219568  Acc:  0.94843334 Val_loss =  0.38311392 Val_acc =  0.8919\n",
            "Iteration  844 : Loss =  0.21880898  Acc:  0.9475833 Val_loss =  0.3817612 Val_acc =  0.889\n",
            "Iteration  845 : Loss =  0.21739803  Acc:  0.9493 Val_loss =  0.38114125 Val_acc =  0.8926\n",
            "Iteration  846 : Loss =  0.2163985  Acc:  0.94928336 Val_loss =  0.3798511 Val_acc =  0.8898\n",
            "Iteration  847 : Loss =  0.21617258  Acc:  0.9497833 Val_loss =  0.37994617 Val_acc =  0.8916\n",
            "Iteration  848 : Loss =  0.21677536  Acc:  0.94916666 Val_loss =  0.38079798 Val_acc =  0.8923\n",
            "Iteration  849 : Loss =  0.21804723  Acc:  0.9482333 Val_loss =  0.38207364 Val_acc =  0.8908\n",
            "Iteration  850 : Loss =  0.21967429  Acc:  0.94811666 Val_loss =  0.38410348 Val_acc =  0.8919\n",
            "Iteration  851 : Loss =  0.2220994  Acc:  0.94588333 Val_loss =  0.3865606 Val_acc =  0.8886\n",
            "Iteration  852 : Loss =  0.22440806  Acc:  0.9454 Val_loss =  0.38898197 Val_acc =  0.8901\n",
            "Iteration  853 : Loss =  0.22899315  Acc:  0.94121665 Val_loss =  0.39388695 Val_acc =  0.887\n",
            "Iteration  854 : Loss =  0.2279926  Acc:  0.9428 Val_loss =  0.39237717 Val_acc =  0.8881\n",
            "Iteration  855 : Loss =  0.23056386  Acc:  0.94056666 Val_loss =  0.39566088 Val_acc =  0.8871\n",
            "Iteration  856 : Loss =  0.2240758  Acc:  0.94443333 Val_loss =  0.38796443 Val_acc =  0.8888\n",
            "Iteration  857 : Loss =  0.22168297  Acc:  0.9465333 Val_loss =  0.38675293 Val_acc =  0.89\n",
            "Iteration  858 : Loss =  0.2223698  Acc:  0.9458 Val_loss =  0.38710862 Val_acc =  0.888\n",
            "Iteration  859 : Loss =  0.22414456  Acc:  0.9461 Val_loss =  0.3895086 Val_acc =  0.8902\n",
            "Iteration  860 : Loss =  0.22488157  Acc:  0.94418335 Val_loss =  0.39089853 Val_acc =  0.8874\n",
            "Iteration  861 : Loss =  0.22200224  Acc:  0.94663334 Val_loss =  0.38747337 Val_acc =  0.8907\n",
            "Iteration  862 : Loss =  0.21891436  Acc:  0.9475167 Val_loss =  0.384864 Val_acc =  0.8909\n",
            "Iteration  863 : Loss =  0.2173098  Acc:  0.9482333 Val_loss =  0.38275284 Val_acc =  0.891\n",
            "Iteration  864 : Loss =  0.21996339  Acc:  0.94773334 Val_loss =  0.38608605 Val_acc =  0.8897\n",
            "Iteration  865 : Loss =  0.22425804  Acc:  0.9445 Val_loss =  0.39058578 Val_acc =  0.8875\n",
            "Iteration  866 : Loss =  0.22472371  Acc:  0.9454333 Val_loss =  0.39142853 Val_acc =  0.8892\n",
            "Iteration  867 : Loss =  0.22227846  Acc:  0.9457667 Val_loss =  0.38871402 Val_acc =  0.8874\n",
            "Iteration  868 : Loss =  0.21743543  Acc:  0.9487 Val_loss =  0.3840654 Val_acc =  0.8906\n",
            "Iteration  869 : Loss =  0.2157656  Acc:  0.9486833 Val_loss =  0.38181522 Val_acc =  0.8912\n",
            "Iteration  870 : Loss =  0.21749131  Acc:  0.9477 Val_loss =  0.38438582 Val_acc =  0.8907\n",
            "Iteration  871 : Loss =  0.21863452  Acc:  0.9486 Val_loss =  0.38525707 Val_acc =  0.891\n",
            "Iteration  872 : Loss =  0.21769834  Acc:  0.9477 Val_loss =  0.38466787 Val_acc =  0.8885\n",
            "Iteration  873 : Loss =  0.21560009  Acc:  0.94985 Val_loss =  0.38269946 Val_acc =  0.8926\n",
            "Iteration  874 : Loss =  0.21451578  Acc:  0.94955 Val_loss =  0.38109377 Val_acc =  0.8905\n",
            "Iteration  875 : Loss =  0.21488759  Acc:  0.9495 Val_loss =  0.38227838 Val_acc =  0.8901\n",
            "Iteration  876 : Loss =  0.2155582  Acc:  0.94928336 Val_loss =  0.38244176 Val_acc =  0.892\n",
            "Iteration  877 : Loss =  0.21596226  Acc:  0.94893336 Val_loss =  0.38350797 Val_acc =  0.8911\n",
            "Iteration  878 : Loss =  0.21531877  Acc:  0.94956666 Val_loss =  0.38304865 Val_acc =  0.8925\n",
            "Iteration  879 : Loss =  0.21511331  Acc:  0.9494333 Val_loss =  0.3827898 Val_acc =  0.891\n",
            "Iteration  880 : Loss =  0.21523549  Acc:  0.9491 Val_loss =  0.3832331 Val_acc =  0.8915\n",
            "Iteration  881 : Loss =  0.21534349  Acc:  0.9493667 Val_loss =  0.38320088 Val_acc =  0.8902\n",
            "Iteration  882 : Loss =  0.21497452  Acc:  0.9493833 Val_loss =  0.3832608 Val_acc =  0.8911\n",
            "Iteration  883 : Loss =  0.21453688  Acc:  0.95025 Val_loss =  0.38292545 Val_acc =  0.8917\n",
            "Iteration  884 : Loss =  0.21480416  Acc:  0.9493 Val_loss =  0.38296098 Val_acc =  0.8903\n",
            "Iteration  885 : Loss =  0.21537805  Acc:  0.94945 Val_loss =  0.38445705 Val_acc =  0.8933\n",
            "Iteration  886 : Loss =  0.21578574  Acc:  0.94845 Val_loss =  0.38389686 Val_acc =  0.8894\n",
            "Iteration  887 : Loss =  0.21538076  Acc:  0.9494333 Val_loss =  0.38432238 Val_acc =  0.8926\n",
            "Iteration  888 : Loss =  0.21439874  Acc:  0.9494333 Val_loss =  0.38271293 Val_acc =  0.8913\n",
            "Iteration  889 : Loss =  0.21448322  Acc:  0.94958335 Val_loss =  0.3832594 Val_acc =  0.89\n",
            "Iteration  890 : Loss =  0.21499595  Acc:  0.94925 Val_loss =  0.38392925 Val_acc =  0.8914\n",
            "Iteration  891 : Loss =  0.21620584  Acc:  0.94815 Val_loss =  0.38534397 Val_acc =  0.8894\n",
            "Iteration  892 : Loss =  0.21631156  Acc:  0.94848335 Val_loss =  0.3859526 Val_acc =  0.891\n",
            "Iteration  893 : Loss =  0.21578759  Acc:  0.94838333 Val_loss =  0.38533664 Val_acc =  0.8892\n",
            "Iteration  894 : Loss =  0.21428479  Acc:  0.94941664 Val_loss =  0.3839376 Val_acc =  0.8916\n",
            "Iteration  895 : Loss =  0.2137332  Acc:  0.95 Val_loss =  0.3835237 Val_acc =  0.8905\n",
            "Iteration  896 : Loss =  0.2132715  Acc:  0.94996667 Val_loss =  0.38256988 Val_acc =  0.8907\n",
            "Iteration  897 : Loss =  0.21346188  Acc:  0.95023334 Val_loss =  0.38360605 Val_acc =  0.892\n",
            "Iteration  898 : Loss =  0.21312708  Acc:  0.9500333 Val_loss =  0.38267747 Val_acc =  0.8908\n",
            "Iteration  899 : Loss =  0.21266854  Acc:  0.95075 Val_loss =  0.38311842 Val_acc =  0.8924\n",
            "Iteration  900 : Loss =  0.21255031  Acc:  0.9502 Val_loss =  0.38255924 Val_acc =  0.8901\n",
            "Iteration  901 : Loss =  0.21262787  Acc:  0.95143336 Val_loss =  0.38316083 Val_acc =  0.8922\n",
            "Iteration  902 : Loss =  0.21295688  Acc:  0.94986665 Val_loss =  0.38346162 Val_acc =  0.8903\n",
            "Iteration  903 : Loss =  0.2127643  Acc:  0.95098335 Val_loss =  0.38352495 Val_acc =  0.8924\n",
            "Iteration  904 : Loss =  0.21234803  Acc:  0.9505 Val_loss =  0.38334656 Val_acc =  0.8915\n",
            "Iteration  905 : Loss =  0.21188807  Acc:  0.9511 Val_loss =  0.38317978 Val_acc =  0.891\n",
            "Iteration  906 : Loss =  0.211873  Acc:  0.9503833 Val_loss =  0.38309723 Val_acc =  0.891\n",
            "Iteration  907 : Loss =  0.21295486  Acc:  0.94991666 Val_loss =  0.38462725 Val_acc =  0.8911\n",
            "Iteration  908 : Loss =  0.21391328  Acc:  0.9494333 Val_loss =  0.38509372 Val_acc =  0.8912\n",
            "Iteration  909 : Loss =  0.21651077  Acc:  0.9472 Val_loss =  0.38854152 Val_acc =  0.8898\n",
            "Iteration  910 : Loss =  0.21684408  Acc:  0.9479333 Val_loss =  0.38827664 Val_acc =  0.8897\n",
            "Iteration  911 : Loss =  0.21958005  Acc:  0.9453667 Val_loss =  0.3918034 Val_acc =  0.8875\n",
            "Iteration  912 : Loss =  0.2176728  Acc:  0.9468333 Val_loss =  0.38933718 Val_acc =  0.8896\n",
            "Iteration  913 : Loss =  0.21725681  Acc:  0.94673336 Val_loss =  0.38922775 Val_acc =  0.8881\n",
            "Iteration  914 : Loss =  0.21387577  Acc:  0.94911665 Val_loss =  0.3856799 Val_acc =  0.8909\n",
            "Iteration  915 : Loss =  0.21115726  Acc:  0.95126665 Val_loss =  0.38294688 Val_acc =  0.8907\n",
            "Iteration  916 : Loss =  0.20944706  Acc:  0.95155 Val_loss =  0.3817257 Val_acc =  0.8908\n",
            "Iteration  917 : Loss =  0.2094848  Acc:  0.95203334 Val_loss =  0.3818606 Val_acc =  0.8906\n",
            "Iteration  918 : Loss =  0.21116987  Acc:  0.9508833 Val_loss =  0.38389724 Val_acc =  0.891\n",
            "Iteration  919 : Loss =  0.21325459  Acc:  0.9500833 Val_loss =  0.38631734 Val_acc =  0.8924\n",
            "Iteration  920 : Loss =  0.21575598  Acc:  0.94768333 Val_loss =  0.38872784 Val_acc =  0.8882\n",
            "Iteration  921 : Loss =  0.21593243  Acc:  0.94825 Val_loss =  0.3891573 Val_acc =  0.8918\n",
            "Iteration  922 : Loss =  0.21607652  Acc:  0.94705 Val_loss =  0.38931993 Val_acc =  0.8883\n",
            "Iteration  923 : Loss =  0.21304177  Acc:  0.9496833 Val_loss =  0.3859784 Val_acc =  0.8916\n",
            "Iteration  924 : Loss =  0.21137503  Acc:  0.9509 Val_loss =  0.38473198 Val_acc =  0.8911\n",
            "Iteration  925 : Loss =  0.20994765  Acc:  0.95131665 Val_loss =  0.3827443 Val_acc =  0.8909\n",
            "Iteration  926 : Loss =  0.20989147  Acc:  0.95173335 Val_loss =  0.3837608 Val_acc =  0.8926\n",
            "Iteration  927 : Loss =  0.21092658  Acc:  0.95095 Val_loss =  0.38428557 Val_acc =  0.8896\n",
            "Iteration  928 : Loss =  0.21236098  Acc:  0.9508333 Val_loss =  0.38651276 Val_acc =  0.8916\n",
            "Iteration  929 : Loss =  0.21401195  Acc:  0.94948334 Val_loss =  0.38805276 Val_acc =  0.8888\n",
            "Iteration  930 : Loss =  0.2151109  Acc:  0.94948334 Val_loss =  0.38921517 Val_acc =  0.8893\n",
            "Iteration  931 : Loss =  0.21481708  Acc:  0.9486167 Val_loss =  0.38894337 Val_acc =  0.8883\n",
            "Iteration  932 : Loss =  0.21250829  Acc:  0.95096666 Val_loss =  0.38646102 Val_acc =  0.8913\n",
            "Iteration  933 : Loss =  0.20930502  Acc:  0.95145 Val_loss =  0.38328052 Val_acc =  0.8902\n",
            "Iteration  934 : Loss =  0.2074084  Acc:  0.95283335 Val_loss =  0.38159963 Val_acc =  0.8915\n",
            "Iteration  935 : Loss =  0.20805873  Acc:  0.9522833 Val_loss =  0.3826321 Val_acc =  0.8916\n",
            "Iteration  936 : Loss =  0.21033576  Acc:  0.95103335 Val_loss =  0.38567862 Val_acc =  0.8902\n",
            "Iteration  937 : Loss =  0.21170604  Acc:  0.9507667 Val_loss =  0.38692933 Val_acc =  0.8915\n",
            "Iteration  938 : Loss =  0.21266177  Acc:  0.94986665 Val_loss =  0.38847235 Val_acc =  0.8895\n",
            "Iteration  939 : Loss =  0.21106696  Acc:  0.9508833 Val_loss =  0.38621196 Val_acc =  0.8925\n",
            "Iteration  940 : Loss =  0.21014342  Acc:  0.9507833 Val_loss =  0.38558623 Val_acc =  0.8904\n",
            "Iteration  941 : Loss =  0.20910549  Acc:  0.9515667 Val_loss =  0.38420612 Val_acc =  0.8917\n",
            "Iteration  942 : Loss =  0.20982283  Acc:  0.95206666 Val_loss =  0.38534307 Val_acc =  0.8917\n",
            "Iteration  943 : Loss =  0.21181308  Acc:  0.94958335 Val_loss =  0.38741958 Val_acc =  0.8889\n",
            "Iteration  944 : Loss =  0.21373744  Acc:  0.94995 Val_loss =  0.39009327 Val_acc =  0.8901\n",
            "Iteration  945 : Loss =  0.21652281  Acc:  0.9475833 Val_loss =  0.39248702 Val_acc =  0.8875\n",
            "Iteration  946 : Loss =  0.21660209  Acc:  0.9481 Val_loss =  0.39338633 Val_acc =  0.8896\n",
            "Iteration  947 : Loss =  0.21726505  Acc:  0.9472833 Val_loss =  0.39283678 Val_acc =  0.8858\n",
            "Iteration  948 : Loss =  0.21336272  Acc:  0.94958335 Val_loss =  0.38965958 Val_acc =  0.892\n",
            "Iteration  949 : Loss =  0.21024606  Acc:  0.95065 Val_loss =  0.3854954 Val_acc =  0.8911\n",
            "Iteration  950 : Loss =  0.20988297  Acc:  0.95101666 Val_loss =  0.38621807 Val_acc =  0.8905\n",
            "Iteration  951 : Loss =  0.21042573  Acc:  0.95168334 Val_loss =  0.3870225 Val_acc =  0.8914\n",
            "Iteration  952 : Loss =  0.21088152  Acc:  0.9507167 Val_loss =  0.3875051 Val_acc =  0.8883\n",
            "Iteration  953 : Loss =  0.20998344  Acc:  0.95185 Val_loss =  0.38749242 Val_acc =  0.8917\n",
            "Iteration  954 : Loss =  0.2088855  Acc:  0.9515167 Val_loss =  0.3854828 Val_acc =  0.8896\n",
            "Iteration  955 : Loss =  0.2079979  Acc:  0.9521 Val_loss =  0.3853816 Val_acc =  0.892\n",
            "Iteration  956 : Loss =  0.20846146  Acc:  0.9515167 Val_loss =  0.38525623 Val_acc =  0.8921\n",
            "Iteration  957 : Loss =  0.21092495  Acc:  0.94945 Val_loss =  0.38843405 Val_acc =  0.889\n",
            "Iteration  958 : Loss =  0.21216643  Acc:  0.9497167 Val_loss =  0.389615 Val_acc =  0.8906\n",
            "Iteration  959 : Loss =  0.21444073  Acc:  0.9475833 Val_loss =  0.39227992 Val_acc =  0.8874\n",
            "Iteration  960 : Loss =  0.21281822  Acc:  0.94913334 Val_loss =  0.3907706 Val_acc =  0.8907\n",
            "Iteration  961 : Loss =  0.21069157  Acc:  0.95005 Val_loss =  0.3884826 Val_acc =  0.8894\n",
            "Iteration  962 : Loss =  0.20742573  Acc:  0.95225 Val_loss =  0.3852482 Val_acc =  0.8916\n",
            "Iteration  963 : Loss =  0.20581871  Acc:  0.9539 Val_loss =  0.3838097 Val_acc =  0.8928\n",
            "Iteration  964 : Loss =  0.20639664  Acc:  0.95285 Val_loss =  0.38429844 Val_acc =  0.8906\n",
            "Iteration  965 : Loss =  0.20756727  Acc:  0.9526 Val_loss =  0.38615888 Val_acc =  0.8921\n",
            "Iteration  966 : Loss =  0.20796831  Acc:  0.95208335 Val_loss =  0.38612664 Val_acc =  0.891\n",
            "Iteration  967 : Loss =  0.20724502  Acc:  0.95233333 Val_loss =  0.3856649 Val_acc =  0.891\n",
            "Iteration  968 : Loss =  0.20666163  Acc:  0.95275 Val_loss =  0.3850837 Val_acc =  0.8905\n",
            "Iteration  969 : Loss =  0.20686889  Acc:  0.9522167 Val_loss =  0.38512602 Val_acc =  0.8913\n",
            "Iteration  970 : Loss =  0.2083236  Acc:  0.95245 Val_loss =  0.3874182 Val_acc =  0.8911\n",
            "Iteration  971 : Loss =  0.21057719  Acc:  0.9501333 Val_loss =  0.38912055 Val_acc =  0.8887\n",
            "Iteration  972 : Loss =  0.2105867  Acc:  0.95126665 Val_loss =  0.39041382 Val_acc =  0.891\n",
            "Iteration  973 : Loss =  0.20985436  Acc:  0.95056665 Val_loss =  0.3886483 Val_acc =  0.8891\n",
            "Iteration  974 : Loss =  0.20666322  Acc:  0.9533333 Val_loss =  0.38613957 Val_acc =  0.8914\n",
            "Iteration  975 : Loss =  0.2045452  Acc:  0.9537167 Val_loss =  0.3834357 Val_acc =  0.8916\n",
            "Iteration  976 : Loss =  0.20422032  Acc:  0.9540333 Val_loss =  0.38306108 Val_acc =  0.8909\n",
            "Iteration  977 : Loss =  0.20502466  Acc:  0.9539667 Val_loss =  0.38477683 Val_acc =  0.8929\n",
            "Iteration  978 : Loss =  0.20582929  Acc:  0.9528667 Val_loss =  0.38527578 Val_acc =  0.8897\n",
            "Iteration  979 : Loss =  0.20580389  Acc:  0.95385 Val_loss =  0.38612166 Val_acc =  0.8928\n",
            "Iteration  980 : Loss =  0.20590681  Acc:  0.95246667 Val_loss =  0.38589558 Val_acc =  0.8893\n",
            "Iteration  981 : Loss =  0.20591815  Acc:  0.9539667 Val_loss =  0.38637352 Val_acc =  0.8921\n",
            "Iteration  982 : Loss =  0.20657843  Acc:  0.95225 Val_loss =  0.38668773 Val_acc =  0.8901\n",
            "Iteration  983 : Loss =  0.20729688  Acc:  0.95203334 Val_loss =  0.38765717 Val_acc =  0.8916\n",
            "Iteration  984 : Loss =  0.20786414  Acc:  0.9515167 Val_loss =  0.3884571 Val_acc =  0.8919\n",
            "Iteration  985 : Loss =  0.20862627  Acc:  0.95075 Val_loss =  0.3894443 Val_acc =  0.8887\n",
            "Iteration  986 : Loss =  0.2079114  Acc:  0.95178336 Val_loss =  0.388709 Val_acc =  0.8912\n",
            "Iteration  987 : Loss =  0.20853736  Acc:  0.9507 Val_loss =  0.3898658 Val_acc =  0.8896\n",
            "Iteration  988 : Loss =  0.20735517  Acc:  0.9518833 Val_loss =  0.38794276 Val_acc =  0.8915\n",
            "Iteration  989 : Loss =  0.2071594  Acc:  0.9518167 Val_loss =  0.38852426 Val_acc =  0.8905\n",
            "Iteration  990 : Loss =  0.20515022  Acc:  0.9535667 Val_loss =  0.38576385 Val_acc =  0.8922\n",
            "Iteration  991 : Loss =  0.20324697  Acc:  0.95385 Val_loss =  0.3844477 Val_acc =  0.8918\n",
            "Iteration  992 : Loss =  0.20192921  Acc:  0.95525 Val_loss =  0.382999 Val_acc =  0.8914\n",
            "Iteration  993 : Loss =  0.2020327  Acc:  0.9546 Val_loss =  0.38329166 Val_acc =  0.8909\n",
            "Iteration  994 : Loss =  0.20346385  Acc:  0.9537333 Val_loss =  0.3853992 Val_acc =  0.8906\n",
            "Iteration  995 : Loss =  0.20501977  Acc:  0.95306665 Val_loss =  0.38671422 Val_acc =  0.8919\n",
            "Iteration  996 : Loss =  0.20738786  Acc:  0.9516 Val_loss =  0.3900863 Val_acc =  0.8896\n",
            "Iteration  997 : Loss =  0.20753485  Acc:  0.9515667 Val_loss =  0.3897628 Val_acc =  0.8907\n",
            "Iteration  998 : Loss =  0.20931342  Acc:  0.94975 Val_loss =  0.3922231 Val_acc =  0.889\n",
            "Iteration  999 : Loss =  0.20768559  Acc:  0.95161664 Val_loss =  0.39007267 Val_acc =  0.8902\n",
            "Iteration  1000 : Loss =  0.20741573  Acc:  0.95136666 Val_loss =  0.3900027 Val_acc =  0.8891\n",
            "Iteration  1001 : Loss =  0.20551029  Acc:  0.95265 Val_loss =  0.38792434 Val_acc =  0.8913\n",
            "Iteration  1002 : Loss =  0.20387933  Acc:  0.9540833 Val_loss =  0.3862762 Val_acc =  0.8921\n",
            "Iteration  1003 : Loss =  0.20278391  Acc:  0.95423335 Val_loss =  0.3855752 Val_acc =  0.8903\n",
            "Iteration  1004 : Loss =  0.2025165  Acc:  0.95525 Val_loss =  0.38528076 Val_acc =  0.8925\n",
            "Iteration  1005 : Loss =  0.2033019  Acc:  0.95415 Val_loss =  0.38627678 Val_acc =  0.8901\n",
            "Iteration  1006 : Loss =  0.20467633  Acc:  0.954 Val_loss =  0.3879571 Val_acc =  0.8917\n",
            "Iteration  1007 : Loss =  0.20596471  Acc:  0.9525333 Val_loss =  0.38906512 Val_acc =  0.8895\n",
            "Iteration  1008 : Loss =  0.2058602  Acc:  0.9528 Val_loss =  0.38927975 Val_acc =  0.8924\n",
            "Iteration  1009 : Loss =  0.2050437  Acc:  0.95266664 Val_loss =  0.38844007 Val_acc =  0.8903\n",
            "Iteration  1010 : Loss =  0.20310447  Acc:  0.95411664 Val_loss =  0.3865569 Val_acc =  0.8917\n",
            "Iteration  1011 : Loss =  0.20247105  Acc:  0.95451665 Val_loss =  0.38638732 Val_acc =  0.8915\n",
            "Iteration  1012 : Loss =  0.20303491  Acc:  0.954 Val_loss =  0.38659433 Val_acc =  0.8911\n",
            "Iteration  1013 : Loss =  0.20442194  Acc:  0.95413333 Val_loss =  0.38902688 Val_acc =  0.8914\n",
            "Iteration  1014 : Loss =  0.20681818  Acc:  0.9519333 Val_loss =  0.39062655 Val_acc =  0.8893\n",
            "Iteration  1015 : Loss =  0.2072527  Acc:  0.95275 Val_loss =  0.3920226 Val_acc =  0.891\n",
            "Iteration  1016 : Loss =  0.20767976  Acc:  0.9512333 Val_loss =  0.3916738 Val_acc =  0.8884\n",
            "Iteration  1017 : Loss =  0.2052114  Acc:  0.95428336 Val_loss =  0.38936135 Val_acc =  0.8913\n",
            "Iteration  1018 : Loss =  0.20302615  Acc:  0.9539833 Val_loss =  0.38694564 Val_acc =  0.8893\n",
            "Iteration  1019 : Loss =  0.20153809  Acc:  0.9557833 Val_loss =  0.38544017 Val_acc =  0.8931\n",
            "Iteration  1020 : Loss =  0.2013978  Acc:  0.95491666 Val_loss =  0.38621807 Val_acc =  0.8918\n",
            "Iteration  1021 : Loss =  0.20178336  Acc:  0.9544 Val_loss =  0.3864692 Val_acc =  0.8901\n",
            "Iteration  1022 : Loss =  0.20202062  Acc:  0.9555 Val_loss =  0.38768858 Val_acc =  0.8931\n",
            "Iteration  1023 : Loss =  0.20249338  Acc:  0.9539667 Val_loss =  0.38787836 Val_acc =  0.8888\n",
            "Iteration  1024 : Loss =  0.20308638  Acc:  0.95491666 Val_loss =  0.38861468 Val_acc =  0.8922\n",
            "Iteration  1025 : Loss =  0.20381565  Acc:  0.95323336 Val_loss =  0.38940847 Val_acc =  0.8882\n",
            "Iteration  1026 : Loss =  0.20366326  Acc:  0.95446664 Val_loss =  0.38907745 Val_acc =  0.8923\n",
            "Iteration  1027 : Loss =  0.20313889  Acc:  0.9535667 Val_loss =  0.38879132 Val_acc =  0.8895\n",
            "Iteration  1028 : Loss =  0.20221728  Acc:  0.9543667 Val_loss =  0.3879579 Val_acc =  0.8915\n",
            "Iteration  1029 : Loss =  0.2019735  Acc:  0.95428336 Val_loss =  0.38785672 Val_acc =  0.8919\n",
            "Iteration  1030 : Loss =  0.2039819  Acc:  0.95271665 Val_loss =  0.39035574 Val_acc =  0.89\n",
            "Iteration  1031 : Loss =  0.20564914  Acc:  0.95225 Val_loss =  0.39158607 Val_acc =  0.8911\n",
            "Iteration  1032 : Loss =  0.20962748  Acc:  0.9496667 Val_loss =  0.3966348 Val_acc =  0.8887\n",
            "Iteration  1033 : Loss =  0.20765688  Acc:  0.95095 Val_loss =  0.39364323 Val_acc =  0.8906\n",
            "Iteration  1034 : Loss =  0.20658547  Acc:  0.9511833 Val_loss =  0.3934278 Val_acc =  0.8898\n",
            "Iteration  1035 : Loss =  0.20161828  Acc:  0.95455 Val_loss =  0.3877196 Val_acc =  0.8917\n",
            "Iteration  1036 : Loss =  0.19964083  Acc:  0.95601666 Val_loss =  0.38600618 Val_acc =  0.8918\n",
            "Iteration  1037 : Loss =  0.20007804  Acc:  0.9554167 Val_loss =  0.38682118 Val_acc =  0.89\n",
            "Iteration  1038 : Loss =  0.20117629  Acc:  0.9554833 Val_loss =  0.3880369 Val_acc =  0.893\n",
            "Iteration  1039 : Loss =  0.20159566  Acc:  0.9543333 Val_loss =  0.38920534 Val_acc =  0.8897\n",
            "Iteration  1040 : Loss =  0.20084855  Acc:  0.95521665 Val_loss =  0.3883209 Val_acc =  0.8924\n",
            "Iteration  1041 : Loss =  0.20040429  Acc:  0.95486665 Val_loss =  0.38800883 Val_acc =  0.8907\n",
            "Iteration  1042 : Loss =  0.20051596  Acc:  0.9551333 Val_loss =  0.38814843 Val_acc =  0.8921\n",
            "Iteration  1043 : Loss =  0.20139746  Acc:  0.9540333 Val_loss =  0.38880044 Val_acc =  0.8907\n",
            "Iteration  1044 : Loss =  0.20200273  Acc:  0.9539 Val_loss =  0.38974753 Val_acc =  0.8908\n",
            "Iteration  1045 : Loss =  0.20231298  Acc:  0.9540167 Val_loss =  0.3901705 Val_acc =  0.8912\n",
            "Iteration  1046 : Loss =  0.20279577  Acc:  0.95358336 Val_loss =  0.39070186 Val_acc =  0.89\n",
            "Iteration  1047 : Loss =  0.20414966  Acc:  0.95316666 Val_loss =  0.39270547 Val_acc =  0.8905\n",
            "Iteration  1048 : Loss =  0.20718312  Acc:  0.95161664 Val_loss =  0.39499706 Val_acc =  0.8883\n",
            "Iteration  1049 : Loss =  0.20821847  Acc:  0.95145 Val_loss =  0.397209 Val_acc =  0.8903\n",
            "Iteration  1050 : Loss =  0.20796129  Acc:  0.9511333 Val_loss =  0.39540893 Val_acc =  0.8883\n",
            "Iteration  1051 : Loss =  0.20223567  Acc:  0.95425 Val_loss =  0.39043975 Val_acc =  0.8912\n",
            "Iteration  1052 : Loss =  0.19774298  Acc:  0.95601666 Val_loss =  0.38529018 Val_acc =  0.8917\n",
            "Iteration  1053 : Loss =  0.19756566  Acc:  0.95666665 Val_loss =  0.38537142 Val_acc =  0.8905\n",
            "Iteration  1054 : Loss =  0.20019802  Acc:  0.95573336 Val_loss =  0.38915753 Val_acc =  0.8924\n",
            "Iteration  1055 : Loss =  0.20275143  Acc:  0.95375 Val_loss =  0.39121145 Val_acc =  0.8904\n",
            "Iteration  1056 : Loss =  0.20184818  Acc:  0.9551833 Val_loss =  0.39156252 Val_acc =  0.8906\n",
            "Iteration  1057 : Loss =  0.20088127  Acc:  0.95455 Val_loss =  0.3897489 Val_acc =  0.8889\n",
            "Iteration  1058 : Loss =  0.19955778  Acc:  0.9562 Val_loss =  0.38884592 Val_acc =  0.8919\n",
            "Iteration  1059 : Loss =  0.19959143  Acc:  0.9551833 Val_loss =  0.38881102 Val_acc =  0.8909\n",
            "Iteration  1060 : Loss =  0.20003077  Acc:  0.9546 Val_loss =  0.38918176 Val_acc =  0.8911\n",
            "Iteration  1061 : Loss =  0.20015842  Acc:  0.95521665 Val_loss =  0.38995346 Val_acc =  0.8913\n",
            "Iteration  1062 : Loss =  0.199974  Acc:  0.9550667 Val_loss =  0.3897118 Val_acc =  0.8897\n",
            "Iteration  1063 : Loss =  0.19991094  Acc:  0.9554667 Val_loss =  0.38981035 Val_acc =  0.8931\n",
            "Iteration  1064 : Loss =  0.20110103  Acc:  0.95453334 Val_loss =  0.391378 Val_acc =  0.8886\n",
            "Iteration  1065 : Loss =  0.20143235  Acc:  0.9543333 Val_loss =  0.3911655 Val_acc =  0.8923\n",
            "Iteration  1066 : Loss =  0.20160827  Acc:  0.9541 Val_loss =  0.3919787 Val_acc =  0.8901\n",
            "Iteration  1067 : Loss =  0.19908679  Acc:  0.9557833 Val_loss =  0.38863912 Val_acc =  0.893\n",
            "Iteration  1068 : Loss =  0.19700652  Acc:  0.9564 Val_loss =  0.38712108 Val_acc =  0.8914\n",
            "Iteration  1069 : Loss =  0.19603047  Acc:  0.95725 Val_loss =  0.38614768 Val_acc =  0.8925\n",
            "Iteration  1070 : Loss =  0.19686356  Acc:  0.95715 Val_loss =  0.3873507 Val_acc =  0.8926\n",
            "Iteration  1071 : Loss =  0.19814385  Acc:  0.95608336 Val_loss =  0.38915217 Val_acc =  0.8885\n",
            "Iteration  1072 : Loss =  0.19823688  Acc:  0.95706666 Val_loss =  0.3894235 Val_acc =  0.8927\n",
            "Iteration  1073 : Loss =  0.19737875  Acc:  0.9563 Val_loss =  0.38878524 Val_acc =  0.8887\n",
            "Iteration  1074 : Loss =  0.19612359  Acc:  0.95773333 Val_loss =  0.3875423 Val_acc =  0.8924\n",
            "Iteration  1075 : Loss =  0.19555587  Acc:  0.9571667 Val_loss =  0.38677633 Val_acc =  0.8907\n",
            "Iteration  1076 : Loss =  0.19546765  Acc:  0.9576 Val_loss =  0.3871394 Val_acc =  0.893\n",
            "Iteration  1077 : Loss =  0.19545159  Acc:  0.9575 Val_loss =  0.3865961 Val_acc =  0.8914\n",
            "Iteration  1078 : Loss =  0.19501704  Acc:  0.9575667 Val_loss =  0.3867673 Val_acc =  0.8917\n",
            "Iteration  1079 : Loss =  0.19443503  Acc:  0.9583833 Val_loss =  0.38598958 Val_acc =  0.8917\n",
            "Iteration  1080 : Loss =  0.19413279  Acc:  0.95776665 Val_loss =  0.38580734 Val_acc =  0.8921\n",
            "Iteration  1081 : Loss =  0.19428532  Acc:  0.95845 Val_loss =  0.38623813 Val_acc =  0.8921\n",
            "Iteration  1082 : Loss =  0.19473293  Acc:  0.95786667 Val_loss =  0.38652405 Val_acc =  0.8911\n",
            "Iteration  1083 : Loss =  0.19506884  Acc:  0.95808333 Val_loss =  0.38739035 Val_acc =  0.8928\n",
            "Iteration  1084 : Loss =  0.195199  Acc:  0.9573 Val_loss =  0.3871752 Val_acc =  0.8915\n",
            "Iteration  1085 : Loss =  0.19493018  Acc:  0.95811665 Val_loss =  0.38753796 Val_acc =  0.8926\n",
            "Iteration  1086 : Loss =  0.1946815  Acc:  0.9579833 Val_loss =  0.3870547 Val_acc =  0.8904\n",
            "Iteration  1087 : Loss =  0.1944947  Acc:  0.95851666 Val_loss =  0.38728234 Val_acc =  0.8926\n",
            "Iteration  1088 : Loss =  0.1945654  Acc:  0.95773333 Val_loss =  0.38728026 Val_acc =  0.8916\n",
            "Iteration  1089 : Loss =  0.19467625  Acc:  0.95856667 Val_loss =  0.3876024 Val_acc =  0.8926\n",
            "Iteration  1090 : Loss =  0.1948128  Acc:  0.9576 Val_loss =  0.38777572 Val_acc =  0.8909\n",
            "Iteration  1091 : Loss =  0.19486836  Acc:  0.9584 Val_loss =  0.3880415 Val_acc =  0.8932\n",
            "Iteration  1092 : Loss =  0.1952494  Acc:  0.9572833 Val_loss =  0.38849473 Val_acc =  0.8903\n",
            "Iteration  1093 : Loss =  0.19638437  Acc:  0.95715 Val_loss =  0.3900966 Val_acc =  0.8916\n",
            "Iteration  1094 : Loss =  0.19948481  Acc:  0.95485 Val_loss =  0.39307195 Val_acc =  0.8902\n",
            "Iteration  1095 : Loss =  0.2071087  Acc:  0.94951665 Val_loss =  0.40178424 Val_acc =  0.8892\n",
            "Iteration  1096 : Loss =  0.21592036  Acc:  0.9450167 Val_loss =  0.4103387 Val_acc =  0.8829\n",
            "Iteration  1097 : Loss =  0.2373037  Acc:  0.9361167 Val_loss =  0.4331225 Val_acc =  0.882\n",
            "Iteration  1098 : Loss =  0.21997467  Acc:  0.9431667 Val_loss =  0.41386068 Val_acc =  0.883\n",
            "Iteration  1099 : Loss =  0.20927969  Acc:  0.94835 Val_loss =  0.4019215 Val_acc =  0.8882\n",
            "Iteration  1100 : Loss =  0.20034847  Acc:  0.9555333 Val_loss =  0.39267787 Val_acc =  0.8919\n",
            "Iteration  1101 : Loss =  0.21020022  Acc:  0.94941664 Val_loss =  0.40251374 Val_acc =  0.8854\n",
            "Iteration  1102 : Loss =  0.21592122  Acc:  0.945 Val_loss =  0.4103887 Val_acc =  0.8868\n",
            "Iteration  1103 : Loss =  0.20297644  Acc:  0.95275 Val_loss =  0.39604285 Val_acc =  0.8884\n",
            "Iteration  1104 : Loss =  0.1942742  Acc:  0.9579833 Val_loss =  0.38710535 Val_acc =  0.8912\n",
            "Iteration  1105 : Loss =  0.19959736  Acc:  0.9547667 Val_loss =  0.3938167 Val_acc =  0.891\n",
            "Iteration  1106 : Loss =  0.20767453  Acc:  0.9507 Val_loss =  0.4013029 Val_acc =  0.8867\n",
            "Iteration  1107 : Loss =  0.20604506  Acc:  0.9506 Val_loss =  0.40063435 Val_acc =  0.888\n",
            "Iteration  1108 : Loss =  0.19753441  Acc:  0.9558333 Val_loss =  0.39194718 Val_acc =  0.8904\n",
            "Iteration  1109 : Loss =  0.19379821  Acc:  0.95811665 Val_loss =  0.3876779 Val_acc =  0.8918\n",
            "Iteration  1110 : Loss =  0.19759393  Acc:  0.9558333 Val_loss =  0.39249197 Val_acc =  0.8914\n",
            "Iteration  1111 : Loss =  0.20180506  Acc:  0.9533833 Val_loss =  0.3966881 Val_acc =  0.8893\n",
            "Iteration  1112 : Loss =  0.19979209  Acc:  0.95428336 Val_loss =  0.3956753 Val_acc =  0.8906\n",
            "Iteration  1113 : Loss =  0.19532773  Acc:  0.95678335 Val_loss =  0.3904688 Val_acc =  0.8899\n",
            "Iteration  1114 : Loss =  0.19335568  Acc:  0.95865 Val_loss =  0.388217 Val_acc =  0.8918\n",
            "Iteration  1115 : Loss =  0.19533592  Acc:  0.9568667 Val_loss =  0.391038 Val_acc =  0.8917\n",
            "Iteration  1116 : Loss =  0.19746855  Acc:  0.95641667 Val_loss =  0.39243633 Val_acc =  0.8904\n",
            "Iteration  1117 : Loss =  0.19734482  Acc:  0.9557 Val_loss =  0.393213 Val_acc =  0.8919\n",
            "Iteration  1118 : Loss =  0.19487488  Acc:  0.9572167 Val_loss =  0.39034787 Val_acc =  0.8921\n",
            "Iteration  1119 : Loss =  0.1932638  Acc:  0.9583833 Val_loss =  0.38884953 Val_acc =  0.8907\n",
            "Iteration  1120 : Loss =  0.19327506  Acc:  0.9582667 Val_loss =  0.38939425 Val_acc =  0.8911\n",
            "Iteration  1121 : Loss =  0.19400951  Acc:  0.9579 Val_loss =  0.38966903 Val_acc =  0.8916\n",
            "Iteration  1122 : Loss =  0.19361725  Acc:  0.95846665 Val_loss =  0.3901896 Val_acc =  0.8918\n",
            "Iteration  1123 : Loss =  0.19277948  Acc:  0.9580167 Val_loss =  0.3890526 Val_acc =  0.8904\n",
            "Iteration  1124 : Loss =  0.19218048  Acc:  0.9593167 Val_loss =  0.38896796 Val_acc =  0.8925\n",
            "Iteration  1125 : Loss =  0.1921913  Acc:  0.95865 Val_loss =  0.3892583 Val_acc =  0.8915\n",
            "Iteration  1126 : Loss =  0.19225077  Acc:  0.95885 Val_loss =  0.38881713 Val_acc =  0.8924\n",
            "Iteration  1127 : Loss =  0.19192821  Acc:  0.95916665 Val_loss =  0.3890838 Val_acc =  0.8908\n",
            "Iteration  1128 : Loss =  0.19136311  Acc:  0.9591 Val_loss =  0.3881783 Val_acc =  0.8922\n",
            "Iteration  1129 : Loss =  0.19114906  Acc:  0.95965 Val_loss =  0.38836366 Val_acc =  0.8913\n",
            "Iteration  1130 : Loss =  0.19137782  Acc:  0.95886666 Val_loss =  0.38859028 Val_acc =  0.8913\n",
            "Iteration  1131 : Loss =  0.19157341  Acc:  0.95958334 Val_loss =  0.38857466 Val_acc =  0.8923\n",
            "Iteration  1132 : Loss =  0.19145602  Acc:  0.95918334 Val_loss =  0.38880825 Val_acc =  0.8913\n",
            "Iteration  1133 : Loss =  0.19082028  Acc:  0.9598167 Val_loss =  0.38799894 Val_acc =  0.8921\n",
            "Iteration  1134 : Loss =  0.19016221  Acc:  0.9598 Val_loss =  0.38777816 Val_acc =  0.8923\n",
            "Iteration  1135 : Loss =  0.18998542  Acc:  0.9599 Val_loss =  0.38757437 Val_acc =  0.8922\n",
            "Iteration  1136 : Loss =  0.19040367  Acc:  0.95985 Val_loss =  0.38828456 Val_acc =  0.8914\n",
            "Iteration  1137 : Loss =  0.1911753  Acc:  0.95953333 Val_loss =  0.3893454 Val_acc =  0.8921\n",
            "Iteration  1138 : Loss =  0.19210148  Acc:  0.95841664 Val_loss =  0.39017713 Val_acc =  0.8906\n",
            "Iteration  1139 : Loss =  0.19325957  Acc:  0.95853335 Val_loss =  0.3916875 Val_acc =  0.8916\n",
            "Iteration  1140 : Loss =  0.1955003  Acc:  0.9565833 Val_loss =  0.39405397 Val_acc =  0.8891\n",
            "Iteration  1141 : Loss =  0.19947277  Acc:  0.95475 Val_loss =  0.398411 Val_acc =  0.8904\n",
            "Iteration  1142 : Loss =  0.20734768  Acc:  0.94995 Val_loss =  0.4066093 Val_acc =  0.8863\n",
            "Iteration  1143 : Loss =  0.21565348  Acc:  0.94675 Val_loss =  0.41507408 Val_acc =  0.8846\n",
            "Iteration  1144 : Loss =  0.22328085  Acc:  0.94311666 Val_loss =  0.4224523 Val_acc =  0.8822\n",
            "Iteration  1145 : Loss =  0.21283  Acc:  0.94801664 Val_loss =  0.4106243 Val_acc =  0.8858\n",
            "Iteration  1146 : Loss =  0.1959388  Acc:  0.95671666 Val_loss =  0.39145124 Val_acc =  0.8897\n",
            "Iteration  1147 : Loss =  0.19328883  Acc:  0.9576333 Val_loss =  0.38864636 Val_acc =  0.8919\n",
            "Iteration  1148 : Loss =  0.20397776  Acc:  0.95273334 Val_loss =  0.40182346 Val_acc =  0.8896\n",
            "Iteration  1149 : Loss =  0.20430103  Acc:  0.95128334 Val_loss =  0.40295005 Val_acc =  0.887\n",
            "Iteration  1150 : Loss =  0.19293562  Acc:  0.95853335 Val_loss =  0.39113387 Val_acc =  0.8934\n",
            "Iteration  1151 : Loss =  0.19272098  Acc:  0.9593667 Val_loss =  0.3921413 Val_acc =  0.8926\n",
            "Iteration  1152 : Loss =  0.19963278  Acc:  0.95531666 Val_loss =  0.39968422 Val_acc =  0.8891\n",
            "Iteration  1153 : Loss =  0.1990526  Acc:  0.9554833 Val_loss =  0.3989778 Val_acc =  0.891\n",
            "Iteration  1154 : Loss =  0.19184452  Acc:  0.9583167 Val_loss =  0.39161706 Val_acc =  0.8911\n",
            "Iteration  1155 : Loss =  0.19055197  Acc:  0.95915 Val_loss =  0.39002496 Val_acc =  0.8918\n",
            "Iteration  1156 : Loss =  0.19539234  Acc:  0.9579833 Val_loss =  0.39560896 Val_acc =  0.891\n",
            "Iteration  1157 : Loss =  0.19549291  Acc:  0.9565333 Val_loss =  0.3956578 Val_acc =  0.8888\n",
            "Iteration  1158 : Loss =  0.19030806  Acc:  0.96038336 Val_loss =  0.39027742 Val_acc =  0.8932\n",
            "Iteration  1159 : Loss =  0.18864578  Acc:  0.96106666 Val_loss =  0.38863164 Val_acc =  0.8926\n",
            "Iteration  1160 : Loss =  0.19204983  Acc:  0.9583 Val_loss =  0.39234802 Val_acc =  0.8896\n",
            "Iteration  1161 : Loss =  0.19402605  Acc:  0.9584 Val_loss =  0.3948993 Val_acc =  0.8909\n",
            "Iteration  1162 : Loss =  0.19171092  Acc:  0.9587 Val_loss =  0.39221555 Val_acc =  0.8898\n",
            "Iteration  1163 : Loss =  0.18843696  Acc:  0.96136665 Val_loss =  0.3889668 Val_acc =  0.8923\n",
            "Iteration  1164 : Loss =  0.18825552  Acc:  0.9608 Val_loss =  0.38895208 Val_acc =  0.8924\n",
            "Iteration  1165 : Loss =  0.19032958  Acc:  0.95956665 Val_loss =  0.39139858 Val_acc =  0.89\n",
            "Iteration  1166 : Loss =  0.19126195  Acc:  0.95993334 Val_loss =  0.39252508 Val_acc =  0.8927\n",
            "Iteration  1167 : Loss =  0.18971884  Acc:  0.95988333 Val_loss =  0.3909217 Val_acc =  0.8902\n",
            "Iteration  1168 : Loss =  0.18781106  Acc:  0.96098334 Val_loss =  0.38922945 Val_acc =  0.8922\n",
            "Iteration  1169 : Loss =  0.1876325  Acc:  0.96138334 Val_loss =  0.38907838 Val_acc =  0.892\n",
            "Iteration  1170 : Loss =  0.18885793  Acc:  0.96021664 Val_loss =  0.39035767 Val_acc =  0.8911\n",
            "Iteration  1171 : Loss =  0.18950889  Acc:  0.9611167 Val_loss =  0.3914202 Val_acc =  0.892\n",
            "Iteration  1172 : Loss =  0.1888223  Acc:  0.9601667 Val_loss =  0.39058784 Val_acc =  0.8907\n",
            "Iteration  1173 : Loss =  0.18770641  Acc:  0.96125 Val_loss =  0.38965523 Val_acc =  0.8926\n",
            "Iteration  1174 : Loss =  0.1874301  Acc:  0.9606 Val_loss =  0.38929322 Val_acc =  0.893\n",
            "Iteration  1175 : Loss =  0.18824416  Acc:  0.96023333 Val_loss =  0.39053288 Val_acc =  0.8911\n",
            "Iteration  1176 : Loss =  0.18912269  Acc:  0.95996666 Val_loss =  0.3913022 Val_acc =  0.8929\n",
            "Iteration  1177 : Loss =  0.18989137  Acc:  0.95918334 Val_loss =  0.3925529 Val_acc =  0.8901\n",
            "Iteration  1178 : Loss =  0.18997467  Acc:  0.9594333 Val_loss =  0.3925105 Val_acc =  0.8917\n",
            "Iteration  1179 : Loss =  0.19094235  Acc:  0.95841664 Val_loss =  0.39369348 Val_acc =  0.8902\n",
            "Iteration  1180 : Loss =  0.19208246  Acc:  0.95776665 Val_loss =  0.39469495 Val_acc =  0.8902\n",
            "Iteration  1181 : Loss =  0.19432919  Acc:  0.9565667 Val_loss =  0.39754903 Val_acc =  0.89\n",
            "Iteration  1182 : Loss =  0.19597456  Acc:  0.95595 Val_loss =  0.39886546 Val_acc =  0.8884\n",
            "Iteration  1183 : Loss =  0.19621068  Acc:  0.95563334 Val_loss =  0.3994639 Val_acc =  0.8905\n",
            "Iteration  1184 : Loss =  0.19384924  Acc:  0.95711666 Val_loss =  0.39661357 Val_acc =  0.889\n",
            "Iteration  1185 : Loss =  0.19059941  Acc:  0.95853335 Val_loss =  0.39350408 Val_acc =  0.8912\n",
            "Iteration  1186 : Loss =  0.1880766  Acc:  0.96025 Val_loss =  0.3905356 Val_acc =  0.8933\n",
            "Iteration  1187 : Loss =  0.1877994  Acc:  0.96071666 Val_loss =  0.3911103 Val_acc =  0.8911\n",
            "Iteration  1188 : Loss =  0.18837449  Acc:  0.96131665 Val_loss =  0.39154163 Val_acc =  0.8922\n",
            "Iteration  1189 : Loss =  0.1889256  Acc:  0.9593833 Val_loss =  0.39268172 Val_acc =  0.891\n",
            "Iteration  1190 : Loss =  0.18924193  Acc:  0.9600833 Val_loss =  0.3931981 Val_acc =  0.8916\n",
            "Iteration  1191 : Loss =  0.1900464  Acc:  0.95916665 Val_loss =  0.39373976 Val_acc =  0.8913\n",
            "Iteration  1192 : Loss =  0.19208571  Acc:  0.95745 Val_loss =  0.3964293 Val_acc =  0.8913\n",
            "Iteration  1193 : Loss =  0.19301333  Acc:  0.9574 Val_loss =  0.39681953 Val_acc =  0.8908\n",
            "Iteration  1194 : Loss =  0.1954022  Acc:  0.9555 Val_loss =  0.4000977 Val_acc =  0.8902\n",
            "Iteration  1195 : Loss =  0.19245067  Acc:  0.95785 Val_loss =  0.39656866 Val_acc =  0.8914\n",
            "Iteration  1196 : Loss =  0.19042625  Acc:  0.95821667 Val_loss =  0.39496103 Val_acc =  0.8905\n",
            "Iteration  1197 : Loss =  0.18708292  Acc:  0.9607667 Val_loss =  0.39113027 Val_acc =  0.8924\n",
            "Iteration  1198 : Loss =  0.18607935  Acc:  0.96165 Val_loss =  0.3902349 Val_acc =  0.893\n",
            "Iteration  1199 : Loss =  0.1869435  Acc:  0.96138334 Val_loss =  0.39184967 Val_acc =  0.891\n",
            "\n",
            "The final test accuracy of the Fashion MNIST dataset is 0.8932999968528748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJJS55tzm16L",
        "colab_type": "code",
        "outputId": "e9b59016-316f-4192-e5f2-0adbb994498b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(test_loss, label=\"Val Loss\", color='blue')\n",
        "plt.plot(test_acc, label=\"Val Acc\", color='orange')\n",
        "plt.plot(training_loss, label=\"Train Loss\", color='green')\n",
        "plt.plot(training_acc, label=\"Train Acc\", color='red')\n",
        "plt.title(\"Number of Iterations vs Training/Validation Loss and Accuracy\")\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Print the test accuracy \n",
        "print()\n",
        "print(\"The final test accuracy of the Fashion MNIST dataset is {}\".format(final_test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The final test accuracy of the Fashion MNIST dataset is 0.8932999968528748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNkZ01N2nNY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}