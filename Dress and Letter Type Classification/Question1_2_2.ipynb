{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question1_2_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-fVVPUgTLBI",
        "colab_type": "text"
      },
      "source": [
        "### Network Architecture B\n",
        "\n",
        "- Layer 1 - 300 neurons (ReLU activation function)\n",
        "- Layer 2 - 100 neurons (ReLU activation function) \n",
        "- Layer 2 - SoftMax Layer (10 neurons)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW_aiNbQTQ32",
        "colab_type": "code",
        "outputId": "860f0812-1848-4804-84c5-d7e5348332ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Colab has two versions of TensorFlow installed: a 1.x version and a 2.x version. \n",
        "# Colab currently uses TF 1.x by default\n",
        "# To enable TF2 execute the following code\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfjwVfZBUIzw",
        "colab_type": "text"
      },
      "source": [
        "### Importing the Libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRIegl_KTby3",
        "colab_type": "code",
        "outputId": "c7f765cf-166b-42c6-8316-37b320caa442",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Importing the libraries \n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skswo6BtUM4f",
        "colab_type": "text"
      },
      "source": [
        "### Creation of 3 Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvR6_Zp-UD2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -------------------------------------------CODE TO PREPARE THE DATASET---------------------------------------------------- \n",
        "\n",
        "def prepare_dataset(fashion_mnist):\n",
        "  # load the training and test data    \n",
        "  (tr_x, tr_y), (te_x, te_y) = fashion_mnist.load_data()\n",
        "  # reshape the feature data\n",
        "  tr_x = tr_x.reshape(tr_x.shape[0], 784)\n",
        "  te_x = te_x.reshape(te_x.shape[0], 784)\n",
        "  # noramlise feature data\n",
        "  tr_x = tr_x / 255.0\n",
        "  te_x = te_x / 255.0\n",
        "  # one hot encode the training labels and get the transpose\n",
        "  tr_y = np_utils.to_categorical(tr_y,10)\n",
        "  tr_y = tr_y.T\n",
        "  # one hot encode the test labels and get the transpose\n",
        "  te_y = np_utils.to_categorical(te_y,10)\n",
        "  te_y = te_y.T\n",
        "  return tr_x, tr_y, te_x, te_y\n",
        "\n",
        "# -----------------------------------CODE TO CALCULATE THE PROBABILITY OF EACH CLASS GIVEN THE TRAINING INSTANCE--------------------------------------\n",
        "\n",
        "def forward_pass(X_train, W1, b1, W2, b2, W3, b3):\n",
        "  \"\"\"\n",
        "  Return the predicted 10 class probabilities matrix for each of the training instances \n",
        "  \"\"\"\n",
        "  # Calculate the pre-activation outputs for each of the 300 neurons in the hidden layer 1 for each of the training instance \n",
        "  # Will get 300 outputs for a single training instance in the form of (300*60000) matrix \n",
        "  # The size of the W1 is (300*784)\n",
        "  # The size of the training feature matrix is (784*60000)\n",
        "  # The size of the b1 is (300*1)\n",
        "  A1=  tf.matmul(W1, X_train) + b1\n",
        "  # Convert each element in A1 through relu activation function \n",
        "  H1= tf.math.maximum(A1, 0)\n",
        "\n",
        "  # Calculate the pre-activation outputs for each of the 100 neurons in the hidden layer 2 for each of the training instance \n",
        "  # Will get 100 outputs for a single training instance in the form of (100*60000) matrix \n",
        "  # The size of the W2 is (100*300)\n",
        "  # The size of the H1 matrix is (300*60000)\n",
        "  # The size of the b2 is (100*1)\n",
        "  A2=  tf.matmul(W2, H1) + b2\n",
        "  # Convert each element in A1 through relu activation function \n",
        "  H2= tf.math.maximum(A2, 0)\n",
        "\n",
        "  # Calculate the pre-activation outputs for each of the 10 neurons in the softmax layer for each of the training instance \n",
        "  # Will get 10 outputs for a single training instance in the form of (10*60000) matrix \n",
        "  # The size of the W3 is (10*100)\n",
        "  # The size of the H2 matrix is (100*60000)\n",
        "  # The size of the bias matrix b3 is (10*1)\n",
        "  A3= tf.matmul(W3, H2) + b3\n",
        "  # Calculate a new matrix where each element is e to the power of pre-activation outputs \n",
        "  exponential_matrix= tf.math.exp(A3)\n",
        "  # Calculation of the final probabilities of each of the 10 classes for each instance in the training set \n",
        "  # Column wise sum calculation \n",
        "  column_sum= tf.reduce_sum(exponential_matrix, 0)\n",
        "  # Divide each element by the column sum so that each column is the probability of each class of a single instance \n",
        "  H3= exponential_matrix/column_sum \n",
        "  # Set the range so that the loss does not come out to be nan \n",
        "  H3= tf.clip_by_value(H3 ,1e-10, 1.0) \n",
        "  \n",
        "  return H3\n",
        "\n",
        "# -------------------------------- CODE TO CALCULATE THE LOSS FOR THE CURRENT SET OF TUNABLE PARAMETERS / WEIGHTS-------------------------------------\n",
        "\n",
        "def cross_entropy(y_train, y_pred_matrix):\n",
        "  \"\"\"\n",
        "  Return the loss value given the predicted probabilities matrix and the actual probabilities matrix\n",
        "  \"\"\"\n",
        "  # Compute the log of each element of the prediction matrix \n",
        "  log_matrix= tf.math.log(y_pred_matrix)\n",
        "  # Multiply each element of the actual labels matrix with the log matrix \n",
        "  product_matrix= y_train *log_matrix\n",
        "  # Take the negation of each element in the product matrix \n",
        "  negated_product_matrix= -1*(product_matrix)\n",
        "  # Compute the cross entropy loss for each of the training instances \n",
        "  # This will contain individual loss for the all training instances \n",
        "  # This operation will perform the column wise sum \n",
        "  single_loss_matrix= tf.reduce_sum(negated_product_matrix, 0)\n",
        "  # Compute the mean cross entropy loss \n",
        "  mean_loss= tf.reduce_mean(single_loss_matrix)\n",
        "\n",
        "  return mean_loss\n",
        "\n",
        "# ----------------------------------------- CALCULATION OF THE TRAINING AND TEST SET ACCURACY------------------------------------------------------\n",
        "\n",
        "def return_labels(matrix):\n",
        "  \"\"\"\n",
        "  Return the corrosponding class label for each vector of probability instance \n",
        "  \"\"\" \n",
        "  class_labels= tf.argmax(matrix) \n",
        "  \n",
        "  return class_labels\n",
        "\n",
        "def calculate_accuracy(feature_data, label_data, W1, b1, W2, b2, W3, b3):\n",
        "  \"\"\" \n",
        "  Return the accuracy value (applicable for both train and the test set) for the given set of weights and the biases \n",
        "  \"\"\"\n",
        "  # Calculate the matrix of predicted probabilities through calling of forward pass \n",
        "  predicted_matrix= forward_pass(feature_data, W1, b1, W2, b2, W3, b3)\n",
        "  # Get the class labels of the actual labels \n",
        "  actual_labels= return_labels(label_data)\n",
        "  # Get the class labels of the predicted probabilities \n",
        "  predicted_labels= return_labels(predicted_matrix)\n",
        "  # Get the correct prediction in the form of boolean array where 1 is correct prediction and 0 is the wrong prediction \n",
        "  correct_predictions= tf.cast(tf.equal(predicted_labels, actual_labels), tf.float32)\n",
        "  # Calculate the accuracy \n",
        "  accuracy= tf.reduce_mean(correct_predictions)\n",
        "\n",
        "  return accuracy \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmoM8duGURi_",
        "colab_type": "text"
      },
      "source": [
        "### Beginning of TensorFlow program "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJWUTJ_2UU8y",
        "colab_type": "code",
        "outputId": "4f6ef38f-403c-49b8-ff27-e27d2e0e5561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Loading the fashion MNIST data-set \n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "# Prepare the dataset \n",
        "train_features, train_labels, test_features, test_labels= prepare_dataset(fashion_mnist)\n",
        "\n",
        "# Get the transpose of feature data \n",
        "train_features= train_features.T \n",
        "test_features= test_features.T\n",
        "\n",
        "# Print the shape of our 4 data structures \n",
        "print( \"Shape of training features \", train_features.shape)\n",
        "print (\"Shape of training labels \", train_labels.shape)\n",
        "print()\n",
        "print( \"Shape of test features \", test_features.shape)\n",
        "print (\"Shape of test labels \", test_labels.shape)\n",
        "print()\n",
        "print(\"The training process of our Softmax Neural Network begins.....\")\n",
        "print()\n",
        "\n",
        "X_train= tf.cast(train_features, tf.float32)\n",
        "y_train= tf.cast(train_labels, tf.float32)\n",
        "X_test= tf.cast(test_features, tf.float32)\n",
        "y_test= tf.cast(test_labels, tf.float32)\n",
        "\n",
        "# Set the Number of features\n",
        "num_features=  X_train.shape[0]\n",
        "# We now specify the size of hidden layer 1\n",
        "hidden1_neurons= 300\n",
        "# We now specify the size of hidden layer 2\n",
        "hidden2_neurons= 100\n",
        "# We now specify the size of output layer \n",
        "output_neurons= 10 \n",
        "\n",
        "# Initialize the weight_matrix 1 and bias_matrix 1 \n",
        "# Each row of this matrix represents the 784 weights of a single neuron in the hidden layer \n",
        "W1= tf.Variable(tf.random.normal([hidden1_neurons, num_features], mean=0.0, stddev=0.05))\n",
        "# It is a column vector where each row/element represents the bias value for a single neuron in the hidden layer \n",
        "b1= tf.Variable(tf.random.normal([hidden1_neurons, 1], mean=0.0, stddev=0.05))\n",
        "\n",
        "# Initialize the weight_matrix 2 and bias_matrix 2 \n",
        "# Each row of this matrix represents the 300 weights of a single neuron in the softmax layer \n",
        "W2= tf.Variable(tf.random.normal([hidden2_neurons, hidden1_neurons], mean=0.0, stddev=0.05))\n",
        "# It is a column vector where each row/element represents the bias value for a single neuron in the softmax layer \n",
        "b2= tf.Variable(tf.random.normal([hidden2_neurons, 1], mean=0.0, stddev=0.05))\n",
        "\n",
        "# Initialize the weight_matrix 3 and bias_matrix 3\n",
        "# Each row of this matrix represents the 300 weights of a single neuron in the softmax layer \n",
        "W3= tf.Variable(tf.random.normal([output_neurons, hidden2_neurons], mean=0.0, stddev=0.05))\n",
        "# It is a column vector where each row/element represents the bias value for a single neuron in the softmax layer \n",
        "b3= tf.Variable(tf.random.normal([output_neurons, 1], mean=0.0, stddev=0.05))\n",
        "\n",
        "# Set the learning rate and the number of iterations \n",
        "learning_rate= 0.01 \n",
        "num_iterations= 1200\n",
        "\n",
        "# Adam optimizer to update the weights of the neural network \n",
        "adam_optimizer= tf.keras.optimizers.Adam()\n",
        "\n",
        "# Create the list to store the training accuracy and loss with each iteration \n",
        "training_loss= []\n",
        "training_acc= []\n",
        "# Create the list to store the test accuracy and loss with each iteration \n",
        "test_loss= []\n",
        "test_acc= []\n",
        "\n",
        "# Run the gradient descent to num_iterations number of times \n",
        "for iteration in range(num_iterations):\n",
        "\n",
        "  # Create an instance of GradientTape to monitor the forward pass and loss calculations\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Calculate the predicted probability matrix for the current weights and the biases for the training set\n",
        "    y_pred_matrix= forward_pass(X_train, W1, b1, W2, b2, W3, b3)\n",
        "    # Calculate the predicted probability matrix for the current weights and the biases for the test set\n",
        "    y_pred_test= forward_pass(X_test, W1, b1, W2, b2, W3, b3)\n",
        "    # Calculate the current training loss with the current predictions and the actual labels of the training set \n",
        "    current_loss_training= cross_entropy(y_train, y_pred_matrix)\n",
        "    # Calculate the current test loss with the current prediction and the actual labels of the test set \n",
        "    current_loss_test= cross_entropy(y_test, y_pred_test)\n",
        "  \n",
        "  # Calculate the gradients (partial derivates) of the loss with respect to each of the tunable weights \n",
        "  gradients= tape.gradient(current_loss_training, [W1, b1, W2, b2, W3, b3])\n",
        "\n",
        "  # Calculate the training accuracy with each iteration \n",
        "  training_accuracy= calculate_accuracy(X_train, y_train, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "  # Calculate the test accuracy with each each iteration \n",
        "  test_accuracy= calculate_accuracy(X_test, y_test, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "  # Print out the current iteration, current loss and current training accuracy \n",
        "  print(\"Iteration \",iteration, \": Loss = \",current_loss_training.numpy(),\" Acc: \", training_accuracy.numpy(),   'Val_loss = ',current_loss_test.numpy(),   'Val_acc = ', test_accuracy.numpy())\n",
        "\n",
        "  # Apply the Adam optimizer to update the weights and biases \n",
        "  adam_optimizer.apply_gradients(zip(gradients, [W1, b1, W2, b2, W3, b3]))\n",
        "\n",
        "  # Append the 4 values (train loss, train acc, test loss, test acc) with the each current iteration for the plotting \n",
        "  training_loss.append(current_loss_training.numpy())\n",
        "  training_acc.append(training_accuracy.numpy())\n",
        "  test_loss.append(current_loss_test.numpy())\n",
        "  test_acc.append(test_accuracy.numpy())\n",
        "\n",
        "# Calculate the test accuracy with the final updated weights and the biases through adam optimizer after running for certain number of iterations \n",
        "final_test_accuracy= calculate_accuracy(X_test, y_test, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "# Print the test accuracy \n",
        "print()\n",
        "print(\"The final test accuracy of the Fashion MNIST dataset is {}\".format(final_test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training features  (784, 60000)\n",
            "Shape of training labels  (10, 60000)\n",
            "\n",
            "Shape of test features  (784, 10000)\n",
            "Shape of test labels  (10, 10000)\n",
            "\n",
            "The training process of our Softmax Neural Network begins.....\n",
            "\n",
            "Iteration  0 : Loss =  2.3215837  Acc:  0.09211667 Val_loss =  2.3215597 Val_acc =  0.0928\n",
            "Iteration  1 : Loss =  2.1883035  Acc:  0.32018334 Val_loss =  2.1890702 Val_acc =  0.3236\n",
            "Iteration  2 : Loss =  2.075652  Acc:  0.52031666 Val_loss =  2.0774643 Val_acc =  0.5214\n",
            "Iteration  3 : Loss =  1.9601916  Acc:  0.58145 Val_loss =  1.9631572 Val_acc =  0.5799\n",
            "Iteration  4 : Loss =  1.839286  Acc:  0.59605 Val_loss =  1.8433559 Val_acc =  0.5931\n",
            "Iteration  5 : Loss =  1.7167648  Acc:  0.60103333 Val_loss =  1.7218746 Val_acc =  0.5939\n",
            "Iteration  6 : Loss =  1.5973281  Acc:  0.61071664 Val_loss =  1.6033067 Val_acc =  0.6057\n",
            "Iteration  7 : Loss =  1.4811459  Acc:  0.6287 Val_loss =  1.4880679 Val_acc =  0.6232\n",
            "Iteration  8 : Loss =  1.3700771  Acc:  0.64248335 Val_loss =  1.3779829 Val_acc =  0.6379\n",
            "Iteration  9 : Loss =  1.2672621  Acc:  0.65385 Val_loss =  1.2757792 Val_acc =  0.6467\n",
            "Iteration  10 : Loss =  1.1735888  Acc:  0.6626833 Val_loss =  1.1825244 Val_acc =  0.6524\n",
            "Iteration  11 : Loss =  1.0913929  Acc:  0.669 Val_loss =  1.100859 Val_acc =  0.6608\n",
            "Iteration  12 : Loss =  1.0202241  Acc:  0.6742167 Val_loss =  1.0304123 Val_acc =  0.6646\n",
            "Iteration  13 : Loss =  0.95895606  Acc:  0.6832333 Val_loss =  0.97004133 Val_acc =  0.672\n",
            "Iteration  14 : Loss =  0.908438  Acc:  0.6922167 Val_loss =  0.9206182 Val_acc =  0.6808\n",
            "Iteration  15 : Loss =  0.8655457  Acc:  0.70095 Val_loss =  0.8789771 Val_acc =  0.6881\n",
            "Iteration  16 : Loss =  0.8294892  Acc:  0.7072667 Val_loss =  0.84392774 Val_acc =  0.6956\n",
            "Iteration  17 : Loss =  0.79972523  Acc:  0.71395 Val_loss =  0.8148574 Val_acc =  0.7018\n",
            "Iteration  18 : Loss =  0.7741893  Acc:  0.7212833 Val_loss =  0.7904002 Val_acc =  0.7067\n",
            "Iteration  19 : Loss =  0.7529864  Acc:  0.7295667 Val_loss =  0.7703952 Val_acc =  0.7145\n",
            "Iteration  20 : Loss =  0.73449284  Acc:  0.73503333 Val_loss =  0.75259846 Val_acc =  0.7222\n",
            "Iteration  21 : Loss =  0.7175461  Acc:  0.7414333 Val_loss =  0.7370439 Val_acc =  0.7278\n",
            "Iteration  22 : Loss =  0.7024219  Acc:  0.7463 Val_loss =  0.7230387 Val_acc =  0.7326\n",
            "Iteration  23 : Loss =  0.68862057  Acc:  0.75185 Val_loss =  0.7101908 Val_acc =  0.7384\n",
            "Iteration  24 : Loss =  0.67562926  Acc:  0.76045 Val_loss =  0.69831073 Val_acc =  0.7467\n",
            "Iteration  25 : Loss =  0.66331595  Acc:  0.76486665 Val_loss =  0.6856486 Val_acc =  0.7521\n",
            "Iteration  26 : Loss =  0.6512262  Acc:  0.77208334 Val_loss =  0.6746568 Val_acc =  0.7579\n",
            "Iteration  27 : Loss =  0.6396439  Acc:  0.7759333 Val_loss =  0.6628229 Val_acc =  0.7638\n",
            "Iteration  28 : Loss =  0.62914354  Acc:  0.78178334 Val_loss =  0.6534522 Val_acc =  0.7705\n",
            "Iteration  29 : Loss =  0.6200303  Acc:  0.7856167 Val_loss =  0.6440029 Val_acc =  0.7724\n",
            "Iteration  30 : Loss =  0.61071783  Acc:  0.78998333 Val_loss =  0.63501906 Val_acc =  0.7767\n",
            "Iteration  31 : Loss =  0.599241  Acc:  0.7945 Val_loss =  0.6240196 Val_acc =  0.7814\n",
            "Iteration  32 : Loss =  0.5902149  Acc:  0.79835 Val_loss =  0.6149729 Val_acc =  0.7845\n",
            "Iteration  33 : Loss =  0.58358634  Acc:  0.8006833 Val_loss =  0.6090287 Val_acc =  0.7865\n",
            "Iteration  34 : Loss =  0.57428175  Acc:  0.80256665 Val_loss =  0.59923524 Val_acc =  0.7893\n",
            "Iteration  35 : Loss =  0.5650203  Acc:  0.8056833 Val_loss =  0.5902493 Val_acc =  0.7933\n",
            "Iteration  36 : Loss =  0.55899537  Acc:  0.80826664 Val_loss =  0.5842905 Val_acc =  0.7937\n",
            "Iteration  37 : Loss =  0.5522037  Acc:  0.8099333 Val_loss =  0.57776934 Val_acc =  0.7974\n",
            "Iteration  38 : Loss =  0.5440964  Acc:  0.81383336 Val_loss =  0.56983155 Val_acc =  0.8018\n",
            "Iteration  39 : Loss =  0.5381863  Acc:  0.81625 Val_loss =  0.563988 Val_acc =  0.8034\n",
            "Iteration  40 : Loss =  0.5334477  Acc:  0.81598336 Val_loss =  0.56005603 Val_acc =  0.8035\n",
            "Iteration  41 : Loss =  0.52703685  Acc:  0.82115 Val_loss =  0.553001 Val_acc =  0.8063\n",
            "Iteration  42 : Loss =  0.5217284  Acc:  0.82096666 Val_loss =  0.5490744 Val_acc =  0.8066\n",
            "Iteration  43 : Loss =  0.5192142  Acc:  0.82101667 Val_loss =  0.5455123 Val_acc =  0.8092\n",
            "Iteration  44 : Loss =  0.5155371  Acc:  0.8215333 Val_loss =  0.54409266 Val_acc =  0.8085\n",
            "Iteration  45 : Loss =  0.51123893  Acc:  0.82405 Val_loss =  0.53745764 Val_acc =  0.813\n",
            "Iteration  46 : Loss =  0.5046722  Acc:  0.8257667 Val_loss =  0.5333248 Val_acc =  0.8126\n",
            "Iteration  47 : Loss =  0.4995185  Acc:  0.82891667 Val_loss =  0.5281047 Val_acc =  0.8159\n",
            "Iteration  48 : Loss =  0.4971532  Acc:  0.8291 Val_loss =  0.5245458 Val_acc =  0.8164\n",
            "Iteration  49 : Loss =  0.4937657  Acc:  0.8290167 Val_loss =  0.52341163 Val_acc =  0.8171\n",
            "Iteration  50 : Loss =  0.4875495  Acc:  0.83316666 Val_loss =  0.5161872 Val_acc =  0.8206\n",
            "Iteration  51 : Loss =  0.4844382  Acc:  0.83323336 Val_loss =  0.5129582 Val_acc =  0.8216\n",
            "Iteration  52 : Loss =  0.4828427  Acc:  0.83318335 Val_loss =  0.5130796 Val_acc =  0.821\n",
            "Iteration  53 : Loss =  0.477856  Acc:  0.83538336 Val_loss =  0.506755 Val_acc =  0.8235\n",
            "Iteration  54 : Loss =  0.47429073  Acc:  0.83631665 Val_loss =  0.5038489 Val_acc =  0.8242\n",
            "Iteration  55 : Loss =  0.47264883  Acc:  0.8361833 Val_loss =  0.50324255 Val_acc =  0.8247\n",
            "Iteration  56 : Loss =  0.4691637  Acc:  0.8376667 Val_loss =  0.4986081 Val_acc =  0.8259\n",
            "Iteration  57 : Loss =  0.46609035  Acc:  0.83858335 Val_loss =  0.4966745 Val_acc =  0.8271\n",
            "Iteration  58 : Loss =  0.46350396  Acc:  0.83898336 Val_loss =  0.49452856 Val_acc =  0.827\n",
            "Iteration  59 : Loss =  0.46028537  Acc:  0.83993334 Val_loss =  0.49084878 Val_acc =  0.8275\n",
            "Iteration  60 : Loss =  0.45848614  Acc:  0.84055 Val_loss =  0.48998022 Val_acc =  0.8282\n",
            "Iteration  61 : Loss =  0.45628768  Acc:  0.8408333 Val_loss =  0.48768282 Val_acc =  0.8285\n",
            "Iteration  62 : Loss =  0.45279938  Acc:  0.8423167 Val_loss =  0.4846047 Val_acc =  0.8297\n",
            "Iteration  63 : Loss =  0.45071897  Acc:  0.84288335 Val_loss =  0.48234347 Val_acc =  0.831\n",
            "Iteration  64 : Loss =  0.44891745  Acc:  0.8436 Val_loss =  0.48081118 Val_acc =  0.8299\n",
            "Iteration  65 : Loss =  0.44633436  Acc:  0.84425 Val_loss =  0.47873852 Val_acc =  0.8322\n",
            "Iteration  66 : Loss =  0.44450626  Acc:  0.8451167 Val_loss =  0.47631276 Val_acc =  0.8328\n",
            "Iteration  67 : Loss =  0.44259664  Acc:  0.8454667 Val_loss =  0.47530606 Val_acc =  0.8334\n",
            "Iteration  68 : Loss =  0.43992457  Acc:  0.8465833 Val_loss =  0.47227013 Val_acc =  0.8329\n",
            "Iteration  69 : Loss =  0.43788552  Acc:  0.84718335 Val_loss =  0.4705201 Val_acc =  0.8333\n",
            "Iteration  70 : Loss =  0.4362872  Acc:  0.8476167 Val_loss =  0.4692912 Val_acc =  0.8353\n",
            "Iteration  71 : Loss =  0.43413532  Acc:  0.8484 Val_loss =  0.46695602 Val_acc =  0.8351\n",
            "Iteration  72 : Loss =  0.43227673  Acc:  0.84926665 Val_loss =  0.46568722 Val_acc =  0.8365\n",
            "Iteration  73 : Loss =  0.43089056  Acc:  0.8498833 Val_loss =  0.46390834 Val_acc =  0.8366\n",
            "Iteration  74 : Loss =  0.4291763  Acc:  0.85046667 Val_loss =  0.46296036 Val_acc =  0.8377\n",
            "Iteration  75 : Loss =  0.42755383  Acc:  0.85118335 Val_loss =  0.46075898 Val_acc =  0.8379\n",
            "Iteration  76 : Loss =  0.42620495  Acc:  0.85083336 Val_loss =  0.46047255 Val_acc =  0.838\n",
            "Iteration  77 : Loss =  0.42529726  Acc:  0.85211664 Val_loss =  0.45849237 Val_acc =  0.8387\n",
            "Iteration  78 : Loss =  0.42340127  Acc:  0.85165 Val_loss =  0.45818242 Val_acc =  0.8378\n",
            "Iteration  79 : Loss =  0.4218811  Acc:  0.853 Val_loss =  0.455587 Val_acc =  0.8386\n",
            "Iteration  80 : Loss =  0.41960773  Acc:  0.8533667 Val_loss =  0.4544414 Val_acc =  0.8402\n",
            "Iteration  81 : Loss =  0.4171923  Acc:  0.85501665 Val_loss =  0.45159003 Val_acc =  0.8405\n",
            "Iteration  82 : Loss =  0.41488415  Acc:  0.8558667 Val_loss =  0.44983476 Val_acc =  0.842\n",
            "Iteration  83 : Loss =  0.41335887  Acc:  0.85646665 Val_loss =  0.44836992 Val_acc =  0.8424\n",
            "Iteration  84 : Loss =  0.4122543  Acc:  0.8567 Val_loss =  0.44733903 Val_acc =  0.8423\n",
            "Iteration  85 : Loss =  0.41111836  Acc:  0.8566833 Val_loss =  0.44695732 Val_acc =  0.8435\n",
            "Iteration  86 : Loss =  0.4101665  Acc:  0.85756665 Val_loss =  0.44539195 Val_acc =  0.8432\n",
            "Iteration  87 : Loss =  0.40891695  Acc:  0.85751665 Val_loss =  0.4452595 Val_acc =  0.843\n",
            "Iteration  88 : Loss =  0.4077228  Acc:  0.85873336 Val_loss =  0.4433332 Val_acc =  0.8441\n",
            "Iteration  89 : Loss =  0.40556672  Acc:  0.85875 Val_loss =  0.4421422 Val_acc =  0.8443\n",
            "Iteration  90 : Loss =  0.40364942  Acc:  0.85995 Val_loss =  0.43972406 Val_acc =  0.8456\n",
            "Iteration  91 : Loss =  0.4018273  Acc:  0.86036664 Val_loss =  0.43854004 Val_acc =  0.8473\n",
            "Iteration  92 : Loss =  0.40018976  Acc:  0.86095 Val_loss =  0.43692324 Val_acc =  0.847\n",
            "Iteration  93 : Loss =  0.39870527  Acc:  0.86155 Val_loss =  0.43560758 Val_acc =  0.8477\n",
            "Iteration  94 : Loss =  0.39749002  Acc:  0.86221665 Val_loss =  0.43483955 Val_acc =  0.8479\n",
            "Iteration  95 : Loss =  0.39655402  Acc:  0.8621333 Val_loss =  0.4337539 Val_acc =  0.8478\n",
            "Iteration  96 : Loss =  0.3957171  Acc:  0.8624667 Val_loss =  0.4335787 Val_acc =  0.8481\n",
            "Iteration  97 : Loss =  0.39526674  Acc:  0.8627167 Val_loss =  0.4327818 Val_acc =  0.8484\n",
            "Iteration  98 : Loss =  0.39459103  Acc:  0.8621 Val_loss =  0.4330406 Val_acc =  0.8472\n",
            "Iteration  99 : Loss =  0.39488977  Acc:  0.8627333 Val_loss =  0.43260312 Val_acc =  0.8487\n",
            "Iteration  100 : Loss =  0.3929478  Acc:  0.8626 Val_loss =  0.43183404 Val_acc =  0.8478\n",
            "Iteration  101 : Loss =  0.39115345  Acc:  0.86395 Val_loss =  0.42931357 Val_acc =  0.8499\n",
            "Iteration  102 : Loss =  0.38779473  Acc:  0.86501664 Val_loss =  0.42665976 Val_acc =  0.8502\n",
            "Iteration  103 : Loss =  0.38561645  Acc:  0.8665 Val_loss =  0.42443627 Val_acc =  0.8519\n",
            "Iteration  104 : Loss =  0.38474363  Acc:  0.8667833 Val_loss =  0.4236802 Val_acc =  0.8529\n",
            "Iteration  105 : Loss =  0.38448405  Acc:  0.86625 Val_loss =  0.4240338 Val_acc =  0.8506\n",
            "Iteration  106 : Loss =  0.38438052  Acc:  0.8668 Val_loss =  0.4236617 Val_acc =  0.8527\n",
            "Iteration  107 : Loss =  0.38291347  Acc:  0.86681664 Val_loss =  0.4230092 Val_acc =  0.8515\n",
            "Iteration  108 : Loss =  0.38141775  Acc:  0.86795 Val_loss =  0.42109415 Val_acc =  0.8524\n",
            "Iteration  109 : Loss =  0.37934133  Acc:  0.86793333 Val_loss =  0.419592 Val_acc =  0.8534\n",
            "Iteration  110 : Loss =  0.37758645  Acc:  0.86911666 Val_loss =  0.41780195 Val_acc =  0.8538\n",
            "Iteration  111 : Loss =  0.37631056  Acc:  0.87008333 Val_loss =  0.4166535 Val_acc =  0.8554\n",
            "Iteration  112 : Loss =  0.37549505  Acc:  0.86938334 Val_loss =  0.41625646 Val_acc =  0.8544\n",
            "Iteration  113 : Loss =  0.3750005  Acc:  0.87013334 Val_loss =  0.4157015 Val_acc =  0.855\n",
            "Iteration  114 : Loss =  0.3742015  Acc:  0.86976665 Val_loss =  0.41547137 Val_acc =  0.8543\n",
            "Iteration  115 : Loss =  0.3732241  Acc:  0.8707833 Val_loss =  0.4143749 Val_acc =  0.8543\n",
            "Iteration  116 : Loss =  0.37161016  Acc:  0.87083334 Val_loss =  0.41332528 Val_acc =  0.8562\n",
            "Iteration  117 : Loss =  0.36991832  Acc:  0.8721333 Val_loss =  0.41152403 Val_acc =  0.8567\n",
            "Iteration  118 : Loss =  0.36831957  Acc:  0.8725 Val_loss =  0.41034102 Val_acc =  0.8568\n",
            "Iteration  119 : Loss =  0.36707935  Acc:  0.87305 Val_loss =  0.40925395 Val_acc =  0.857\n",
            "Iteration  120 : Loss =  0.36616817  Acc:  0.87365 Val_loss =  0.40847072 Val_acc =  0.8578\n",
            "Iteration  121 : Loss =  0.3654303  Acc:  0.87343335 Val_loss =  0.4081518 Val_acc =  0.858\n",
            "Iteration  122 : Loss =  0.36476824  Acc:  0.8734 Val_loss =  0.40747866 Val_acc =  0.859\n",
            "Iteration  123 : Loss =  0.3640045  Acc:  0.87395 Val_loss =  0.4072219 Val_acc =  0.8583\n",
            "Iteration  124 : Loss =  0.36328802  Acc:  0.8739333 Val_loss =  0.40646836 Val_acc =  0.859\n",
            "Iteration  125 : Loss =  0.36227313  Acc:  0.87441665 Val_loss =  0.40600538 Val_acc =  0.859\n",
            "Iteration  126 : Loss =  0.36147213  Acc:  0.87476665 Val_loss =  0.4051359 Val_acc =  0.8591\n",
            "Iteration  127 : Loss =  0.36031985  Acc:  0.87525 Val_loss =  0.4045224 Val_acc =  0.8589\n",
            "Iteration  128 : Loss =  0.35978016  Acc:  0.87546664 Val_loss =  0.4040029 Val_acc =  0.859\n",
            "Iteration  129 : Loss =  0.35905227  Acc:  0.8752 Val_loss =  0.4036205 Val_acc =  0.859\n",
            "Iteration  130 : Loss =  0.35904285  Acc:  0.87553334 Val_loss =  0.40393522 Val_acc =  0.8601\n",
            "Iteration  131 : Loss =  0.3587702  Acc:  0.8749667 Val_loss =  0.40368333 Val_acc =  0.8592\n",
            "Iteration  132 : Loss =  0.35736027  Acc:  0.8760333 Val_loss =  0.40283966 Val_acc =  0.8603\n",
            "Iteration  133 : Loss =  0.35518  Acc:  0.8764 Val_loss =  0.40055484 Val_acc =  0.8608\n",
            "Iteration  134 : Loss =  0.35249865  Acc:  0.87745 Val_loss =  0.39831436 Val_acc =  0.8618\n",
            "Iteration  135 : Loss =  0.3512222  Acc:  0.87838334 Val_loss =  0.39717457 Val_acc =  0.8629\n",
            "Iteration  136 : Loss =  0.35129592  Acc:  0.87796664 Val_loss =  0.39748207 Val_acc =  0.8614\n",
            "Iteration  137 : Loss =  0.3518216  Acc:  0.87796664 Val_loss =  0.39834628 Val_acc =  0.8621\n",
            "Iteration  138 : Loss =  0.35077077  Acc:  0.87743336 Val_loss =  0.39743453 Val_acc =  0.8617\n",
            "Iteration  139 : Loss =  0.34885252  Acc:  0.8789 Val_loss =  0.3957849 Val_acc =  0.8632\n",
            "Iteration  140 : Loss =  0.34665293  Acc:  0.8794 Val_loss =  0.39378363 Val_acc =  0.8627\n",
            "Iteration  141 : Loss =  0.34564257  Acc:  0.88013333 Val_loss =  0.39294446 Val_acc =  0.8639\n",
            "Iteration  142 : Loss =  0.34549722  Acc:  0.8797333 Val_loss =  0.39320624 Val_acc =  0.865\n",
            "Iteration  143 : Loss =  0.345375  Acc:  0.87946665 Val_loss =  0.39306587 Val_acc =  0.8626\n",
            "Iteration  144 : Loss =  0.344222  Acc:  0.88026667 Val_loss =  0.39249048 Val_acc =  0.8647\n",
            "Iteration  145 : Loss =  0.34259626  Acc:  0.88081664 Val_loss =  0.39072528 Val_acc =  0.8636\n",
            "Iteration  146 : Loss =  0.3410407  Acc:  0.88163334 Val_loss =  0.38964662 Val_acc =  0.8648\n",
            "Iteration  147 : Loss =  0.34021986  Acc:  0.88193333 Val_loss =  0.38900027 Val_acc =  0.8656\n",
            "Iteration  148 : Loss =  0.3399118  Acc:  0.8817667 Val_loss =  0.38881186 Val_acc =  0.8641\n",
            "Iteration  149 : Loss =  0.33962116  Acc:  0.88203335 Val_loss =  0.38905156 Val_acc =  0.8658\n",
            "Iteration  150 : Loss =  0.3388385  Acc:  0.8821167 Val_loss =  0.38823768 Val_acc =  0.8645\n",
            "Iteration  151 : Loss =  0.33789578  Acc:  0.88261664 Val_loss =  0.3877884 Val_acc =  0.8657\n",
            "Iteration  152 : Loss =  0.3368874  Acc:  0.88245 Val_loss =  0.3869545 Val_acc =  0.8649\n",
            "Iteration  153 : Loss =  0.3368287  Acc:  0.8825333 Val_loss =  0.3870823 Val_acc =  0.8651\n",
            "Iteration  154 : Loss =  0.33789498  Acc:  0.88135 Val_loss =  0.3887286 Val_acc =  0.864\n",
            "Iteration  155 : Loss =  0.3408731  Acc:  0.88021666 Val_loss =  0.39164954 Val_acc =  0.8628\n",
            "Iteration  156 : Loss =  0.34327772  Acc:  0.8782833 Val_loss =  0.394929 Val_acc =  0.8604\n",
            "Iteration  157 : Loss =  0.3425687  Acc:  0.87975 Val_loss =  0.3939537 Val_acc =  0.8613\n",
            "Iteration  158 : Loss =  0.33540064  Acc:  0.88185 Val_loss =  0.3870974 Val_acc =  0.8655\n",
            "Iteration  159 : Loss =  0.3318218  Acc:  0.8842 Val_loss =  0.38360542 Val_acc =  0.8671\n",
            "Iteration  160 : Loss =  0.3334125  Acc:  0.88255 Val_loss =  0.38514644 Val_acc =  0.865\n",
            "Iteration  161 : Loss =  0.33365053  Acc:  0.88271666 Val_loss =  0.38630682 Val_acc =  0.8654\n",
            "Iteration  162 : Loss =  0.3305529  Acc:  0.88423336 Val_loss =  0.38295314 Val_acc =  0.8668\n",
            "Iteration  163 : Loss =  0.3288252  Acc:  0.88488334 Val_loss =  0.3815615 Val_acc =  0.8664\n",
            "Iteration  164 : Loss =  0.33019343  Acc:  0.88498336 Val_loss =  0.38359317 Val_acc =  0.8665\n",
            "Iteration  165 : Loss =  0.32992566  Acc:  0.88371664 Val_loss =  0.38290438 Val_acc =  0.865\n",
            "Iteration  166 : Loss =  0.3260088  Acc:  0.88545 Val_loss =  0.3798275 Val_acc =  0.8696\n",
            "Iteration  167 : Loss =  0.3249668  Acc:  0.88628334 Val_loss =  0.37871826 Val_acc =  0.8697\n",
            "Iteration  168 : Loss =  0.32654795  Acc:  0.8851167 Val_loss =  0.38028258 Val_acc =  0.8659\n",
            "Iteration  169 : Loss =  0.32556385  Acc:  0.88603336 Val_loss =  0.38021585 Val_acc =  0.8679\n",
            "Iteration  170 : Loss =  0.32312968  Acc:  0.88701665 Val_loss =  0.3772822 Val_acc =  0.869\n",
            "Iteration  171 : Loss =  0.32246324  Acc:  0.8871 Val_loss =  0.3772192 Val_acc =  0.8683\n",
            "Iteration  172 : Loss =  0.3227475  Acc:  0.88655 Val_loss =  0.37790668 Val_acc =  0.8692\n",
            "Iteration  173 : Loss =  0.321572  Acc:  0.88748336 Val_loss =  0.37645766 Val_acc =  0.8696\n",
            "Iteration  174 : Loss =  0.3197524  Acc:  0.88743335 Val_loss =  0.37535146 Val_acc =  0.8696\n",
            "Iteration  175 : Loss =  0.31955853  Acc:  0.88766664 Val_loss =  0.3751981 Val_acc =  0.8695\n",
            "Iteration  176 : Loss =  0.319753  Acc:  0.8875333 Val_loss =  0.37554356 Val_acc =  0.8694\n",
            "Iteration  177 : Loss =  0.31863162  Acc:  0.88725 Val_loss =  0.37500963 Val_acc =  0.8688\n",
            "Iteration  178 : Loss =  0.31717116  Acc:  0.88886666 Val_loss =  0.37348762 Val_acc =  0.8691\n",
            "Iteration  179 : Loss =  0.31668136  Acc:  0.88858336 Val_loss =  0.37327746 Val_acc =  0.8681\n",
            "Iteration  180 : Loss =  0.3164365  Acc:  0.8885667 Val_loss =  0.37361142 Val_acc =  0.8712\n",
            "Iteration  181 : Loss =  0.3155679  Acc:  0.88911664 Val_loss =  0.3724935 Val_acc =  0.8699\n",
            "Iteration  182 : Loss =  0.31429005  Acc:  0.8890167 Val_loss =  0.37192744 Val_acc =  0.8707\n",
            "Iteration  183 : Loss =  0.31360912  Acc:  0.88946664 Val_loss =  0.37120593 Val_acc =  0.8702\n",
            "Iteration  184 : Loss =  0.3132605  Acc:  0.8898 Val_loss =  0.37112954 Val_acc =  0.8695\n",
            "Iteration  185 : Loss =  0.31257552  Acc:  0.8893333 Val_loss =  0.37094715 Val_acc =  0.8706\n",
            "Iteration  186 : Loss =  0.3115875  Acc:  0.89058334 Val_loss =  0.36982346 Val_acc =  0.8694\n",
            "Iteration  187 : Loss =  0.3107771  Acc:  0.89025 Val_loss =  0.3696023 Val_acc =  0.8695\n",
            "Iteration  188 : Loss =  0.31023887  Acc:  0.8904667 Val_loss =  0.36924776 Val_acc =  0.8708\n",
            "Iteration  189 : Loss =  0.3096963  Acc:  0.89075 Val_loss =  0.36885694 Val_acc =  0.8704\n",
            "Iteration  190 : Loss =  0.30891895  Acc:  0.89065 Val_loss =  0.36860913 Val_acc =  0.8714\n",
            "Iteration  191 : Loss =  0.30809644  Acc:  0.8915667 Val_loss =  0.3677212 Val_acc =  0.8704\n",
            "Iteration  192 : Loss =  0.30737263  Acc:  0.89103335 Val_loss =  0.3675475 Val_acc =  0.8708\n",
            "Iteration  193 : Loss =  0.30675387  Acc:  0.8915667 Val_loss =  0.36698636 Val_acc =  0.8705\n",
            "Iteration  194 : Loss =  0.3061127  Acc:  0.89173335 Val_loss =  0.36663216 Val_acc =  0.8718\n",
            "Iteration  195 : Loss =  0.30541405  Acc:  0.89173335 Val_loss =  0.3663282 Val_acc =  0.8719\n",
            "Iteration  196 : Loss =  0.30472326  Acc:  0.89275 Val_loss =  0.36569962 Val_acc =  0.8706\n",
            "Iteration  197 : Loss =  0.3041445  Acc:  0.89203334 Val_loss =  0.36558682 Val_acc =  0.8716\n",
            "Iteration  198 : Loss =  0.30381805  Acc:  0.89315 Val_loss =  0.36544293 Val_acc =  0.8724\n",
            "Iteration  199 : Loss =  0.30420455  Acc:  0.8918333 Val_loss =  0.36600536 Val_acc =  0.8702\n",
            "Iteration  200 : Loss =  0.30739948  Acc:  0.89131665 Val_loss =  0.37006605 Val_acc =  0.8705\n",
            "Iteration  201 : Loss =  0.3145911  Acc:  0.8876333 Val_loss =  0.37653148 Val_acc =  0.8636\n",
            "Iteration  202 : Loss =  0.33791772  Acc:  0.8811833 Val_loss =  0.40255606 Val_acc =  0.8618\n",
            "Iteration  203 : Loss =  0.3150505  Acc:  0.8865 Val_loss =  0.3770613 Val_acc =  0.8626\n",
            "Iteration  204 : Loss =  0.30167845  Acc:  0.8930333 Val_loss =  0.36457387 Val_acc =  0.8722\n",
            "Iteration  205 : Loss =  0.30237988  Acc:  0.89318335 Val_loss =  0.36597496 Val_acc =  0.8727\n",
            "Iteration  206 : Loss =  0.30963686  Acc:  0.8891 Val_loss =  0.37222013 Val_acc =  0.8659\n",
            "Iteration  207 : Loss =  0.31096193  Acc:  0.88991666 Val_loss =  0.37530532 Val_acc =  0.8696\n",
            "Iteration  208 : Loss =  0.29929674  Acc:  0.8929667 Val_loss =  0.3634643 Val_acc =  0.8724\n",
            "Iteration  209 : Loss =  0.30597466  Acc:  0.8911833 Val_loss =  0.36935636 Val_acc =  0.8693\n",
            "Iteration  210 : Loss =  0.31146803  Acc:  0.8903 Val_loss =  0.37732872 Val_acc =  0.8711\n",
            "Iteration  211 : Loss =  0.2981655  Acc:  0.89335 Val_loss =  0.36299863 Val_acc =  0.8717\n",
            "Iteration  212 : Loss =  0.30352595  Acc:  0.89238334 Val_loss =  0.3672947 Val_acc =  0.8697\n",
            "Iteration  213 : Loss =  0.30761555  Acc:  0.89093333 Val_loss =  0.37420753 Val_acc =  0.8713\n",
            "Iteration  214 : Loss =  0.29586607  Acc:  0.89446664 Val_loss =  0.36134872 Val_acc =  0.8731\n",
            "Iteration  215 : Loss =  0.3019078  Acc:  0.89248335 Val_loss =  0.36619663 Val_acc =  0.87\n",
            "Iteration  216 : Loss =  0.30291584  Acc:  0.89255 Val_loss =  0.36995447 Val_acc =  0.8718\n",
            "Iteration  217 : Loss =  0.29413834  Acc:  0.89533335 Val_loss =  0.36065903 Val_acc =  0.8727\n",
            "Iteration  218 : Loss =  0.30087963  Acc:  0.89271665 Val_loss =  0.3660223 Val_acc =  0.8699\n",
            "Iteration  219 : Loss =  0.29890382  Acc:  0.89365 Val_loss =  0.36627474 Val_acc =  0.8726\n",
            "Iteration  220 : Loss =  0.2941082  Acc:  0.89515 Val_loss =  0.36156568 Val_acc =  0.8709\n",
            "Iteration  221 : Loss =  0.29975706  Acc:  0.89316666 Val_loss =  0.36549297 Val_acc =  0.8701\n",
            "Iteration  222 : Loss =  0.29483047  Acc:  0.89491665 Val_loss =  0.36241 Val_acc =  0.8742\n",
            "Iteration  223 : Loss =  0.29490146  Acc:  0.89491665 Val_loss =  0.36328226 Val_acc =  0.8706\n",
            "Iteration  224 : Loss =  0.29725704  Acc:  0.89381665 Val_loss =  0.3637218 Val_acc =  0.8704\n",
            "Iteration  225 : Loss =  0.29158136  Acc:  0.8961833 Val_loss =  0.35937706 Val_acc =  0.876\n",
            "Iteration  226 : Loss =  0.29442447  Acc:  0.8947667 Val_loss =  0.36376303 Val_acc =  0.8691\n",
            "Iteration  227 : Loss =  0.29400572  Acc:  0.89485 Val_loss =  0.36159346 Val_acc =  0.8705\n",
            "Iteration  228 : Loss =  0.29124075  Acc:  0.8965 Val_loss =  0.35930604 Val_acc =  0.8765\n",
            "Iteration  229 : Loss =  0.29096466  Acc:  0.8962333 Val_loss =  0.36088312 Val_acc =  0.8726\n",
            "Iteration  230 : Loss =  0.29091373  Acc:  0.8957667 Val_loss =  0.3597737 Val_acc =  0.8713\n",
            "Iteration  231 : Loss =  0.29041433  Acc:  0.89668334 Val_loss =  0.3589254 Val_acc =  0.8749\n",
            "Iteration  232 : Loss =  0.28735137  Acc:  0.8980333 Val_loss =  0.3572319 Val_acc =  0.8754\n",
            "Iteration  233 : Loss =  0.2891334  Acc:  0.8965667 Val_loss =  0.35898617 Val_acc =  0.8716\n",
            "Iteration  234 : Loss =  0.28675544  Acc:  0.8986167 Val_loss =  0.35604241 Val_acc =  0.875\n",
            "Iteration  235 : Loss =  0.28703913  Acc:  0.89786667 Val_loss =  0.35695675 Val_acc =  0.8758\n",
            "Iteration  236 : Loss =  0.2862765  Acc:  0.89706665 Val_loss =  0.35673493 Val_acc =  0.8729\n",
            "Iteration  237 : Loss =  0.28367725  Acc:  0.8991333 Val_loss =  0.35391584 Val_acc =  0.8752\n",
            "Iteration  238 : Loss =  0.286632  Acc:  0.8976833 Val_loss =  0.35696274 Val_acc =  0.8758\n",
            "Iteration  239 : Loss =  0.28305453  Acc:  0.89893335 Val_loss =  0.35398132 Val_acc =  0.8754\n",
            "Iteration  240 : Loss =  0.2831075  Acc:  0.89855 Val_loss =  0.35430253 Val_acc =  0.8744\n",
            "Iteration  241 : Loss =  0.2833995  Acc:  0.89885 Val_loss =  0.35453317 Val_acc =  0.876\n",
            "Iteration  242 : Loss =  0.28151184  Acc:  0.8997667 Val_loss =  0.35278377 Val_acc =  0.8764\n",
            "Iteration  243 : Loss =  0.28223953  Acc:  0.8987833 Val_loss =  0.35409984 Val_acc =  0.8739\n",
            "Iteration  244 : Loss =  0.28048655  Acc:  0.90031666 Val_loss =  0.35255367 Val_acc =  0.8767\n",
            "Iteration  245 : Loss =  0.2805437  Acc:  0.9001 Val_loss =  0.35226095 Val_acc =  0.8756\n",
            "Iteration  246 : Loss =  0.2799681  Acc:  0.89995 Val_loss =  0.35230324 Val_acc =  0.8755\n",
            "Iteration  247 : Loss =  0.27951008  Acc:  0.9004667 Val_loss =  0.35253143 Val_acc =  0.8767\n",
            "Iteration  248 : Loss =  0.27889928  Acc:  0.9005833 Val_loss =  0.35133734 Val_acc =  0.8762\n",
            "Iteration  249 : Loss =  0.27797765  Acc:  0.90063334 Val_loss =  0.3508405 Val_acc =  0.8761\n",
            "Iteration  250 : Loss =  0.2783164  Acc:  0.9004167 Val_loss =  0.35213822 Val_acc =  0.8769\n",
            "Iteration  251 : Loss =  0.27714902  Acc:  0.9012333 Val_loss =  0.35045537 Val_acc =  0.8756\n",
            "Iteration  252 : Loss =  0.27694747  Acc:  0.90155 Val_loss =  0.35035917 Val_acc =  0.8761\n",
            "Iteration  253 : Loss =  0.2765985  Acc:  0.90105 Val_loss =  0.3509614 Val_acc =  0.8779\n",
            "Iteration  254 : Loss =  0.27561325  Acc:  0.90133333 Val_loss =  0.34976688 Val_acc =  0.877\n",
            "Iteration  255 : Loss =  0.27572215  Acc:  0.9018 Val_loss =  0.3497557 Val_acc =  0.8766\n",
            "Iteration  256 : Loss =  0.27507642  Acc:  0.9019833 Val_loss =  0.3499164 Val_acc =  0.878\n",
            "Iteration  257 : Loss =  0.2745408  Acc:  0.90173334 Val_loss =  0.34952763 Val_acc =  0.8771\n",
            "Iteration  258 : Loss =  0.27411523  Acc:  0.9023167 Val_loss =  0.34891215 Val_acc =  0.8769\n",
            "Iteration  259 : Loss =  0.27354497  Acc:  0.9022667 Val_loss =  0.34892887 Val_acc =  0.8771\n",
            "Iteration  260 : Loss =  0.27330706  Acc:  0.90208334 Val_loss =  0.34898174 Val_acc =  0.8774\n",
            "Iteration  261 : Loss =  0.27267447  Acc:  0.90255 Val_loss =  0.34827787 Val_acc =  0.8773\n",
            "Iteration  262 : Loss =  0.27235603  Acc:  0.90278333 Val_loss =  0.3483627 Val_acc =  0.8771\n",
            "Iteration  263 : Loss =  0.27192593  Acc:  0.90275 Val_loss =  0.34820145 Val_acc =  0.8769\n",
            "Iteration  264 : Loss =  0.27123407  Acc:  0.9029667 Val_loss =  0.34767592 Val_acc =  0.8772\n",
            "Iteration  265 : Loss =  0.27100202  Acc:  0.90328336 Val_loss =  0.3476181 Val_acc =  0.8773\n",
            "Iteration  266 : Loss =  0.27054137  Acc:  0.90321666 Val_loss =  0.3474579 Val_acc =  0.8778\n",
            "Iteration  267 : Loss =  0.27002934  Acc:  0.9035 Val_loss =  0.34731385 Val_acc =  0.878\n",
            "Iteration  268 : Loss =  0.26970577  Acc:  0.9037 Val_loss =  0.34699607 Val_acc =  0.878\n",
            "Iteration  269 : Loss =  0.2691948  Acc:  0.9036833 Val_loss =  0.34682977 Val_acc =  0.878\n",
            "Iteration  270 : Loss =  0.26876143  Acc:  0.90391666 Val_loss =  0.34678486 Val_acc =  0.8786\n",
            "Iteration  271 : Loss =  0.26831624  Acc:  0.9041 Val_loss =  0.3463552 Val_acc =  0.8776\n",
            "Iteration  272 : Loss =  0.26782408  Acc:  0.90426666 Val_loss =  0.34615806 Val_acc =  0.8778\n",
            "Iteration  273 : Loss =  0.2674749  Acc:  0.90428334 Val_loss =  0.34624848 Val_acc =  0.8789\n",
            "Iteration  274 : Loss =  0.2670185  Acc:  0.90451664 Val_loss =  0.3457748 Val_acc =  0.8779\n",
            "Iteration  275 : Loss =  0.26654145  Acc:  0.90461665 Val_loss =  0.34563148 Val_acc =  0.8778\n",
            "Iteration  276 : Loss =  0.2661931  Acc:  0.9047167 Val_loss =  0.3457441 Val_acc =  0.8791\n",
            "Iteration  277 : Loss =  0.2657628  Acc:  0.90498334 Val_loss =  0.3452794 Val_acc =  0.8778\n",
            "Iteration  278 : Loss =  0.26529992  Acc:  0.90505 Val_loss =  0.3452738 Val_acc =  0.878\n",
            "Iteration  279 : Loss =  0.26491895  Acc:  0.90506667 Val_loss =  0.3451753 Val_acc =  0.8786\n",
            "Iteration  280 : Loss =  0.2645021  Acc:  0.9054 Val_loss =  0.34490603 Val_acc =  0.8783\n",
            "Iteration  281 : Loss =  0.26409808  Acc:  0.90536666 Val_loss =  0.34478348 Val_acc =  0.8786\n",
            "Iteration  282 : Loss =  0.26374146  Acc:  0.90565 Val_loss =  0.34477904 Val_acc =  0.8786\n",
            "Iteration  283 : Loss =  0.26339185  Acc:  0.90561664 Val_loss =  0.3445007 Val_acc =  0.8792\n",
            "Iteration  284 : Loss =  0.26312196  Acc:  0.90613335 Val_loss =  0.3447292 Val_acc =  0.8784\n",
            "Iteration  285 : Loss =  0.2630261  Acc:  0.90585 Val_loss =  0.34471503 Val_acc =  0.8786\n",
            "Iteration  286 : Loss =  0.26309484  Acc:  0.9061 Val_loss =  0.34526998 Val_acc =  0.8773\n",
            "Iteration  287 : Loss =  0.2635748  Acc:  0.90501666 Val_loss =  0.34572425 Val_acc =  0.8775\n",
            "Iteration  288 : Loss =  0.2645788  Acc:  0.9052333 Val_loss =  0.3474361 Val_acc =  0.876\n",
            "Iteration  289 : Loss =  0.26640537  Acc:  0.9037 Val_loss =  0.34913385 Val_acc =  0.8761\n",
            "Iteration  290 : Loss =  0.2681263  Acc:  0.90335 Val_loss =  0.35168248 Val_acc =  0.8734\n",
            "Iteration  291 : Loss =  0.2692391  Acc:  0.9027333 Val_loss =  0.35268363 Val_acc =  0.8751\n",
            "Iteration  292 : Loss =  0.26665124  Acc:  0.9041 Val_loss =  0.35061994 Val_acc =  0.8739\n",
            "Iteration  293 : Loss =  0.26224282  Acc:  0.9055 Val_loss =  0.3460148 Val_acc =  0.8783\n",
            "Iteration  294 : Loss =  0.25887284  Acc:  0.9073667 Val_loss =  0.3429982 Val_acc =  0.8795\n",
            "Iteration  295 : Loss =  0.2591469  Acc:  0.90775 Val_loss =  0.34362286 Val_acc =  0.8783\n",
            "Iteration  296 : Loss =  0.26135483  Acc:  0.9058167 Val_loss =  0.34596232 Val_acc =  0.878\n",
            "Iteration  297 : Loss =  0.26190272  Acc:  0.9063333 Val_loss =  0.34710294 Val_acc =  0.8759\n",
            "Iteration  298 : Loss =  0.25998905  Acc:  0.90631664 Val_loss =  0.34516948 Val_acc =  0.8784\n",
            "Iteration  299 : Loss =  0.25725186  Acc:  0.90858334 Val_loss =  0.3426427 Val_acc =  0.8789\n",
            "Iteration  300 : Loss =  0.25642985  Acc:  0.90833336 Val_loss =  0.3422043 Val_acc =  0.8802\n",
            "Iteration  301 : Loss =  0.25740832  Acc:  0.9076667 Val_loss =  0.34322992 Val_acc =  0.8796\n",
            "Iteration  302 : Loss =  0.25818318  Acc:  0.908 Val_loss =  0.34452736 Val_acc =  0.8776\n",
            "Iteration  303 : Loss =  0.25756198  Acc:  0.90725 Val_loss =  0.3440563 Val_acc =  0.8785\n",
            "Iteration  304 : Loss =  0.25571904  Acc:  0.90921664 Val_loss =  0.34237686 Val_acc =  0.8779\n",
            "Iteration  305 : Loss =  0.2543842  Acc:  0.9088167 Val_loss =  0.3413921 Val_acc =  0.8807\n",
            "Iteration  306 : Loss =  0.25419727  Acc:  0.90898335 Val_loss =  0.34129795 Val_acc =  0.8806\n",
            "Iteration  307 : Loss =  0.25467286  Acc:  0.90928334 Val_loss =  0.34219342 Val_acc =  0.8776\n",
            "Iteration  308 : Loss =  0.2548856  Acc:  0.9081 Val_loss =  0.34263918 Val_acc =  0.8798\n",
            "Iteration  309 : Loss =  0.25418824  Acc:  0.9094667 Val_loss =  0.34217507 Val_acc =  0.8779\n",
            "Iteration  310 : Loss =  0.25306877  Acc:  0.909 Val_loss =  0.34136495 Val_acc =  0.881\n",
            "Iteration  311 : Loss =  0.25204965  Acc:  0.90995 Val_loss =  0.34051922 Val_acc =  0.8806\n",
            "Iteration  312 : Loss =  0.25162584  Acc:  0.9102 Val_loss =  0.34048536 Val_acc =  0.8806\n",
            "Iteration  313 : Loss =  0.25166103  Acc:  0.9099333 Val_loss =  0.34074512 Val_acc =  0.8806\n",
            "Iteration  314 : Loss =  0.2516937  Acc:  0.9104 Val_loss =  0.34110612 Val_acc =  0.8785\n",
            "Iteration  315 : Loss =  0.2514397  Acc:  0.9098833 Val_loss =  0.34113765 Val_acc =  0.8807\n",
            "Iteration  316 : Loss =  0.25076193  Acc:  0.9106333 Val_loss =  0.34066227 Val_acc =  0.8786\n",
            "Iteration  317 : Loss =  0.24997692  Acc:  0.91048336 Val_loss =  0.34026456 Val_acc =  0.8811\n",
            "Iteration  318 : Loss =  0.24929094  Acc:  0.9109167 Val_loss =  0.33969957 Val_acc =  0.8806\n",
            "Iteration  319 : Loss =  0.24885714  Acc:  0.9112333 Val_loss =  0.3396396 Val_acc =  0.8804\n",
            "Iteration  320 : Loss =  0.24863029  Acc:  0.9113167 Val_loss =  0.33957145 Val_acc =  0.8805\n",
            "Iteration  321 : Loss =  0.2484653  Acc:  0.91153336 Val_loss =  0.33972684 Val_acc =  0.8801\n",
            "Iteration  322 : Loss =  0.24824879  Acc:  0.91111666 Val_loss =  0.33983126 Val_acc =  0.8806\n",
            "Iteration  323 : Loss =  0.24788705  Acc:  0.9116333 Val_loss =  0.33969444 Val_acc =  0.8804\n",
            "Iteration  324 : Loss =  0.24743487  Acc:  0.9113333 Val_loss =  0.33960575 Val_acc =  0.8811\n",
            "Iteration  325 : Loss =  0.24689078  Acc:  0.91216666 Val_loss =  0.33920336 Val_acc =  0.8806\n",
            "Iteration  326 : Loss =  0.24635667  Acc:  0.91175 Val_loss =  0.3390591 Val_acc =  0.8811\n",
            "Iteration  327 : Loss =  0.24585085  Acc:  0.9120833 Val_loss =  0.33870378 Val_acc =  0.8808\n",
            "Iteration  328 : Loss =  0.24540545  Acc:  0.9120833 Val_loss =  0.33864188 Val_acc =  0.8811\n",
            "Iteration  329 : Loss =  0.24501629  Acc:  0.9123333 Val_loss =  0.3384218 Val_acc =  0.8808\n",
            "Iteration  330 : Loss =  0.24466956  Acc:  0.91286665 Val_loss =  0.33845156 Val_acc =  0.8811\n",
            "Iteration  331 : Loss =  0.24435538  Acc:  0.9124 Val_loss =  0.3383422 Val_acc =  0.8814\n",
            "Iteration  332 : Loss =  0.24406387  Acc:  0.91315 Val_loss =  0.33841097 Val_acc =  0.8816\n",
            "Iteration  333 : Loss =  0.24380709  Acc:  0.9127833 Val_loss =  0.3383754 Val_acc =  0.8811\n",
            "Iteration  334 : Loss =  0.2435863  Acc:  0.91328335 Val_loss =  0.33850133 Val_acc =  0.8805\n",
            "Iteration  335 : Loss =  0.24344824  Acc:  0.91248333 Val_loss =  0.3386421 Val_acc =  0.8816\n",
            "Iteration  336 : Loss =  0.24339797  Acc:  0.9135 Val_loss =  0.3389006 Val_acc =  0.8794\n",
            "Iteration  337 : Loss =  0.24354777  Acc:  0.9127667 Val_loss =  0.3393291 Val_acc =  0.8813\n",
            "Iteration  338 : Loss =  0.24383312  Acc:  0.9131333 Val_loss =  0.3398938 Val_acc =  0.8786\n",
            "Iteration  339 : Loss =  0.24451071  Acc:  0.91215 Val_loss =  0.34093088 Val_acc =  0.881\n",
            "Iteration  340 : Loss =  0.24518976  Acc:  0.91228336 Val_loss =  0.34186864 Val_acc =  0.8773\n",
            "Iteration  341 : Loss =  0.2462848  Acc:  0.91108334 Val_loss =  0.34340996 Val_acc =  0.8791\n",
            "Iteration  342 : Loss =  0.24630098  Acc:  0.9116667 Val_loss =  0.34352806 Val_acc =  0.8776\n",
            "Iteration  343 : Loss =  0.24619599  Acc:  0.91108334 Val_loss =  0.3440053 Val_acc =  0.8793\n",
            "Iteration  344 : Loss =  0.24389564  Acc:  0.91293335 Val_loss =  0.34148178 Val_acc =  0.8776\n",
            "Iteration  345 : Loss =  0.24204609  Acc:  0.91316664 Val_loss =  0.34042343 Val_acc =  0.8806\n",
            "Iteration  346 : Loss =  0.24031396  Acc:  0.91363335 Val_loss =  0.33818394 Val_acc =  0.8791\n",
            "Iteration  347 : Loss =  0.24042878  Acc:  0.9138333 Val_loss =  0.33946162 Val_acc =  0.8817\n",
            "Iteration  348 : Loss =  0.24231926  Acc:  0.91275 Val_loss =  0.34059623 Val_acc =  0.8793\n",
            "Iteration  349 : Loss =  0.24385944  Acc:  0.9125 Val_loss =  0.34365174 Val_acc =  0.8782\n",
            "Iteration  350 : Loss =  0.24582903  Acc:  0.9108833 Val_loss =  0.34454516 Val_acc =  0.8776\n",
            "Iteration  351 : Loss =  0.24322268  Acc:  0.91275 Val_loss =  0.3435232 Val_acc =  0.8787\n",
            "Iteration  352 : Loss =  0.24029854  Acc:  0.91366667 Val_loss =  0.3396139 Val_acc =  0.8792\n",
            "Iteration  353 : Loss =  0.23783043  Acc:  0.9148167 Val_loss =  0.33818695 Val_acc =  0.8827\n",
            "Iteration  354 : Loss =  0.23812483  Acc:  0.91541666 Val_loss =  0.3384195 Val_acc =  0.8788\n",
            "Iteration  355 : Loss =  0.2397719  Acc:  0.9138167 Val_loss =  0.34051156 Val_acc =  0.8807\n",
            "Iteration  356 : Loss =  0.23970792  Acc:  0.91476667 Val_loss =  0.34092405 Val_acc =  0.8798\n",
            "Iteration  357 : Loss =  0.23770888  Acc:  0.91455 Val_loss =  0.33905488 Val_acc =  0.8818\n",
            "Iteration  358 : Loss =  0.23522347  Acc:  0.9167 Val_loss =  0.3368192 Val_acc =  0.8822\n",
            "Iteration  359 : Loss =  0.23441312  Acc:  0.9163833 Val_loss =  0.3362815 Val_acc =  0.8821\n",
            "Iteration  360 : Loss =  0.23527366  Acc:  0.916 Val_loss =  0.33725265 Val_acc =  0.8817\n",
            "Iteration  361 : Loss =  0.23614788  Acc:  0.91588336 Val_loss =  0.33863202 Val_acc =  0.8796\n",
            "Iteration  362 : Loss =  0.23587112  Acc:  0.91548336 Val_loss =  0.33840644 Val_acc =  0.8819\n",
            "Iteration  363 : Loss =  0.23434897  Acc:  0.917 Val_loss =  0.33728984 Val_acc =  0.8809\n",
            "Iteration  364 : Loss =  0.23296595  Acc:  0.91656667 Val_loss =  0.33621967 Val_acc =  0.8832\n",
            "Iteration  365 : Loss =  0.23262496  Acc:  0.91693336 Val_loss =  0.3358982 Val_acc =  0.8816\n",
            "Iteration  366 : Loss =  0.23314042  Acc:  0.9172 Val_loss =  0.33714068 Val_acc =  0.8821\n",
            "Iteration  367 : Loss =  0.2337812  Acc:  0.91653335 Val_loss =  0.3374308 Val_acc =  0.8814\n",
            "Iteration  368 : Loss =  0.23355825  Acc:  0.9167167 Val_loss =  0.33822113 Val_acc =  0.8814\n",
            "Iteration  369 : Loss =  0.23322427  Acc:  0.9165 Val_loss =  0.33729658 Val_acc =  0.8811\n",
            "Iteration  370 : Loss =  0.23299092  Acc:  0.9168 Val_loss =  0.33834472 Val_acc =  0.8819\n",
            "Iteration  371 : Loss =  0.23365143  Acc:  0.91616666 Val_loss =  0.33821407 Val_acc =  0.8795\n",
            "Iteration  372 : Loss =  0.23635972  Acc:  0.91473335 Val_loss =  0.3424356 Val_acc =  0.881\n",
            "Iteration  373 : Loss =  0.2373  Acc:  0.9145833 Val_loss =  0.34242535 Val_acc =  0.8781\n",
            "Iteration  374 : Loss =  0.2401386  Acc:  0.9128 Val_loss =  0.34688964 Val_acc =  0.8794\n",
            "Iteration  375 : Loss =  0.23625948  Acc:  0.91478336 Val_loss =  0.34190565 Val_acc =  0.8782\n",
            "Iteration  376 : Loss =  0.23330134  Acc:  0.91658336 Val_loss =  0.3402824 Val_acc =  0.8814\n",
            "Iteration  377 : Loss =  0.22984082  Acc:  0.9181 Val_loss =  0.33610654 Val_acc =  0.8815\n",
            "Iteration  378 : Loss =  0.22891963  Acc:  0.9186 Val_loss =  0.33598608 Val_acc =  0.8825\n",
            "Iteration  379 : Loss =  0.2297631  Acc:  0.9178 Val_loss =  0.336977 Val_acc =  0.8817\n",
            "Iteration  380 : Loss =  0.23058343  Acc:  0.9178 Val_loss =  0.3377848 Val_acc =  0.8801\n",
            "Iteration  381 : Loss =  0.23127086  Acc:  0.9169833 Val_loss =  0.33956283 Val_acc =  0.881\n",
            "Iteration  382 : Loss =  0.23008968  Acc:  0.91755 Val_loss =  0.3376141 Val_acc =  0.8803\n",
            "Iteration  383 : Loss =  0.22936372  Acc:  0.9180667 Val_loss =  0.33838144 Val_acc =  0.8819\n",
            "Iteration  384 : Loss =  0.22853832  Acc:  0.9184833 Val_loss =  0.33674708 Val_acc =  0.8811\n",
            "Iteration  385 : Loss =  0.22751898  Acc:  0.9188667 Val_loss =  0.33687213 Val_acc =  0.8807\n",
            "Iteration  386 : Loss =  0.22679281  Acc:  0.91901666 Val_loss =  0.33603647 Val_acc =  0.8818\n",
            "Iteration  387 : Loss =  0.22626883  Acc:  0.92015 Val_loss =  0.33577228 Val_acc =  0.8816\n",
            "Iteration  388 : Loss =  0.226218  Acc:  0.91891664 Val_loss =  0.33641574 Val_acc =  0.8827\n",
            "Iteration  389 : Loss =  0.22629668  Acc:  0.9195167 Val_loss =  0.33596766 Val_acc =  0.8815\n",
            "Iteration  390 : Loss =  0.22653714  Acc:  0.91941667 Val_loss =  0.33743414 Val_acc =  0.882\n",
            "Iteration  391 : Loss =  0.22659281  Acc:  0.91863334 Val_loss =  0.33674243 Val_acc =  0.881\n",
            "Iteration  392 : Loss =  0.22588864  Acc:  0.91973335 Val_loss =  0.33720976 Val_acc =  0.881\n",
            "Iteration  393 : Loss =  0.2249297  Acc:  0.9198667 Val_loss =  0.3357298 Val_acc =  0.8816\n",
            "Iteration  394 : Loss =  0.22373246  Acc:  0.92088336 Val_loss =  0.33538175 Val_acc =  0.882\n",
            "Iteration  395 : Loss =  0.22293304  Acc:  0.9206833 Val_loss =  0.3346383 Val_acc =  0.8814\n",
            "Iteration  396 : Loss =  0.22266382  Acc:  0.92053336 Val_loss =  0.33467823 Val_acc =  0.8811\n",
            "Iteration  397 : Loss =  0.22274452  Acc:  0.9210333 Val_loss =  0.3352912 Val_acc =  0.8821\n",
            "Iteration  398 : Loss =  0.22292759  Acc:  0.9206833 Val_loss =  0.3352805 Val_acc =  0.8823\n",
            "Iteration  399 : Loss =  0.2229128  Acc:  0.921 Val_loss =  0.3361616 Val_acc =  0.8812\n",
            "Iteration  400 : Loss =  0.22283658  Acc:  0.92075 Val_loss =  0.33560568 Val_acc =  0.8815\n",
            "Iteration  401 : Loss =  0.22247256  Acc:  0.92106664 Val_loss =  0.33632883 Val_acc =  0.8817\n",
            "Iteration  402 : Loss =  0.2221152  Acc:  0.9214 Val_loss =  0.3354619 Val_acc =  0.8819\n",
            "Iteration  403 : Loss =  0.22179219  Acc:  0.9210167 Val_loss =  0.33625618 Val_acc =  0.8821\n",
            "Iteration  404 : Loss =  0.22144456  Acc:  0.92118335 Val_loss =  0.33547172 Val_acc =  0.8816\n",
            "Iteration  405 : Loss =  0.22141415  Acc:  0.9210167 Val_loss =  0.33627632 Val_acc =  0.882\n",
            "Iteration  406 : Loss =  0.221245  Acc:  0.92141664 Val_loss =  0.33589536 Val_acc =  0.8814\n",
            "Iteration  407 : Loss =  0.22144143  Acc:  0.9212 Val_loss =  0.3368173 Val_acc =  0.8818\n",
            "Iteration  408 : Loss =  0.22142504  Acc:  0.92145 Val_loss =  0.3367641 Val_acc =  0.8809\n",
            "Iteration  409 : Loss =  0.22167327  Acc:  0.92083335 Val_loss =  0.33760834 Val_acc =  0.8817\n",
            "Iteration  410 : Loss =  0.22136064  Acc:  0.92123336 Val_loss =  0.3372486 Val_acc =  0.88\n",
            "Iteration  411 : Loss =  0.22121595  Acc:  0.92118335 Val_loss =  0.33775273 Val_acc =  0.8813\n",
            "Iteration  412 : Loss =  0.22038795  Acc:  0.92155 Val_loss =  0.33680624 Val_acc =  0.8811\n",
            "Iteration  413 : Loss =  0.2198888  Acc:  0.92183334 Val_loss =  0.33700043 Val_acc =  0.881\n",
            "Iteration  414 : Loss =  0.21887161  Acc:  0.92225 Val_loss =  0.33572036 Val_acc =  0.8817\n",
            "Iteration  415 : Loss =  0.2182489  Acc:  0.9223667 Val_loss =  0.335946 Val_acc =  0.8823\n",
            "Iteration  416 : Loss =  0.21753167  Acc:  0.92268336 Val_loss =  0.3349259 Val_acc =  0.883\n",
            "Iteration  417 : Loss =  0.21718404  Acc:  0.92293334 Val_loss =  0.33552882 Val_acc =  0.8819\n",
            "Iteration  418 : Loss =  0.21714108  Acc:  0.9225 Val_loss =  0.3350976 Val_acc =  0.8819\n",
            "Iteration  419 : Loss =  0.2173417  Acc:  0.92286664 Val_loss =  0.33638266 Val_acc =  0.8805\n",
            "Iteration  420 : Loss =  0.21805446  Acc:  0.92198336 Val_loss =  0.33651623 Val_acc =  0.8815\n",
            "Iteration  421 : Loss =  0.21867448  Acc:  0.9224167 Val_loss =  0.3385038 Val_acc =  0.8803\n",
            "Iteration  422 : Loss =  0.21985565  Acc:  0.9212 Val_loss =  0.33872646 Val_acc =  0.8816\n",
            "Iteration  423 : Loss =  0.21975991  Acc:  0.9213167 Val_loss =  0.34028864 Val_acc =  0.8808\n",
            "Iteration  424 : Loss =  0.21993093  Acc:  0.921 Val_loss =  0.33927354 Val_acc =  0.8809\n",
            "Iteration  425 : Loss =  0.2179901  Acc:  0.922 Val_loss =  0.33895087 Val_acc =  0.8809\n",
            "Iteration  426 : Loss =  0.2162331  Acc:  0.9231167 Val_loss =  0.3362236 Val_acc =  0.8822\n",
            "Iteration  427 : Loss =  0.21492545  Acc:  0.9238167 Val_loss =  0.33605146 Val_acc =  0.8826\n",
            "Iteration  428 : Loss =  0.21472077  Acc:  0.9238833 Val_loss =  0.33571255 Val_acc =  0.8806\n",
            "Iteration  429 : Loss =  0.21596481  Acc:  0.92298335 Val_loss =  0.33749223 Val_acc =  0.8813\n",
            "Iteration  430 : Loss =  0.21765023  Acc:  0.92268336 Val_loss =  0.33969152 Val_acc =  0.8788\n",
            "Iteration  431 : Loss =  0.21935205  Acc:  0.92155 Val_loss =  0.34154713 Val_acc =  0.8795\n",
            "Iteration  432 : Loss =  0.21959952  Acc:  0.92121667 Val_loss =  0.3423384 Val_acc =  0.8776\n",
            "Iteration  433 : Loss =  0.21862282  Acc:  0.92175 Val_loss =  0.34146905 Val_acc =  0.8802\n",
            "Iteration  434 : Loss =  0.21589181  Acc:  0.9231167 Val_loss =  0.33870417 Val_acc =  0.8798\n",
            "Iteration  435 : Loss =  0.2142466  Acc:  0.9238833 Val_loss =  0.3377562 Val_acc =  0.881\n",
            "Iteration  436 : Loss =  0.21364163  Acc:  0.9237 Val_loss =  0.33643994 Val_acc =  0.8822\n",
            "Iteration  437 : Loss =  0.21439742  Acc:  0.9234167 Val_loss =  0.33871272 Val_acc =  0.8808\n",
            "Iteration  438 : Loss =  0.21526004  Acc:  0.9224 Val_loss =  0.33858204 Val_acc =  0.8818\n",
            "Iteration  439 : Loss =  0.21348861  Acc:  0.9235167 Val_loss =  0.33837378 Val_acc =  0.8804\n",
            "Iteration  440 : Loss =  0.21143197  Acc:  0.9243 Val_loss =  0.3356434 Val_acc =  0.8832\n",
            "Iteration  441 : Loss =  0.2096774  Acc:  0.926 Val_loss =  0.33460978 Val_acc =  0.8817\n",
            "Iteration  442 : Loss =  0.2094775  Acc:  0.92551666 Val_loss =  0.33473474 Val_acc =  0.8815\n",
            "Iteration  443 : Loss =  0.21037224  Acc:  0.92475 Val_loss =  0.3354937 Val_acc =  0.8825\n",
            "Iteration  444 : Loss =  0.21093164  Acc:  0.9249 Val_loss =  0.33721274 Val_acc =  0.8816\n",
            "Iteration  445 : Loss =  0.2110149  Acc:  0.9245333 Val_loss =  0.33660227 Val_acc =  0.8824\n",
            "Iteration  446 : Loss =  0.20994563  Acc:  0.9253833 Val_loss =  0.33673453 Val_acc =  0.8815\n",
            "Iteration  447 : Loss =  0.20913349  Acc:  0.92536664 Val_loss =  0.33531094 Val_acc =  0.8821\n",
            "Iteration  448 : Loss =  0.20902024  Acc:  0.9259167 Val_loss =  0.33605903 Val_acc =  0.8818\n",
            "Iteration  449 : Loss =  0.20953394  Acc:  0.9258 Val_loss =  0.3364272 Val_acc =  0.881\n",
            "Iteration  450 : Loss =  0.21045254  Acc:  0.9247 Val_loss =  0.33793554 Val_acc =  0.8808\n",
            "Iteration  451 : Loss =  0.21097064  Acc:  0.92513335 Val_loss =  0.3386538 Val_acc =  0.8792\n",
            "Iteration  452 : Loss =  0.21164662  Acc:  0.9241833 Val_loss =  0.3399728 Val_acc =  0.88\n",
            "Iteration  453 : Loss =  0.2109539  Acc:  0.9248667 Val_loss =  0.33915284 Val_acc =  0.8792\n",
            "Iteration  454 : Loss =  0.21130303  Acc:  0.92435 Val_loss =  0.34028754 Val_acc =  0.8798\n",
            "Iteration  455 : Loss =  0.20959847  Acc:  0.92506665 Val_loss =  0.33799642 Val_acc =  0.8802\n",
            "Iteration  456 : Loss =  0.20918272  Acc:  0.92583334 Val_loss =  0.3388062 Val_acc =  0.8804\n",
            "Iteration  457 : Loss =  0.2082412  Acc:  0.9253333 Val_loss =  0.33702722 Val_acc =  0.8826\n",
            "Iteration  458 : Loss =  0.20753226  Acc:  0.9264 Val_loss =  0.337812 Val_acc =  0.8811\n",
            "Iteration  459 : Loss =  0.20684892  Acc:  0.9258 Val_loss =  0.3364014 Val_acc =  0.8825\n",
            "Iteration  460 : Loss =  0.20570333  Acc:  0.92761666 Val_loss =  0.33639228 Val_acc =  0.8822\n",
            "Iteration  461 : Loss =  0.20456444  Acc:  0.9273667 Val_loss =  0.33503178 Val_acc =  0.882\n",
            "Iteration  462 : Loss =  0.20388333  Acc:  0.9277 Val_loss =  0.3346814 Val_acc =  0.8816\n",
            "Iteration  463 : Loss =  0.20395502  Acc:  0.92786664 Val_loss =  0.33543238 Val_acc =  0.8813\n",
            "Iteration  464 : Loss =  0.20463414  Acc:  0.92681664 Val_loss =  0.33580288 Val_acc =  0.8822\n",
            "Iteration  465 : Loss =  0.20537631  Acc:  0.92715 Val_loss =  0.337857 Val_acc =  0.8817\n",
            "Iteration  466 : Loss =  0.20581396  Acc:  0.92605 Val_loss =  0.3374148 Val_acc =  0.8816\n",
            "Iteration  467 : Loss =  0.20571677  Acc:  0.9270333 Val_loss =  0.33879775 Val_acc =  0.8814\n",
            "Iteration  468 : Loss =  0.20530762  Acc:  0.92645 Val_loss =  0.3374215 Val_acc =  0.8818\n",
            "Iteration  469 : Loss =  0.2054066  Acc:  0.92691666 Val_loss =  0.33880594 Val_acc =  0.8806\n",
            "Iteration  470 : Loss =  0.20497495  Acc:  0.9271167 Val_loss =  0.3378364 Val_acc =  0.8816\n",
            "Iteration  471 : Loss =  0.20554997  Acc:  0.9264 Val_loss =  0.33932236 Val_acc =  0.8811\n",
            "Iteration  472 : Loss =  0.20466968  Acc:  0.9275 Val_loss =  0.33840767 Val_acc =  0.8799\n",
            "Iteration  473 : Loss =  0.20451552  Acc:  0.92683333 Val_loss =  0.338661 Val_acc =  0.8815\n",
            "Iteration  474 : Loss =  0.20384486  Acc:  0.9277 Val_loss =  0.3382429 Val_acc =  0.88\n",
            "Iteration  475 : Loss =  0.20353812  Acc:  0.9275 Val_loss =  0.33828527 Val_acc =  0.8815\n",
            "Iteration  476 : Loss =  0.20290011  Acc:  0.92838335 Val_loss =  0.33807 Val_acc =  0.8814\n",
            "Iteration  477 : Loss =  0.20172603  Acc:  0.9284 Val_loss =  0.3371063 Val_acc =  0.8816\n",
            "Iteration  478 : Loss =  0.20025355  Acc:  0.92936665 Val_loss =  0.3359651 Val_acc =  0.8825\n",
            "Iteration  479 : Loss =  0.19930197  Acc:  0.9292667 Val_loss =  0.33513427 Val_acc =  0.8819\n",
            "Iteration  480 : Loss =  0.19918594  Acc:  0.9296833 Val_loss =  0.33542424 Val_acc =  0.8823\n",
            "Iteration  481 : Loss =  0.19957721  Acc:  0.92938334 Val_loss =  0.3360113 Val_acc =  0.8831\n",
            "Iteration  482 : Loss =  0.19999462  Acc:  0.92931664 Val_loss =  0.33692577 Val_acc =  0.8821\n",
            "Iteration  483 : Loss =  0.20020021  Acc:  0.92935 Val_loss =  0.33739907 Val_acc =  0.8816\n",
            "Iteration  484 : Loss =  0.2003348  Acc:  0.92861664 Val_loss =  0.33788988 Val_acc =  0.8824\n",
            "Iteration  485 : Loss =  0.20047489  Acc:  0.92906666 Val_loss =  0.33828917 Val_acc =  0.8799\n",
            "Iteration  486 : Loss =  0.20110935  Acc:  0.92775 Val_loss =  0.339209 Val_acc =  0.8814\n",
            "Iteration  487 : Loss =  0.20113042  Acc:  0.9286 Val_loss =  0.33937588 Val_acc =  0.8799\n",
            "Iteration  488 : Loss =  0.20192493  Acc:  0.9270833 Val_loss =  0.34065 Val_acc =  0.8807\n",
            "Iteration  489 : Loss =  0.20106302  Acc:  0.9281833 Val_loss =  0.3396193 Val_acc =  0.8803\n",
            "Iteration  490 : Loss =  0.20155387  Acc:  0.92763335 Val_loss =  0.34107074 Val_acc =  0.8811\n",
            "Iteration  491 : Loss =  0.20080371  Acc:  0.92803335 Val_loss =  0.33963642 Val_acc =  0.8815\n",
            "Iteration  492 : Loss =  0.20122549  Acc:  0.92836666 Val_loss =  0.34161925 Val_acc =  0.8822\n",
            "Iteration  493 : Loss =  0.20107448  Acc:  0.92731667 Val_loss =  0.3402852 Val_acc =  0.8814\n",
            "Iteration  494 : Loss =  0.20049708  Acc:  0.92871666 Val_loss =  0.34165934 Val_acc =  0.8811\n",
            "Iteration  495 : Loss =  0.20010081  Acc:  0.92801666 Val_loss =  0.34005365 Val_acc =  0.8824\n",
            "Iteration  496 : Loss =  0.19871835  Acc:  0.92953336 Val_loss =  0.34017357 Val_acc =  0.8816\n",
            "Iteration  497 : Loss =  0.19785105  Acc:  0.9295833 Val_loss =  0.3386042 Val_acc =  0.8815\n",
            "Iteration  498 : Loss =  0.19645062  Acc:  0.9306167 Val_loss =  0.33789343 Val_acc =  0.882\n",
            "Iteration  499 : Loss =  0.19558012  Acc:  0.93041664 Val_loss =  0.33734632 Val_acc =  0.883\n",
            "Iteration  500 : Loss =  0.1955393  Acc:  0.93015 Val_loss =  0.33729458 Val_acc =  0.8821\n",
            "Iteration  501 : Loss =  0.19652216  Acc:  0.93011665 Val_loss =  0.33939594 Val_acc =  0.8814\n",
            "Iteration  502 : Loss =  0.19752523  Acc:  0.9293 Val_loss =  0.3398444 Val_acc =  0.8807\n",
            "Iteration  503 : Loss =  0.19847588  Acc:  0.9292167 Val_loss =  0.34227115 Val_acc =  0.8818\n",
            "Iteration  504 : Loss =  0.19836503  Acc:  0.9284667 Val_loss =  0.34108302 Val_acc =  0.8803\n",
            "Iteration  505 : Loss =  0.1970818  Acc:  0.92973334 Val_loss =  0.34117293 Val_acc =  0.8809\n",
            "Iteration  506 : Loss =  0.19520098  Acc:  0.9302667 Val_loss =  0.33829418 Val_acc =  0.881\n",
            "Iteration  507 : Loss =  0.19350663  Acc:  0.93163335 Val_loss =  0.33767626 Val_acc =  0.8818\n",
            "Iteration  508 : Loss =  0.19266228  Acc:  0.93156666 Val_loss =  0.33659694 Val_acc =  0.8835\n",
            "Iteration  509 : Loss =  0.1925685  Acc:  0.93156666 Val_loss =  0.33696166 Val_acc =  0.8825\n",
            "Iteration  510 : Loss =  0.192852  Acc:  0.9320833 Val_loss =  0.33807805 Val_acc =  0.8826\n",
            "Iteration  511 : Loss =  0.19342588  Acc:  0.93093336 Val_loss =  0.33832443 Val_acc =  0.8815\n",
            "Iteration  512 : Loss =  0.19380154  Acc:  0.93158334 Val_loss =  0.3400213 Val_acc =  0.8825\n",
            "Iteration  513 : Loss =  0.19413887  Acc:  0.9303 Val_loss =  0.3393166 Val_acc =  0.8813\n",
            "Iteration  514 : Loss =  0.19370899  Acc:  0.93175 Val_loss =  0.3404018 Val_acc =  0.8818\n",
            "Iteration  515 : Loss =  0.1928403  Acc:  0.931 Val_loss =  0.3385763 Val_acc =  0.8822\n",
            "Iteration  516 : Loss =  0.19164217  Acc:  0.9324167 Val_loss =  0.33862373 Val_acc =  0.8816\n",
            "Iteration  517 : Loss =  0.19068834  Acc:  0.9320667 Val_loss =  0.337359 Val_acc =  0.8816\n",
            "Iteration  518 : Loss =  0.19032842  Acc:  0.93271667 Val_loss =  0.33753622 Val_acc =  0.8828\n",
            "Iteration  519 : Loss =  0.19040668  Acc:  0.9328667 Val_loss =  0.3381609 Val_acc =  0.8819\n",
            "Iteration  520 : Loss =  0.19087075  Acc:  0.9321667 Val_loss =  0.3383854 Val_acc =  0.8822\n",
            "Iteration  521 : Loss =  0.19152384  Acc:  0.9327 Val_loss =  0.3401642 Val_acc =  0.8813\n",
            "Iteration  522 : Loss =  0.19268464  Acc:  0.93091667 Val_loss =  0.3406574 Val_acc =  0.882\n",
            "Iteration  523 : Loss =  0.19404891  Acc:  0.93085 Val_loss =  0.34345773 Val_acc =  0.8804\n",
            "Iteration  524 : Loss =  0.1961502  Acc:  0.9292667 Val_loss =  0.34487095 Val_acc =  0.8797\n",
            "Iteration  525 : Loss =  0.19760364  Acc:  0.9291833 Val_loss =  0.34759483 Val_acc =  0.88\n",
            "Iteration  526 : Loss =  0.19905007  Acc:  0.9278833 Val_loss =  0.34863517 Val_acc =  0.8792\n",
            "Iteration  527 : Loss =  0.19854173  Acc:  0.9288333 Val_loss =  0.3486339 Val_acc =  0.8799\n",
            "Iteration  528 : Loss =  0.19970621  Acc:  0.92766666 Val_loss =  0.3502478 Val_acc =  0.8806\n",
            "Iteration  529 : Loss =  0.19617258  Acc:  0.92948335 Val_loss =  0.34609222 Val_acc =  0.8804\n",
            "Iteration  530 : Loss =  0.19583064  Acc:  0.92971665 Val_loss =  0.34711155 Val_acc =  0.881\n",
            "Iteration  531 : Loss =  0.19382758  Acc:  0.9303333 Val_loss =  0.344027 Val_acc =  0.8811\n",
            "Iteration  532 : Loss =  0.19102815  Acc:  0.93255 Val_loss =  0.34276572 Val_acc =  0.8824\n",
            "Iteration  533 : Loss =  0.18952982  Acc:  0.93185 Val_loss =  0.34066552 Val_acc =  0.8817\n",
            "Iteration  534 : Loss =  0.18942057  Acc:  0.9328167 Val_loss =  0.34113336 Val_acc =  0.8807\n",
            "Iteration  535 : Loss =  0.19158378  Acc:  0.9315 Val_loss =  0.34381378 Val_acc =  0.8812\n",
            "Iteration  536 : Loss =  0.19181342  Acc:  0.93135 Val_loss =  0.3433327 Val_acc =  0.8808\n",
            "Iteration  537 : Loss =  0.1910427  Acc:  0.9318333 Val_loss =  0.34433126 Val_acc =  0.8815\n",
            "Iteration  538 : Loss =  0.18869916  Acc:  0.9320667 Val_loss =  0.34096548 Val_acc =  0.8824\n",
            "Iteration  539 : Loss =  0.1864898  Acc:  0.9346167 Val_loss =  0.34017828 Val_acc =  0.8817\n",
            "Iteration  540 : Loss =  0.1860041  Acc:  0.9339167 Val_loss =  0.3393258 Val_acc =  0.8833\n",
            "Iteration  541 : Loss =  0.18686122  Acc:  0.9341 Val_loss =  0.3404419 Val_acc =  0.8817\n",
            "Iteration  542 : Loss =  0.18840323  Acc:  0.9327833 Val_loss =  0.3427733 Val_acc =  0.8818\n",
            "Iteration  543 : Loss =  0.18842256  Acc:  0.9328 Val_loss =  0.3420812 Val_acc =  0.8824\n",
            "Iteration  544 : Loss =  0.1876135  Acc:  0.93365 Val_loss =  0.34274068 Val_acc =  0.8821\n",
            "Iteration  545 : Loss =  0.18583408  Acc:  0.93333334 Val_loss =  0.3401918 Val_acc =  0.8837\n",
            "Iteration  546 : Loss =  0.18436962  Acc:  0.93476665 Val_loss =  0.33959603 Val_acc =  0.882\n",
            "Iteration  547 : Loss =  0.18379027  Acc:  0.93523335 Val_loss =  0.33906075 Val_acc =  0.8829\n",
            "Iteration  548 : Loss =  0.18410996  Acc:  0.93475 Val_loss =  0.33944356 Val_acc =  0.8828\n",
            "Iteration  549 : Loss =  0.18475607  Acc:  0.93438333 Val_loss =  0.34112367 Val_acc =  0.8807\n",
            "Iteration  550 : Loss =  0.18505064  Acc:  0.93378335 Val_loss =  0.34077442 Val_acc =  0.884\n",
            "Iteration  551 : Loss =  0.18471028  Acc:  0.9345 Val_loss =  0.3416956 Val_acc =  0.8817\n",
            "Iteration  552 : Loss =  0.1837885  Acc:  0.93443334 Val_loss =  0.34025264 Val_acc =  0.8833\n",
            "Iteration  553 : Loss =  0.18296543  Acc:  0.9349667 Val_loss =  0.34006476 Val_acc =  0.8817\n",
            "Iteration  554 : Loss =  0.18248607  Acc:  0.9360667 Val_loss =  0.33986259 Val_acc =  0.8824\n",
            "Iteration  555 : Loss =  0.1824759  Acc:  0.93546665 Val_loss =  0.33982465 Val_acc =  0.8829\n",
            "Iteration  556 : Loss =  0.18268223  Acc:  0.936 Val_loss =  0.34109232 Val_acc =  0.8816\n",
            "Iteration  557 : Loss =  0.18289752  Acc:  0.93516666 Val_loss =  0.3405966 Val_acc =  0.8819\n",
            "Iteration  558 : Loss =  0.18270582  Acc:  0.9357833 Val_loss =  0.34175268 Val_acc =  0.8821\n",
            "Iteration  559 : Loss =  0.18235719  Acc:  0.9350833 Val_loss =  0.3407199 Val_acc =  0.8816\n",
            "Iteration  560 : Loss =  0.18171257  Acc:  0.93605 Val_loss =  0.3411641 Val_acc =  0.8827\n",
            "Iteration  561 : Loss =  0.181128  Acc:  0.9360667 Val_loss =  0.3402727 Val_acc =  0.882\n",
            "Iteration  562 : Loss =  0.1806459  Acc:  0.93656665 Val_loss =  0.3404851 Val_acc =  0.8827\n",
            "Iteration  563 : Loss =  0.18037294  Acc:  0.9360833 Val_loss =  0.3403831 Val_acc =  0.8822\n",
            "Iteration  564 : Loss =  0.18028043  Acc:  0.9364833 Val_loss =  0.34049934 Val_acc =  0.8836\n",
            "Iteration  565 : Loss =  0.18033634  Acc:  0.9360167 Val_loss =  0.3411967 Val_acc =  0.8818\n",
            "Iteration  566 : Loss =  0.180471  Acc:  0.93591666 Val_loss =  0.3412433 Val_acc =  0.8841\n",
            "Iteration  567 : Loss =  0.18084463  Acc:  0.93563336 Val_loss =  0.34223968 Val_acc =  0.8815\n",
            "Iteration  568 : Loss =  0.18118688  Acc:  0.9356667 Val_loss =  0.3424263 Val_acc =  0.8836\n",
            "Iteration  569 : Loss =  0.18220781  Acc:  0.93478334 Val_loss =  0.3441145 Val_acc =  0.8818\n",
            "Iteration  570 : Loss =  0.1828375  Acc:  0.93516666 Val_loss =  0.34473762 Val_acc =  0.8822\n",
            "Iteration  571 : Loss =  0.18519782  Acc:  0.9332167 Val_loss =  0.34752446 Val_acc =  0.8821\n",
            "Iteration  572 : Loss =  0.18600146  Acc:  0.93395 Val_loss =  0.34877318 Val_acc =  0.8814\n",
            "Iteration  573 : Loss =  0.18981849  Acc:  0.9313833 Val_loss =  0.35253045 Val_acc =  0.8805\n",
            "Iteration  574 : Loss =  0.19062409  Acc:  0.93215 Val_loss =  0.3543698 Val_acc =  0.8789\n",
            "Iteration  575 : Loss =  0.19257952  Acc:  0.93011665 Val_loss =  0.35542843 Val_acc =  0.878\n",
            "Iteration  576 : Loss =  0.19032735  Acc:  0.93195 Val_loss =  0.3548772 Val_acc =  0.8795\n",
            "Iteration  577 : Loss =  0.18602039  Acc:  0.9332833 Val_loss =  0.34886223 Val_acc =  0.8807\n",
            "Iteration  578 : Loss =  0.18034206  Acc:  0.93666667 Val_loss =  0.34463426 Val_acc =  0.8819\n",
            "Iteration  579 : Loss =  0.1794309  Acc:  0.9363 Val_loss =  0.34323096 Val_acc =  0.8831\n",
            "Iteration  580 : Loss =  0.18320613  Acc:  0.9338667 Val_loss =  0.3476994 Val_acc =  0.882\n",
            "Iteration  581 : Loss =  0.18502328  Acc:  0.93445 Val_loss =  0.35049978 Val_acc =  0.8811\n",
            "Iteration  582 : Loss =  0.18355593  Acc:  0.93368334 Val_loss =  0.34868512 Val_acc =  0.8811\n",
            "Iteration  583 : Loss =  0.17893088  Acc:  0.93721664 Val_loss =  0.34517857 Val_acc =  0.8822\n",
            "Iteration  584 : Loss =  0.1762754  Acc:  0.9378667 Val_loss =  0.3421998 Val_acc =  0.8829\n",
            "Iteration  585 : Loss =  0.17733255  Acc:  0.9364 Val_loss =  0.34372553 Val_acc =  0.8817\n",
            "Iteration  586 : Loss =  0.17995928  Acc:  0.93661666 Val_loss =  0.34691206 Val_acc =  0.8821\n",
            "Iteration  587 : Loss =  0.18181573  Acc:  0.93455 Val_loss =  0.34838888 Val_acc =  0.8804\n",
            "Iteration  588 : Loss =  0.17984109  Acc:  0.93675 Val_loss =  0.34708342 Val_acc =  0.8826\n",
            "Iteration  589 : Loss =  0.177013  Acc:  0.93693334 Val_loss =  0.34402677 Val_acc =  0.8819\n",
            "Iteration  590 : Loss =  0.17497952  Acc:  0.93845 Val_loss =  0.34240055 Val_acc =  0.8828\n",
            "Iteration  591 : Loss =  0.17519473  Acc:  0.93846667 Val_loss =  0.3428706 Val_acc =  0.8839\n",
            "Iteration  592 : Loss =  0.17658271  Acc:  0.9367833 Val_loss =  0.34423232 Val_acc =  0.882\n",
            "Iteration  593 : Loss =  0.17732714  Acc:  0.9379 Val_loss =  0.34598354 Val_acc =  0.8831\n",
            "Iteration  594 : Loss =  0.17683865  Acc:  0.93645 Val_loss =  0.34519857 Val_acc =  0.8825\n",
            "Iteration  595 : Loss =  0.175136  Acc:  0.9389667 Val_loss =  0.34434482 Val_acc =  0.883\n",
            "Iteration  596 : Loss =  0.17390507  Acc:  0.93865 Val_loss =  0.34319723 Val_acc =  0.8829\n",
            "Iteration  597 : Loss =  0.17374244  Acc:  0.93865 Val_loss =  0.343194 Val_acc =  0.8821\n",
            "Iteration  598 : Loss =  0.17430246  Acc:  0.9393 Val_loss =  0.34429616 Val_acc =  0.8834\n",
            "Iteration  599 : Loss =  0.17479491  Acc:  0.93771666 Val_loss =  0.34446374 Val_acc =  0.8823\n",
            "Iteration  600 : Loss =  0.17456117  Acc:  0.9392167 Val_loss =  0.34532842 Val_acc =  0.8831\n",
            "Iteration  601 : Loss =  0.17374922  Acc:  0.9389 Val_loss =  0.34399462 Val_acc =  0.8827\n",
            "Iteration  602 : Loss =  0.17283458  Acc:  0.9396167 Val_loss =  0.34401593 Val_acc =  0.883\n",
            "Iteration  603 : Loss =  0.17233591  Acc:  0.93941665 Val_loss =  0.3432473 Val_acc =  0.8837\n",
            "Iteration  604 : Loss =  0.17231615  Acc:  0.93945 Val_loss =  0.34384874 Val_acc =  0.8829\n",
            "Iteration  605 : Loss =  0.17254746  Acc:  0.9392 Val_loss =  0.34424305 Val_acc =  0.8847\n",
            "Iteration  606 : Loss =  0.17270905  Acc:  0.9387 Val_loss =  0.34463122 Val_acc =  0.8827\n",
            "Iteration  607 : Loss =  0.17251885  Acc:  0.9396833 Val_loss =  0.3449793 Val_acc =  0.8844\n",
            "Iteration  608 : Loss =  0.17216189  Acc:  0.9389167 Val_loss =  0.34474757 Val_acc =  0.8824\n",
            "Iteration  609 : Loss =  0.17166819  Acc:  0.9403167 Val_loss =  0.34462094 Val_acc =  0.8835\n",
            "Iteration  610 : Loss =  0.17137936  Acc:  0.9396333 Val_loss =  0.34443432 Val_acc =  0.8826\n",
            "Iteration  611 : Loss =  0.1712572  Acc:  0.9400833 Val_loss =  0.34460333 Val_acc =  0.8827\n",
            "Iteration  612 : Loss =  0.17152742  Acc:  0.93971664 Val_loss =  0.34538412 Val_acc =  0.8825\n",
            "Iteration  613 : Loss =  0.1722791  Acc:  0.93915 Val_loss =  0.34592268 Val_acc =  0.8826\n",
            "Iteration  614 : Loss =  0.17350882  Acc:  0.93906665 Val_loss =  0.34835404 Val_acc =  0.8823\n",
            "Iteration  615 : Loss =  0.17619452  Acc:  0.9374833 Val_loss =  0.35002318 Val_acc =  0.8824\n",
            "Iteration  616 : Loss =  0.17889126  Acc:  0.9360167 Val_loss =  0.3547335 Val_acc =  0.8805\n",
            "Iteration  617 : Loss =  0.18319525  Acc:  0.9331833 Val_loss =  0.35694572 Val_acc =  0.8807\n",
            "Iteration  618 : Loss =  0.18348593  Acc:  0.93343335 Val_loss =  0.35994482 Val_acc =  0.8802\n",
            "Iteration  619 : Loss =  0.18066941  Acc:  0.9343 Val_loss =  0.3547351 Val_acc =  0.8814\n",
            "Iteration  620 : Loss =  0.17604992  Acc:  0.93705 Val_loss =  0.3520417 Val_acc =  0.8822\n",
            "Iteration  621 : Loss =  0.17233385  Acc:  0.93958336 Val_loss =  0.34772533 Val_acc =  0.8834\n",
            "Iteration  622 : Loss =  0.17229843  Acc:  0.9386333 Val_loss =  0.34782106 Val_acc =  0.8824\n",
            "Iteration  623 : Loss =  0.17377424  Acc:  0.9390333 Val_loss =  0.3514709 Val_acc =  0.8811\n",
            "Iteration  624 : Loss =  0.17568669  Acc:  0.9371833 Val_loss =  0.35179356 Val_acc =  0.8825\n",
            "Iteration  625 : Loss =  0.17434476  Acc:  0.9383 Val_loss =  0.3531013 Val_acc =  0.8818\n",
            "Iteration  626 : Loss =  0.17172427  Acc:  0.9386333 Val_loss =  0.3486583 Val_acc =  0.8836\n",
            "Iteration  627 : Loss =  0.16979586  Acc:  0.9397 Val_loss =  0.34729072 Val_acc =  0.8817\n",
            "Iteration  628 : Loss =  0.169725  Acc:  0.9410167 Val_loss =  0.34789848 Val_acc =  0.8822\n",
            "Iteration  629 : Loss =  0.17135799  Acc:  0.93913335 Val_loss =  0.34839427 Val_acc =  0.8841\n",
            "Iteration  630 : Loss =  0.1720098  Acc:  0.9400833 Val_loss =  0.35176754 Val_acc =  0.8812\n",
            "Iteration  631 : Loss =  0.17089942  Acc:  0.9392833 Val_loss =  0.34860575 Val_acc =  0.8849\n",
            "Iteration  632 : Loss =  0.16819343  Acc:  0.9415333 Val_loss =  0.3481076 Val_acc =  0.8838\n",
            "Iteration  633 : Loss =  0.16652125  Acc:  0.94195 Val_loss =  0.3458224 Val_acc =  0.8843\n",
            "Iteration  634 : Loss =  0.1668843  Acc:  0.9414833 Val_loss =  0.3460278 Val_acc =  0.8842\n",
            "Iteration  635 : Loss =  0.1681883  Acc:  0.94156665 Val_loss =  0.348966 Val_acc =  0.8821\n",
            "Iteration  636 : Loss =  0.16887894  Acc:  0.9399833 Val_loss =  0.34827504 Val_acc =  0.8841\n",
            "Iteration  637 : Loss =  0.16812335  Acc:  0.9416 Val_loss =  0.34945044 Val_acc =  0.8829\n",
            "Iteration  638 : Loss =  0.16698126  Acc:  0.94063336 Val_loss =  0.34716204 Val_acc =  0.8837\n",
            "Iteration  639 : Loss =  0.16619891  Acc:  0.9422 Val_loss =  0.34740353 Val_acc =  0.8849\n",
            "Iteration  640 : Loss =  0.16607875  Acc:  0.9418167 Val_loss =  0.34761354 Val_acc =  0.8826\n",
            "Iteration  641 : Loss =  0.16610667  Acc:  0.94205 Val_loss =  0.3471261 Val_acc =  0.8858\n",
            "Iteration  642 : Loss =  0.1659499  Acc:  0.94231665 Val_loss =  0.34822294 Val_acc =  0.8837\n",
            "Iteration  643 : Loss =  0.16566266  Acc:  0.9421333 Val_loss =  0.34726596 Val_acc =  0.8845\n",
            "Iteration  644 : Loss =  0.16548401  Acc:  0.94243336 Val_loss =  0.34836605 Val_acc =  0.8839\n",
            "Iteration  645 : Loss =  0.16560824  Acc:  0.94185 Val_loss =  0.34823972 Val_acc =  0.8836\n",
            "Iteration  646 : Loss =  0.16546051  Acc:  0.9421833 Val_loss =  0.3484925 Val_acc =  0.885\n",
            "Iteration  647 : Loss =  0.16495737  Acc:  0.94266665 Val_loss =  0.34836718 Val_acc =  0.8837\n",
            "Iteration  648 : Loss =  0.16412304  Acc:  0.9431667 Val_loss =  0.3475695 Val_acc =  0.8842\n",
            "Iteration  649 : Loss =  0.16347139  Acc:  0.9432667 Val_loss =  0.34734735 Val_acc =  0.8849\n",
            "Iteration  650 : Loss =  0.16328984  Acc:  0.94308335 Val_loss =  0.34743574 Val_acc =  0.8841\n",
            "Iteration  651 : Loss =  0.16348289  Acc:  0.9433 Val_loss =  0.34799662 Val_acc =  0.8851\n",
            "Iteration  652 : Loss =  0.16377349  Acc:  0.94255 Val_loss =  0.3485136 Val_acc =  0.8827\n",
            "Iteration  653 : Loss =  0.1638917  Acc:  0.94296664 Val_loss =  0.34902704 Val_acc =  0.8853\n",
            "Iteration  654 : Loss =  0.16386157  Acc:  0.94233334 Val_loss =  0.34898075 Val_acc =  0.8832\n",
            "Iteration  655 : Loss =  0.16381256  Acc:  0.9432833 Val_loss =  0.3497564 Val_acc =  0.8841\n",
            "Iteration  656 : Loss =  0.1640772  Acc:  0.94215 Val_loss =  0.34959656 Val_acc =  0.8835\n",
            "Iteration  657 : Loss =  0.16464892  Acc:  0.94306666 Val_loss =  0.35159966 Val_acc =  0.8834\n",
            "Iteration  658 : Loss =  0.16624853  Acc:  0.94116664 Val_loss =  0.3521954 Val_acc =  0.8831\n",
            "Iteration  659 : Loss =  0.16766928  Acc:  0.94145 Val_loss =  0.35533702 Val_acc =  0.8832\n",
            "Iteration  660 : Loss =  0.17199394  Acc:  0.9384 Val_loss =  0.35872573 Val_acc =  0.8814\n",
            "Iteration  661 : Loss =  0.17245257  Acc:  0.9382833 Val_loss =  0.36050043 Val_acc =  0.8795\n",
            "Iteration  662 : Loss =  0.17833959  Acc:  0.9355 Val_loss =  0.3659893 Val_acc =  0.8812\n",
            "Iteration  663 : Loss =  0.1725103  Acc:  0.93811667 Val_loss =  0.36022907 Val_acc =  0.8805\n",
            "Iteration  664 : Loss =  0.1700266  Acc:  0.93908334 Val_loss =  0.35830185 Val_acc =  0.8826\n",
            "Iteration  665 : Loss =  0.16446315  Acc:  0.94145 Val_loss =  0.35140175 Val_acc =  0.8844\n",
            "Iteration  666 : Loss =  0.16222893  Acc:  0.94345 Val_loss =  0.35120744 Val_acc =  0.885\n",
            "Iteration  667 : Loss =  0.16358125  Acc:  0.94193333 Val_loss =  0.35162982 Val_acc =  0.883\n",
            "Iteration  668 : Loss =  0.16603447  Acc:  0.9416 Val_loss =  0.35574344 Val_acc =  0.8825\n",
            "Iteration  669 : Loss =  0.16867249  Acc:  0.93955 Val_loss =  0.35825482 Val_acc =  0.8811\n",
            "Iteration  670 : Loss =  0.16589533  Acc:  0.9410833 Val_loss =  0.35592285 Val_acc =  0.8818\n",
            "Iteration  671 : Loss =  0.16296197  Acc:  0.9422167 Val_loss =  0.35319966 Val_acc =  0.8817\n",
            "Iteration  672 : Loss =  0.15999694  Acc:  0.94445 Val_loss =  0.35007897 Val_acc =  0.8856\n",
            "Iteration  673 : Loss =  0.15974723  Acc:  0.9446167 Val_loss =  0.35064468 Val_acc =  0.8852\n",
            "Iteration  674 : Loss =  0.16152015  Acc:  0.94276667 Val_loss =  0.352243 Val_acc =  0.882\n",
            "Iteration  675 : Loss =  0.16331048  Acc:  0.94318336 Val_loss =  0.3549811 Val_acc =  0.8827\n",
            "Iteration  676 : Loss =  0.16411729  Acc:  0.94131666 Val_loss =  0.35512167 Val_acc =  0.8819\n",
            "Iteration  677 : Loss =  0.1625777  Acc:  0.94365 Val_loss =  0.35464782 Val_acc =  0.8828\n",
            "Iteration  678 : Loss =  0.16071296  Acc:  0.94315 Val_loss =  0.35201293 Val_acc =  0.8831\n",
            "Iteration  679 : Loss =  0.15899421  Acc:  0.9451333 Val_loss =  0.351677 Val_acc =  0.8847\n",
            "Iteration  680 : Loss =  0.1582945  Acc:  0.9451 Val_loss =  0.35032126 Val_acc =  0.885\n",
            "Iteration  681 : Loss =  0.15843107  Acc:  0.9446667 Val_loss =  0.35132733 Val_acc =  0.8829\n",
            "Iteration  682 : Loss =  0.15899642  Acc:  0.9446333 Val_loss =  0.35236007 Val_acc =  0.8844\n",
            "Iteration  683 : Loss =  0.15964428  Acc:  0.94346666 Val_loss =  0.35261148 Val_acc =  0.8826\n",
            "Iteration  684 : Loss =  0.15959822  Acc:  0.94493335 Val_loss =  0.35391283 Val_acc =  0.8843\n",
            "Iteration  685 : Loss =  0.15932052  Acc:  0.94366664 Val_loss =  0.35265797 Val_acc =  0.8839\n",
            "Iteration  686 : Loss =  0.15834342  Acc:  0.94555 Val_loss =  0.3532188 Val_acc =  0.8841\n",
            "Iteration  687 : Loss =  0.1573862  Acc:  0.94488335 Val_loss =  0.35126287 Val_acc =  0.8838\n",
            "Iteration  688 : Loss =  0.15676373  Acc:  0.9459 Val_loss =  0.3518527 Val_acc =  0.886\n",
            "Iteration  689 : Loss =  0.15673833  Acc:  0.94588333 Val_loss =  0.35162428 Val_acc =  0.8858\n",
            "Iteration  690 : Loss =  0.15727966  Acc:  0.9451333 Val_loss =  0.35274282 Val_acc =  0.8829\n",
            "Iteration  691 : Loss =  0.15787517  Acc:  0.94546664 Val_loss =  0.35365722 Val_acc =  0.8854\n",
            "Iteration  692 : Loss =  0.15877357  Acc:  0.944 Val_loss =  0.35470673 Val_acc =  0.8831\n",
            "Iteration  693 : Loss =  0.15905525  Acc:  0.9443167 Val_loss =  0.35509467 Val_acc =  0.8849\n",
            "Iteration  694 : Loss =  0.1598387  Acc:  0.94365 Val_loss =  0.35670108 Val_acc =  0.8826\n",
            "Iteration  695 : Loss =  0.16056657  Acc:  0.94385 Val_loss =  0.35662988 Val_acc =  0.8853\n",
            "Iteration  696 : Loss =  0.1613129  Acc:  0.9429333 Val_loss =  0.35932165 Val_acc =  0.8809\n",
            "Iteration  697 : Loss =  0.162819  Acc:  0.9427 Val_loss =  0.3588575 Val_acc =  0.8832\n",
            "Iteration  698 : Loss =  0.16188337  Acc:  0.9433333 Val_loss =  0.36066777 Val_acc =  0.8816\n",
            "Iteration  699 : Loss =  0.16109827  Acc:  0.94325 Val_loss =  0.35738468 Val_acc =  0.8836\n",
            "Iteration  700 : Loss =  0.15799043  Acc:  0.9453167 Val_loss =  0.35681728 Val_acc =  0.8835\n",
            "Iteration  701 : Loss =  0.1557375  Acc:  0.9453833 Val_loss =  0.35340297 Val_acc =  0.8845\n",
            "Iteration  702 : Loss =  0.15505013  Acc:  0.94633335 Val_loss =  0.35370502 Val_acc =  0.8854\n",
            "Iteration  703 : Loss =  0.15622787  Acc:  0.94575 Val_loss =  0.3557483 Val_acc =  0.8816\n",
            "Iteration  704 : Loss =  0.15818489  Acc:  0.94435 Val_loss =  0.35685986 Val_acc =  0.8846\n",
            "Iteration  705 : Loss =  0.15983376  Acc:  0.94381666 Val_loss =  0.3607614 Val_acc =  0.8816\n",
            "Iteration  706 : Loss =  0.16124967  Acc:  0.9428 Val_loss =  0.36014804 Val_acc =  0.8828\n",
            "Iteration  707 : Loss =  0.15971343  Acc:  0.9436333 Val_loss =  0.361025 Val_acc =  0.8821\n",
            "Iteration  708 : Loss =  0.15789102  Acc:  0.94451666 Val_loss =  0.35721454 Val_acc =  0.8848\n",
            "Iteration  709 : Loss =  0.1547141  Acc:  0.94656664 Val_loss =  0.3552438 Val_acc =  0.8854\n",
            "Iteration  710 : Loss =  0.15321128  Acc:  0.9467667 Val_loss =  0.3539578 Val_acc =  0.8844\n",
            "Iteration  711 : Loss =  0.15369697  Acc:  0.94671667 Val_loss =  0.35413772 Val_acc =  0.8846\n",
            "Iteration  712 : Loss =  0.15532511  Acc:  0.9461 Val_loss =  0.35750782 Val_acc =  0.8825\n",
            "Iteration  713 : Loss =  0.15692988  Acc:  0.9450333 Val_loss =  0.35804856 Val_acc =  0.8836\n",
            "Iteration  714 : Loss =  0.15706168  Acc:  0.945 Val_loss =  0.3603985 Val_acc =  0.8817\n",
            "Iteration  715 : Loss =  0.15637201  Acc:  0.9455 Val_loss =  0.35834342 Val_acc =  0.8841\n",
            "Iteration  716 : Loss =  0.15420045  Acc:  0.9465333 Val_loss =  0.35712245 Val_acc =  0.8825\n",
            "Iteration  717 : Loss =  0.15233064  Acc:  0.94783336 Val_loss =  0.35508952 Val_acc =  0.8851\n",
            "Iteration  718 : Loss =  0.15156615  Acc:  0.9479333 Val_loss =  0.3542127 Val_acc =  0.8849\n",
            "Iteration  719 : Loss =  0.15202568  Acc:  0.94815 Val_loss =  0.35570145 Val_acc =  0.885\n",
            "Iteration  720 : Loss =  0.15315272  Acc:  0.94691664 Val_loss =  0.3559097 Val_acc =  0.8848\n",
            "Iteration  721 : Loss =  0.1537646  Acc:  0.94685 Val_loss =  0.35849485 Val_acc =  0.8831\n",
            "Iteration  722 : Loss =  0.15406214  Acc:  0.9463 Val_loss =  0.3578247 Val_acc =  0.8844\n",
            "Iteration  723 : Loss =  0.15302472  Acc:  0.9471667 Val_loss =  0.35847044 Val_acc =  0.8834\n",
            "Iteration  724 : Loss =  0.15182777  Acc:  0.94771665 Val_loss =  0.35644794 Val_acc =  0.8844\n",
            "Iteration  725 : Loss =  0.15058695  Acc:  0.9482833 Val_loss =  0.3557987 Val_acc =  0.8844\n",
            "Iteration  726 : Loss =  0.15007715  Acc:  0.9487333 Val_loss =  0.355519 Val_acc =  0.8834\n",
            "Iteration  727 : Loss =  0.15025766  Acc:  0.94836664 Val_loss =  0.35560796 Val_acc =  0.8844\n",
            "Iteration  728 : Loss =  0.15082969  Acc:  0.9482167 Val_loss =  0.3572356 Val_acc =  0.8834\n",
            "Iteration  729 : Loss =  0.15139945  Acc:  0.94773334 Val_loss =  0.3572988 Val_acc =  0.885\n",
            "Iteration  730 : Loss =  0.15156175  Acc:  0.94775 Val_loss =  0.3588109 Val_acc =  0.8825\n",
            "Iteration  731 : Loss =  0.1513834  Acc:  0.9477 Val_loss =  0.35772982 Val_acc =  0.8853\n",
            "Iteration  732 : Loss =  0.15082614  Acc:  0.94811666 Val_loss =  0.3582196 Val_acc =  0.8824\n",
            "Iteration  733 : Loss =  0.15013544  Acc:  0.94843334 Val_loss =  0.35731828 Val_acc =  0.8848\n",
            "Iteration  734 : Loss =  0.14995636  Acc:  0.94845 Val_loss =  0.35759732 Val_acc =  0.883\n",
            "Iteration  735 : Loss =  0.15038474  Acc:  0.9486667 Val_loss =  0.35860375 Val_acc =  0.8847\n",
            "Iteration  736 : Loss =  0.15207793  Acc:  0.94696665 Val_loss =  0.36007008 Val_acc =  0.8829\n",
            "Iteration  737 : Loss =  0.15517582  Acc:  0.94621664 Val_loss =  0.3649133 Val_acc =  0.8808\n",
            "Iteration  738 : Loss =  0.16106208  Acc:  0.9423 Val_loss =  0.3695007 Val_acc =  0.8801\n",
            "Iteration  739 : Loss =  0.16783682  Acc:  0.9395 Val_loss =  0.37894768 Val_acc =  0.8779\n",
            "Iteration  740 : Loss =  0.1766681  Acc:  0.9346333 Val_loss =  0.3857959 Val_acc =  0.8763\n",
            "Iteration  741 : Loss =  0.1728872  Acc:  0.9364667 Val_loss =  0.38432285 Val_acc =  0.876\n",
            "Iteration  742 : Loss =  0.16457076  Acc:  0.94046664 Val_loss =  0.3727228 Val_acc =  0.8798\n",
            "Iteration  743 : Loss =  0.15129004  Acc:  0.9479667 Val_loss =  0.35998908 Val_acc =  0.8839\n",
            "Iteration  744 : Loss =  0.1526957  Acc:  0.94715 Val_loss =  0.36259466 Val_acc =  0.8828\n",
            "Iteration  745 : Loss =  0.16018932  Acc:  0.94276667 Val_loss =  0.3691015 Val_acc =  0.8798\n",
            "Iteration  746 : Loss =  0.15881118  Acc:  0.94421667 Val_loss =  0.37107843 Val_acc =  0.8803\n",
            "Iteration  747 : Loss =  0.1520236  Acc:  0.9467 Val_loss =  0.36248064 Val_acc =  0.8827\n",
            "Iteration  748 : Loss =  0.14944322  Acc:  0.94838333 Val_loss =  0.36028445 Val_acc =  0.8848\n",
            "Iteration  749 : Loss =  0.15359169  Acc:  0.94701666 Val_loss =  0.36679444 Val_acc =  0.8825\n",
            "Iteration  750 : Loss =  0.15617383  Acc:  0.94451666 Val_loss =  0.36739543 Val_acc =  0.8816\n",
            "Iteration  751 : Loss =  0.15090476  Acc:  0.94825 Val_loss =  0.36470947 Val_acc =  0.883\n",
            "Iteration  752 : Loss =  0.14701995  Acc:  0.9492 Val_loss =  0.35950118 Val_acc =  0.8837\n",
            "Iteration  753 : Loss =  0.14913991  Acc:  0.94846666 Val_loss =  0.36083788 Val_acc =  0.8847\n",
            "Iteration  754 : Loss =  0.15184437  Acc:  0.94755 Val_loss =  0.36624506 Val_acc =  0.882\n",
            "Iteration  755 : Loss =  0.15122046  Acc:  0.94715 Val_loss =  0.36325982 Val_acc =  0.8838\n",
            "Iteration  756 : Loss =  0.14687866  Acc:  0.94965 Val_loss =  0.36097187 Val_acc =  0.8848\n",
            "Iteration  757 : Loss =  0.14561208  Acc:  0.95061666 Val_loss =  0.35922423 Val_acc =  0.8857\n",
            "Iteration  758 : Loss =  0.14778487  Acc:  0.94885 Val_loss =  0.3606647 Val_acc =  0.8841\n",
            "Iteration  759 : Loss =  0.14892636  Acc:  0.94893336 Val_loss =  0.3640865 Val_acc =  0.8836\n",
            "Iteration  760 : Loss =  0.14771233  Acc:  0.9490167 Val_loss =  0.3612918 Val_acc =  0.8845\n",
            "Iteration  761 : Loss =  0.1454573  Acc:  0.9504667 Val_loss =  0.36078334 Val_acc =  0.8845\n",
            "Iteration  762 : Loss =  0.14538066  Acc:  0.9504333 Val_loss =  0.36091188 Val_acc =  0.885\n",
            "Iteration  763 : Loss =  0.1467191  Acc:  0.9489667 Val_loss =  0.3619411 Val_acc =  0.8825\n",
            "Iteration  764 : Loss =  0.1468653  Acc:  0.94991666 Val_loss =  0.36320758 Val_acc =  0.8844\n",
            "Iteration  765 : Loss =  0.14550029  Acc:  0.94995 Val_loss =  0.36097008 Val_acc =  0.8833\n",
            "Iteration  766 : Loss =  0.14412072  Acc:  0.9515833 Val_loss =  0.36035374 Val_acc =  0.8846\n",
            "Iteration  767 : Loss =  0.14411648  Acc:  0.9514 Val_loss =  0.3610264 Val_acc =  0.885\n",
            "Iteration  768 : Loss =  0.1448787  Acc:  0.9500667 Val_loss =  0.36095956 Val_acc =  0.8837\n",
            "Iteration  769 : Loss =  0.14503427  Acc:  0.95066667 Val_loss =  0.36267412 Val_acc =  0.8846\n",
            "Iteration  770 : Loss =  0.14431858  Acc:  0.95053333 Val_loss =  0.36095983 Val_acc =  0.8832\n",
            "Iteration  771 : Loss =  0.14337988  Acc:  0.95173335 Val_loss =  0.36099783 Val_acc =  0.8848\n",
            "Iteration  772 : Loss =  0.1431359  Acc:  0.9518667 Val_loss =  0.36105886 Val_acc =  0.8843\n",
            "Iteration  773 : Loss =  0.14350072  Acc:  0.95091665 Val_loss =  0.36105537 Val_acc =  0.8839\n",
            "Iteration  774 : Loss =  0.14379399  Acc:  0.9514833 Val_loss =  0.36271337 Val_acc =  0.8853\n",
            "Iteration  775 : Loss =  0.14369577  Acc:  0.95065 Val_loss =  0.36133516 Val_acc =  0.8841\n",
            "Iteration  776 : Loss =  0.14316794  Acc:  0.95201665 Val_loss =  0.36256167 Val_acc =  0.8842\n",
            "Iteration  777 : Loss =  0.14288959  Acc:  0.95163333 Val_loss =  0.3617281 Val_acc =  0.8838\n",
            "Iteration  778 : Loss =  0.14306235  Acc:  0.9514667 Val_loss =  0.36278284 Val_acc =  0.8833\n",
            "Iteration  779 : Loss =  0.14355244  Acc:  0.95105 Val_loss =  0.3630237 Val_acc =  0.8846\n",
            "Iteration  780 : Loss =  0.14415559  Acc:  0.95045 Val_loss =  0.3642328 Val_acc =  0.8823\n",
            "Iteration  781 : Loss =  0.14433697  Acc:  0.9510667 Val_loss =  0.3643694 Val_acc =  0.8837\n",
            "Iteration  782 : Loss =  0.14482084  Acc:  0.9502 Val_loss =  0.36561465 Val_acc =  0.882\n",
            "Iteration  783 : Loss =  0.14497575  Acc:  0.95021665 Val_loss =  0.36513212 Val_acc =  0.8838\n",
            "Iteration  784 : Loss =  0.14611271  Acc:  0.94913334 Val_loss =  0.36783892 Val_acc =  0.8823\n",
            "Iteration  785 : Loss =  0.14670326  Acc:  0.9487 Val_loss =  0.36707163 Val_acc =  0.883\n",
            "Iteration  786 : Loss =  0.14764582  Acc:  0.94811666 Val_loss =  0.36970153 Val_acc =  0.8813\n",
            "Iteration  787 : Loss =  0.14696735  Acc:  0.94878334 Val_loss =  0.3679196 Val_acc =  0.8831\n",
            "Iteration  788 : Loss =  0.14600651  Acc:  0.94881666 Val_loss =  0.3684737 Val_acc =  0.8821\n",
            "Iteration  789 : Loss =  0.14369817  Acc:  0.95096666 Val_loss =  0.36562213 Val_acc =  0.884\n",
            "Iteration  790 : Loss =  0.14220163  Acc:  0.95128334 Val_loss =  0.36441815 Val_acc =  0.8834\n",
            "Iteration  791 : Loss =  0.14111525  Acc:  0.95233333 Val_loss =  0.36398995 Val_acc =  0.8849\n",
            "Iteration  792 : Loss =  0.14078283  Acc:  0.95246667 Val_loss =  0.3634859 Val_acc =  0.884\n",
            "Iteration  793 : Loss =  0.14075863  Acc:  0.9527 Val_loss =  0.3645951 Val_acc =  0.8859\n",
            "Iteration  794 : Loss =  0.14094651  Acc:  0.9523 Val_loss =  0.3646058 Val_acc =  0.884\n",
            "Iteration  795 : Loss =  0.14123301  Acc:  0.9518333 Val_loss =  0.36567643 Val_acc =  0.884\n",
            "Iteration  796 : Loss =  0.14170146  Acc:  0.9518167 Val_loss =  0.36588207 Val_acc =  0.8846\n",
            "Iteration  797 : Loss =  0.14286174  Acc:  0.95098335 Val_loss =  0.3675063 Val_acc =  0.882\n",
            "Iteration  798 : Loss =  0.14335254  Acc:  0.95115 Val_loss =  0.36830774 Val_acc =  0.8831\n",
            "Iteration  799 : Loss =  0.14490098  Acc:  0.9493167 Val_loss =  0.37028593 Val_acc =  0.8823\n",
            "Iteration  800 : Loss =  0.1444457  Acc:  0.9503833 Val_loss =  0.36962602 Val_acc =  0.8825\n",
            "Iteration  801 : Loss =  0.14476746  Acc:  0.94956666 Val_loss =  0.37107494 Val_acc =  0.8825\n",
            "Iteration  802 : Loss =  0.14313577  Acc:  0.9507167 Val_loss =  0.36858 Val_acc =  0.8825\n",
            "Iteration  803 : Loss =  0.14176425  Acc:  0.95171666 Val_loss =  0.36860183 Val_acc =  0.8826\n",
            "Iteration  804 : Loss =  0.14002907  Acc:  0.95236665 Val_loss =  0.365726 Val_acc =  0.8839\n",
            "Iteration  805 : Loss =  0.13895854  Acc:  0.95336664 Val_loss =  0.3664835 Val_acc =  0.884\n",
            "Iteration  806 : Loss =  0.13869762  Acc:  0.9532833 Val_loss =  0.36538848 Val_acc =  0.8834\n",
            "Iteration  807 : Loss =  0.13893132  Acc:  0.9535 Val_loss =  0.36647677 Val_acc =  0.8855\n",
            "Iteration  808 : Loss =  0.13943502  Acc:  0.95246667 Val_loss =  0.36662066 Val_acc =  0.8829\n",
            "Iteration  809 : Loss =  0.13973916  Acc:  0.95305 Val_loss =  0.36749637 Val_acc =  0.884\n",
            "Iteration  810 : Loss =  0.14019014  Acc:  0.95233333 Val_loss =  0.36869377 Val_acc =  0.8826\n",
            "Iteration  811 : Loss =  0.14014512  Acc:  0.95248336 Val_loss =  0.3681319 Val_acc =  0.8839\n",
            "Iteration  812 : Loss =  0.14043278  Acc:  0.9522 Val_loss =  0.3696217 Val_acc =  0.883\n",
            "Iteration  813 : Loss =  0.1402055  Acc:  0.9524 Val_loss =  0.36842176 Val_acc =  0.8834\n",
            "Iteration  814 : Loss =  0.14022142  Acc:  0.95238334 Val_loss =  0.37011072 Val_acc =  0.8826\n",
            "Iteration  815 : Loss =  0.13992995  Acc:  0.95196664 Val_loss =  0.36879307 Val_acc =  0.8845\n",
            "Iteration  816 : Loss =  0.13954547  Acc:  0.9525333 Val_loss =  0.36976972 Val_acc =  0.8828\n",
            "Iteration  817 : Loss =  0.13896371  Acc:  0.95235 Val_loss =  0.36875248 Val_acc =  0.8847\n",
            "Iteration  818 : Loss =  0.1380372  Acc:  0.95341665 Val_loss =  0.3685019 Val_acc =  0.8846\n",
            "Iteration  819 : Loss =  0.13716066  Acc:  0.9538 Val_loss =  0.367489 Val_acc =  0.885\n",
            "Iteration  820 : Loss =  0.13643286  Acc:  0.9546 Val_loss =  0.36697006 Val_acc =  0.8847\n",
            "Iteration  821 : Loss =  0.13604034  Acc:  0.95505 Val_loss =  0.36752757 Val_acc =  0.8843\n",
            "Iteration  822 : Loss =  0.13601518  Acc:  0.95456666 Val_loss =  0.3672609 Val_acc =  0.8849\n",
            "Iteration  823 : Loss =  0.13625516  Acc:  0.9548 Val_loss =  0.36857477 Val_acc =  0.8846\n",
            "Iteration  824 : Loss =  0.1367282  Acc:  0.95358336 Val_loss =  0.36816 Val_acc =  0.8845\n",
            "Iteration  825 : Loss =  0.13724834  Acc:  0.95376664 Val_loss =  0.3703919 Val_acc =  0.8838\n",
            "Iteration  826 : Loss =  0.13796732  Acc:  0.9526333 Val_loss =  0.36966464 Val_acc =  0.884\n",
            "Iteration  827 : Loss =  0.13841693  Acc:  0.95318335 Val_loss =  0.3721313 Val_acc =  0.8828\n",
            "Iteration  828 : Loss =  0.13900027  Acc:  0.9522833 Val_loss =  0.37088618 Val_acc =  0.8834\n",
            "Iteration  829 : Loss =  0.13918285  Acc:  0.95278335 Val_loss =  0.3735314 Val_acc =  0.8824\n",
            "Iteration  830 : Loss =  0.13911104  Acc:  0.9521833 Val_loss =  0.37139305 Val_acc =  0.8835\n",
            "Iteration  831 : Loss =  0.13875161  Acc:  0.9529833 Val_loss =  0.37349254 Val_acc =  0.8828\n",
            "Iteration  832 : Loss =  0.13796513  Acc:  0.9529833 Val_loss =  0.3713368 Val_acc =  0.8828\n",
            "Iteration  833 : Loss =  0.13767503  Acc:  0.95318335 Val_loss =  0.3726911 Val_acc =  0.8829\n",
            "Iteration  834 : Loss =  0.13719217  Acc:  0.95343333 Val_loss =  0.37183136 Val_acc =  0.8834\n",
            "Iteration  835 : Loss =  0.13759297  Acc:  0.95265 Val_loss =  0.3723625 Val_acc =  0.882\n",
            "Iteration  836 : Loss =  0.13822503  Acc:  0.9532167 Val_loss =  0.37433386 Val_acc =  0.8821\n",
            "Iteration  837 : Loss =  0.14005095  Acc:  0.9515167 Val_loss =  0.37438053 Val_acc =  0.8823\n",
            "Iteration  838 : Loss =  0.14172962  Acc:  0.95133334 Val_loss =  0.37943447 Val_acc =  0.8805\n",
            "Iteration  839 : Loss =  0.1441667  Acc:  0.9496667 Val_loss =  0.37846828 Val_acc =  0.8807\n",
            "Iteration  840 : Loss =  0.14389826  Acc:  0.95061666 Val_loss =  0.38260958 Val_acc =  0.8807\n",
            "Iteration  841 : Loss =  0.14231515  Acc:  0.95035 Val_loss =  0.37672105 Val_acc =  0.8825\n",
            "Iteration  842 : Loss =  0.1375729  Acc:  0.95358336 Val_loss =  0.37560117 Val_acc =  0.8829\n",
            "Iteration  843 : Loss =  0.13387685  Acc:  0.95505 Val_loss =  0.36971173 Val_acc =  0.8837\n",
            "Iteration  844 : Loss =  0.13330355  Acc:  0.95533335 Val_loss =  0.36995167 Val_acc =  0.8849\n",
            "Iteration  845 : Loss =  0.13548292  Acc:  0.9546833 Val_loss =  0.37438595 Val_acc =  0.8833\n",
            "Iteration  846 : Loss =  0.1383057  Acc:  0.95208335 Val_loss =  0.3750941 Val_acc =  0.8818\n",
            "Iteration  847 : Loss =  0.13902728  Acc:  0.953 Val_loss =  0.37946704 Val_acc =  0.8803\n",
            "Iteration  848 : Loss =  0.13790186  Acc:  0.95225 Val_loss =  0.37505645 Val_acc =  0.8831\n",
            "Iteration  849 : Loss =  0.13490072  Acc:  0.9549 Val_loss =  0.37468892 Val_acc =  0.8824\n",
            "Iteration  850 : Loss =  0.13281709  Acc:  0.9551833 Val_loss =  0.37132612 Val_acc =  0.8839\n",
            "Iteration  851 : Loss =  0.13288768  Acc:  0.95535 Val_loss =  0.37101406 Val_acc =  0.8851\n",
            "Iteration  852 : Loss =  0.1345668  Acc:  0.9547833 Val_loss =  0.37540585 Val_acc =  0.8831\n",
            "Iteration  853 : Loss =  0.13648897  Acc:  0.95276666 Val_loss =  0.37436783 Val_acc =  0.8842\n",
            "Iteration  854 : Loss =  0.136778  Acc:  0.95395 Val_loss =  0.3785516 Val_acc =  0.8826\n",
            "Iteration  855 : Loss =  0.13620263  Acc:  0.95288336 Val_loss =  0.37482238 Val_acc =  0.8842\n",
            "Iteration  856 : Loss =  0.1346273  Acc:  0.9546 Val_loss =  0.37604007 Val_acc =  0.8831\n",
            "Iteration  857 : Loss =  0.13426794  Acc:  0.9544167 Val_loss =  0.37471643 Val_acc =  0.8832\n",
            "Iteration  858 : Loss =  0.13589528  Acc:  0.95335 Val_loss =  0.37620482 Val_acc =  0.8823\n",
            "Iteration  859 : Loss =  0.1366228  Acc:  0.95325 Val_loss =  0.3789121 Val_acc =  0.8828\n",
            "Iteration  860 : Loss =  0.13848709  Acc:  0.9518 Val_loss =  0.37895826 Val_acc =  0.8819\n",
            "Iteration  861 : Loss =  0.13619833  Acc:  0.95341665 Val_loss =  0.37897518 Val_acc =  0.883\n",
            "Iteration  862 : Loss =  0.13497977  Acc:  0.95345 Val_loss =  0.37642297 Val_acc =  0.8826\n",
            "Iteration  863 : Loss =  0.13334198  Acc:  0.95521665 Val_loss =  0.37513965 Val_acc =  0.8836\n",
            "Iteration  864 : Loss =  0.13357851  Acc:  0.95526665 Val_loss =  0.37684718 Val_acc =  0.8832\n",
            "Iteration  865 : Loss =  0.13415127  Acc:  0.9543833 Val_loss =  0.37539652 Val_acc =  0.8838\n",
            "Iteration  866 : Loss =  0.13352275  Acc:  0.95523334 Val_loss =  0.37787825 Val_acc =  0.8828\n",
            "Iteration  867 : Loss =  0.13171816  Acc:  0.95531666 Val_loss =  0.37370017 Val_acc =  0.8844\n",
            "Iteration  868 : Loss =  0.12995654  Acc:  0.95741665 Val_loss =  0.3737344 Val_acc =  0.8845\n",
            "Iteration  869 : Loss =  0.12983045  Acc:  0.9572333 Val_loss =  0.37377465 Val_acc =  0.8849\n",
            "Iteration  870 : Loss =  0.13100658  Acc:  0.95628333 Val_loss =  0.3740825 Val_acc =  0.8844\n",
            "Iteration  871 : Loss =  0.13203532  Acc:  0.9558833 Val_loss =  0.37736842 Val_acc =  0.8836\n",
            "Iteration  872 : Loss =  0.1321239  Acc:  0.9556 Val_loss =  0.37559128 Val_acc =  0.8843\n",
            "Iteration  873 : Loss =  0.1314379  Acc:  0.9561167 Val_loss =  0.37703556 Val_acc =  0.8837\n",
            "Iteration  874 : Loss =  0.13086994  Acc:  0.95673335 Val_loss =  0.37561393 Val_acc =  0.8838\n",
            "Iteration  875 : Loss =  0.1316273  Acc:  0.95558333 Val_loss =  0.37637055 Val_acc =  0.8828\n",
            "Iteration  876 : Loss =  0.1324639  Acc:  0.95533335 Val_loss =  0.37868986 Val_acc =  0.8827\n",
            "Iteration  877 : Loss =  0.13436389  Acc:  0.9538 Val_loss =  0.37951124 Val_acc =  0.8834\n",
            "Iteration  878 : Loss =  0.13352051  Acc:  0.95453334 Val_loss =  0.38017175 Val_acc =  0.8834\n",
            "Iteration  879 : Loss =  0.1337373  Acc:  0.95375 Val_loss =  0.3795767 Val_acc =  0.8827\n",
            "Iteration  880 : Loss =  0.13274828  Acc:  0.9543333 Val_loss =  0.37845433 Val_acc =  0.8833\n",
            "Iteration  881 : Loss =  0.13326633  Acc:  0.9547833 Val_loss =  0.38077843 Val_acc =  0.8831\n",
            "Iteration  882 : Loss =  0.13351914  Acc:  0.9540667 Val_loss =  0.37837476 Val_acc =  0.884\n",
            "Iteration  883 : Loss =  0.13245691  Acc:  0.95525 Val_loss =  0.38089544 Val_acc =  0.8836\n",
            "Iteration  884 : Loss =  0.1306597  Acc:  0.9554 Val_loss =  0.37664855 Val_acc =  0.8842\n",
            "Iteration  885 : Loss =  0.12889208  Acc:  0.9573 Val_loss =  0.37717208 Val_acc =  0.8842\n",
            "Iteration  886 : Loss =  0.12843055  Acc:  0.95703334 Val_loss =  0.3764658 Val_acc =  0.8847\n",
            "Iteration  887 : Loss =  0.12874906  Acc:  0.9571667 Val_loss =  0.37657887 Val_acc =  0.8856\n",
            "Iteration  888 : Loss =  0.1289282  Acc:  0.95708334 Val_loss =  0.37839848 Val_acc =  0.8841\n",
            "Iteration  889 : Loss =  0.12872791  Acc:  0.95695 Val_loss =  0.37718618 Val_acc =  0.8845\n",
            "Iteration  890 : Loss =  0.1283769  Acc:  0.9573333 Val_loss =  0.37764814 Val_acc =  0.8843\n",
            "Iteration  891 : Loss =  0.12849848  Acc:  0.9579333 Val_loss =  0.3774851 Val_acc =  0.8839\n",
            "Iteration  892 : Loss =  0.12961735  Acc:  0.95636666 Val_loss =  0.37867168 Val_acc =  0.883\n",
            "Iteration  893 : Loss =  0.13033198  Acc:  0.95626664 Val_loss =  0.38093537 Val_acc =  0.8827\n",
            "Iteration  894 : Loss =  0.13204215  Acc:  0.9547333 Val_loss =  0.38195685 Val_acc =  0.8834\n",
            "Iteration  895 : Loss =  0.13148868  Acc:  0.95531666 Val_loss =  0.38230106 Val_acc =  0.8829\n",
            "Iteration  896 : Loss =  0.13239565  Acc:  0.9541 Val_loss =  0.38305393 Val_acc =  0.8824\n",
            "Iteration  897 : Loss =  0.13165022  Acc:  0.9547 Val_loss =  0.38176772 Val_acc =  0.8825\n",
            "Iteration  898 : Loss =  0.13201338  Acc:  0.95465 Val_loss =  0.38425875 Val_acc =  0.8832\n",
            "Iteration  899 : Loss =  0.13102736  Acc:  0.9548 Val_loss =  0.3805948 Val_acc =  0.8841\n",
            "Iteration  900 : Loss =  0.12912329  Acc:  0.9564833 Val_loss =  0.38139847 Val_acc =  0.8838\n",
            "Iteration  901 : Loss =  0.12707134  Acc:  0.95703334 Val_loss =  0.3775175 Val_acc =  0.8846\n",
            "Iteration  902 : Loss =  0.12602703  Acc:  0.95886666 Val_loss =  0.37811917 Val_acc =  0.8851\n",
            "Iteration  903 : Loss =  0.12641518  Acc:  0.9579167 Val_loss =  0.37902567 Val_acc =  0.8847\n",
            "Iteration  904 : Loss =  0.12746757  Acc:  0.9572 Val_loss =  0.37973824 Val_acc =  0.8843\n",
            "Iteration  905 : Loss =  0.12847798  Acc:  0.9566 Val_loss =  0.38191998 Val_acc =  0.8828\n",
            "Iteration  906 : Loss =  0.12925707  Acc:  0.95631665 Val_loss =  0.3823454 Val_acc =  0.8828\n",
            "Iteration  907 : Loss =  0.13074765  Acc:  0.9550167 Val_loss =  0.38390836 Val_acc =  0.8822\n",
            "Iteration  908 : Loss =  0.1317415  Acc:  0.9548333 Val_loss =  0.3862741 Val_acc =  0.8811\n",
            "Iteration  909 : Loss =  0.13430281  Acc:  0.9537 Val_loss =  0.38723564 Val_acc =  0.8805\n",
            "Iteration  910 : Loss =  0.13397737  Acc:  0.95381665 Val_loss =  0.38998136 Val_acc =  0.881\n",
            "Iteration  911 : Loss =  0.13406242  Acc:  0.9536667 Val_loss =  0.38705134 Val_acc =  0.8803\n",
            "Iteration  912 : Loss =  0.13197197  Acc:  0.9552 Val_loss =  0.3878856 Val_acc =  0.8805\n",
            "Iteration  913 : Loss =  0.13084537  Acc:  0.95485 Val_loss =  0.38448632 Val_acc =  0.8829\n",
            "Iteration  914 : Loss =  0.129864  Acc:  0.9559 Val_loss =  0.3846929 Val_acc =  0.8842\n",
            "Iteration  915 : Loss =  0.13036053  Acc:  0.95558333 Val_loss =  0.38544193 Val_acc =  0.8832\n",
            "Iteration  916 : Loss =  0.130423  Acc:  0.95505 Val_loss =  0.38579103 Val_acc =  0.8832\n",
            "Iteration  917 : Loss =  0.12914775  Acc:  0.9562 Val_loss =  0.38500476 Val_acc =  0.8823\n",
            "Iteration  918 : Loss =  0.13003947  Acc:  0.95535 Val_loss =  0.3868047 Val_acc =  0.8833\n",
            "Iteration  919 : Loss =  0.12977111  Acc:  0.9562 Val_loss =  0.38575548 Val_acc =  0.8816\n",
            "Iteration  920 : Loss =  0.1295362  Acc:  0.95498335 Val_loss =  0.3867183 Val_acc =  0.8822\n",
            "Iteration  921 : Loss =  0.12782148  Acc:  0.95671666 Val_loss =  0.38404045 Val_acc =  0.8844\n",
            "Iteration  922 : Loss =  0.12627418  Acc:  0.9575167 Val_loss =  0.38280743 Val_acc =  0.8837\n",
            "Iteration  923 : Loss =  0.12641197  Acc:  0.95753336 Val_loss =  0.38483316 Val_acc =  0.8834\n",
            "Iteration  924 : Loss =  0.12851724  Acc:  0.95563334 Val_loss =  0.38418856 Val_acc =  0.8841\n",
            "Iteration  925 : Loss =  0.12937  Acc:  0.9557833 Val_loss =  0.38850036 Val_acc =  0.8833\n",
            "Iteration  926 : Loss =  0.128528  Acc:  0.9555167 Val_loss =  0.38484082 Val_acc =  0.8823\n",
            "Iteration  927 : Loss =  0.12725493  Acc:  0.95643336 Val_loss =  0.38550442 Val_acc =  0.8829\n",
            "Iteration  928 : Loss =  0.12547895  Acc:  0.95788336 Val_loss =  0.38365835 Val_acc =  0.8837\n",
            "Iteration  929 : Loss =  0.12448337  Acc:  0.9585 Val_loss =  0.3822695 Val_acc =  0.8839\n",
            "Iteration  930 : Loss =  0.12307163  Acc:  0.95995 Val_loss =  0.38253355 Val_acc =  0.8843\n",
            "Iteration  931 : Loss =  0.12243901  Acc:  0.95933336 Val_loss =  0.38098794 Val_acc =  0.884\n",
            "Iteration  932 : Loss =  0.12299736  Acc:  0.95945 Val_loss =  0.38274428 Val_acc =  0.8835\n",
            "Iteration  933 : Loss =  0.12414707  Acc:  0.9584 Val_loss =  0.38410956 Val_acc =  0.884\n",
            "Iteration  934 : Loss =  0.12519339  Acc:  0.95771664 Val_loss =  0.38531053 Val_acc =  0.8843\n",
            "Iteration  935 : Loss =  0.12497333  Acc:  0.9576333 Val_loss =  0.38506308 Val_acc =  0.8833\n",
            "Iteration  936 : Loss =  0.124358505  Acc:  0.9583833 Val_loss =  0.38502178 Val_acc =  0.8835\n",
            "Iteration  937 : Loss =  0.123538636  Acc:  0.95845 Val_loss =  0.3830614 Val_acc =  0.8847\n",
            "Iteration  938 : Loss =  0.12282285  Acc:  0.96005 Val_loss =  0.38421592 Val_acc =  0.8838\n",
            "Iteration  939 : Loss =  0.12221193  Acc:  0.95885 Val_loss =  0.38210902 Val_acc =  0.8845\n",
            "Iteration  940 : Loss =  0.12163497  Acc:  0.96038336 Val_loss =  0.38376403 Val_acc =  0.8841\n",
            "Iteration  941 : Loss =  0.12131602  Acc:  0.96015 Val_loss =  0.38278526 Val_acc =  0.8845\n",
            "Iteration  942 : Loss =  0.121420555  Acc:  0.96021664 Val_loss =  0.3835008 Val_acc =  0.8838\n",
            "Iteration  943 : Loss =  0.12190652  Acc:  0.9601 Val_loss =  0.3845425 Val_acc =  0.8837\n",
            "Iteration  944 : Loss =  0.122216016  Acc:  0.9594667 Val_loss =  0.3846941 Val_acc =  0.8842\n",
            "Iteration  945 : Loss =  0.12243156  Acc:  0.95966667 Val_loss =  0.38597587 Val_acc =  0.8837\n",
            "Iteration  946 : Loss =  0.12211918  Acc:  0.95963335 Val_loss =  0.38497904 Val_acc =  0.8837\n",
            "Iteration  947 : Loss =  0.121936746  Acc:  0.95993334 Val_loss =  0.38538927 Val_acc =  0.8839\n",
            "Iteration  948 : Loss =  0.121505305  Acc:  0.96021664 Val_loss =  0.38477284 Val_acc =  0.8836\n",
            "Iteration  949 : Loss =  0.12129669  Acc:  0.96021664 Val_loss =  0.38439137 Val_acc =  0.8836\n",
            "Iteration  950 : Loss =  0.12066632  Acc:  0.9605833 Val_loss =  0.38475743 Val_acc =  0.8831\n",
            "Iteration  951 : Loss =  0.12019777  Acc:  0.96056664 Val_loss =  0.38440394 Val_acc =  0.8844\n",
            "Iteration  952 : Loss =  0.11970893  Acc:  0.9608333 Val_loss =  0.38446105 Val_acc =  0.884\n",
            "Iteration  953 : Loss =  0.11939818  Acc:  0.96105 Val_loss =  0.3843828 Val_acc =  0.8842\n",
            "Iteration  954 : Loss =  0.11930023  Acc:  0.9609 Val_loss =  0.38423792 Val_acc =  0.8846\n",
            "Iteration  955 : Loss =  0.119230695  Acc:  0.9612667 Val_loss =  0.38469347 Val_acc =  0.8842\n",
            "Iteration  956 : Loss =  0.11913436  Acc:  0.9611 Val_loss =  0.38464552 Val_acc =  0.8838\n",
            "Iteration  957 : Loss =  0.119062  Acc:  0.9612833 Val_loss =  0.38514546 Val_acc =  0.8843\n",
            "Iteration  958 : Loss =  0.119021855  Acc:  0.96146667 Val_loss =  0.3855084 Val_acc =  0.8842\n",
            "Iteration  959 : Loss =  0.11912008  Acc:  0.96078336 Val_loss =  0.38521114 Val_acc =  0.8835\n",
            "Iteration  960 : Loss =  0.11946354  Acc:  0.9613 Val_loss =  0.38661665 Val_acc =  0.8836\n",
            "Iteration  961 : Loss =  0.120124206  Acc:  0.9597833 Val_loss =  0.38628232 Val_acc =  0.8835\n",
            "Iteration  962 : Loss =  0.12129318  Acc:  0.9598 Val_loss =  0.38968498 Val_acc =  0.8831\n",
            "Iteration  963 : Loss =  0.123891175  Acc:  0.95753336 Val_loss =  0.3902441 Val_acc =  0.8839\n",
            "Iteration  964 : Loss =  0.12733887  Acc:  0.95661664 Val_loss =  0.39694083 Val_acc =  0.8823\n",
            "Iteration  965 : Loss =  0.13526212  Acc:  0.9514 Val_loss =  0.40145537 Val_acc =  0.88\n",
            "Iteration  966 : Loss =  0.13868544  Acc:  0.9508167 Val_loss =  0.40887955 Val_acc =  0.8787\n",
            "Iteration  967 : Loss =  0.14463992  Acc:  0.94768333 Val_loss =  0.41107652 Val_acc =  0.8776\n",
            "Iteration  968 : Loss =  0.13132878  Acc:  0.95481664 Val_loss =  0.40079853 Val_acc =  0.8796\n",
            "Iteration  969 : Loss =  0.12274645  Acc:  0.9583833 Val_loss =  0.3891858 Val_acc =  0.8839\n",
            "Iteration  970 : Loss =  0.12661715  Acc:  0.95665 Val_loss =  0.39346972 Val_acc =  0.8823\n",
            "Iteration  971 : Loss =  0.13649632  Acc:  0.9508167 Val_loss =  0.4058219 Val_acc =  0.8825\n",
            "Iteration  972 : Loss =  0.13522132  Acc:  0.95166665 Val_loss =  0.4025993 Val_acc =  0.8787\n",
            "Iteration  973 : Loss =  0.12663409  Acc:  0.95615 Val_loss =  0.3981972 Val_acc =  0.8832\n",
            "Iteration  974 : Loss =  0.12143014  Acc:  0.95926666 Val_loss =  0.39066964 Val_acc =  0.884\n",
            "Iteration  975 : Loss =  0.12419155  Acc:  0.9572833 Val_loss =  0.39403582 Val_acc =  0.8832\n",
            "Iteration  976 : Loss =  0.12667081  Acc:  0.9573333 Val_loss =  0.39823875 Val_acc =  0.8817\n",
            "Iteration  977 : Loss =  0.12766501  Acc:  0.95481664 Val_loss =  0.39624614 Val_acc =  0.883\n",
            "Iteration  978 : Loss =  0.12225617  Acc:  0.95985 Val_loss =  0.3953617 Val_acc =  0.8828\n",
            "Iteration  979 : Loss =  0.118588604  Acc:  0.95998335 Val_loss =  0.38833073 Val_acc =  0.8858\n",
            "Iteration  980 : Loss =  0.11881581  Acc:  0.9605167 Val_loss =  0.38930026 Val_acc =  0.8841\n",
            "Iteration  981 : Loss =  0.121160924  Acc:  0.9597333 Val_loss =  0.3929594 Val_acc =  0.8843\n",
            "Iteration  982 : Loss =  0.123136364  Acc:  0.9575667 Val_loss =  0.3927314 Val_acc =  0.8836\n",
            "Iteration  983 : Loss =  0.121043935  Acc:  0.9604833 Val_loss =  0.39483106 Val_acc =  0.8833\n",
            "Iteration  984 : Loss =  0.11844314  Acc:  0.96001667 Val_loss =  0.39005756 Val_acc =  0.8834\n",
            "Iteration  985 : Loss =  0.1174396  Acc:  0.9622167 Val_loss =  0.3909162 Val_acc =  0.8824\n",
            "Iteration  986 : Loss =  0.11882498  Acc:  0.96078336 Val_loss =  0.3922035 Val_acc =  0.8839\n",
            "Iteration  987 : Loss =  0.12032134  Acc:  0.9589667 Val_loss =  0.3920692 Val_acc =  0.8825\n",
            "Iteration  988 : Loss =  0.12007215  Acc:  0.95995 Val_loss =  0.3945335 Val_acc =  0.8828\n",
            "Iteration  989 : Loss =  0.11819451  Acc:  0.96026665 Val_loss =  0.39030728 Val_acc =  0.8844\n",
            "Iteration  990 : Loss =  0.11683521  Acc:  0.9615833 Val_loss =  0.3904603 Val_acc =  0.8847\n",
            "Iteration  991 : Loss =  0.11709762  Acc:  0.9616 Val_loss =  0.3911606 Val_acc =  0.8838\n",
            "Iteration  992 : Loss =  0.118299186  Acc:  0.95975 Val_loss =  0.39089698 Val_acc =  0.8832\n",
            "Iteration  993 : Loss =  0.11841224  Acc:  0.96148336 Val_loss =  0.39430234 Val_acc =  0.8832\n",
            "Iteration  994 : Loss =  0.117484964  Acc:  0.9603 Val_loss =  0.39114684 Val_acc =  0.8835\n",
            "Iteration  995 : Loss =  0.11603957  Acc:  0.9625667 Val_loss =  0.39189 Val_acc =  0.8831\n",
            "Iteration  996 : Loss =  0.11565472  Acc:  0.9623 Val_loss =  0.39080197 Val_acc =  0.8849\n",
            "Iteration  997 : Loss =  0.11626826  Acc:  0.9615833 Val_loss =  0.39126188 Val_acc =  0.8838\n",
            "Iteration  998 : Loss =  0.11707326  Acc:  0.9615667 Val_loss =  0.39387846 Val_acc =  0.8834\n",
            "Iteration  999 : Loss =  0.11699737  Acc:  0.96096665 Val_loss =  0.3922188 Val_acc =  0.8835\n",
            "Iteration  1000 : Loss =  0.116393894  Acc:  0.9619167 Val_loss =  0.39351523 Val_acc =  0.8845\n",
            "Iteration  1001 : Loss =  0.11583867  Acc:  0.96195 Val_loss =  0.39225677 Val_acc =  0.8832\n",
            "Iteration  1002 : Loss =  0.11634035  Acc:  0.9613 Val_loss =  0.39250395 Val_acc =  0.8841\n",
            "Iteration  1003 : Loss =  0.117462575  Acc:  0.96143335 Val_loss =  0.39559743 Val_acc =  0.8822\n",
            "Iteration  1004 : Loss =  0.119054995  Acc:  0.95986664 Val_loss =  0.3953375 Val_acc =  0.8822\n",
            "Iteration  1005 : Loss =  0.12000255  Acc:  0.95956665 Val_loss =  0.39890563 Val_acc =  0.8818\n",
            "Iteration  1006 : Loss =  0.1212668  Acc:  0.95853335 Val_loss =  0.39815494 Val_acc =  0.8802\n",
            "Iteration  1007 : Loss =  0.12163602  Acc:  0.95858335 Val_loss =  0.40015382 Val_acc =  0.8809\n",
            "Iteration  1008 : Loss =  0.123185724  Acc:  0.9572167 Val_loss =  0.4015497 Val_acc =  0.8805\n",
            "Iteration  1009 : Loss =  0.1222478  Acc:  0.95891666 Val_loss =  0.40024027 Val_acc =  0.8809\n",
            "Iteration  1010 : Loss =  0.121783875  Acc:  0.95806664 Val_loss =  0.4008089 Val_acc =  0.8815\n",
            "Iteration  1011 : Loss =  0.11791021  Acc:  0.96091664 Val_loss =  0.39603922 Val_acc =  0.8833\n",
            "Iteration  1012 : Loss =  0.11527869  Acc:  0.9619 Val_loss =  0.3942936 Val_acc =  0.8848\n",
            "Iteration  1013 : Loss =  0.11366859  Acc:  0.9630833 Val_loss =  0.39327788 Val_acc =  0.8844\n",
            "Iteration  1014 : Loss =  0.11371805  Acc:  0.96258336 Val_loss =  0.39290014 Val_acc =  0.8841\n",
            "Iteration  1015 : Loss =  0.11485929  Acc:  0.9622167 Val_loss =  0.39526427 Val_acc =  0.8845\n",
            "Iteration  1016 : Loss =  0.11626403  Acc:  0.9618667 Val_loss =  0.3965171 Val_acc =  0.8826\n",
            "Iteration  1017 : Loss =  0.11836958  Acc:  0.9599 Val_loss =  0.3990525 Val_acc =  0.8833\n",
            "Iteration  1018 : Loss =  0.1182757  Acc:  0.96091664 Val_loss =  0.39945748 Val_acc =  0.8811\n",
            "Iteration  1019 : Loss =  0.119101554  Acc:  0.9594833 Val_loss =  0.39940658 Val_acc =  0.8814\n",
            "Iteration  1020 : Loss =  0.11726188  Acc:  0.96101665 Val_loss =  0.39915997 Val_acc =  0.8822\n",
            "Iteration  1021 : Loss =  0.11617354  Acc:  0.9608333 Val_loss =  0.39671642 Val_acc =  0.8823\n",
            "Iteration  1022 : Loss =  0.114776775  Acc:  0.96238333 Val_loss =  0.3966609 Val_acc =  0.8824\n",
            "Iteration  1023 : Loss =  0.113772444  Acc:  0.96241665 Val_loss =  0.39501986 Val_acc =  0.8835\n",
            "Iteration  1024 : Loss =  0.1128674  Acc:  0.9634 Val_loss =  0.39471015 Val_acc =  0.8837\n",
            "Iteration  1025 : Loss =  0.11232213  Acc:  0.9636833 Val_loss =  0.39456868 Val_acc =  0.8836\n",
            "Iteration  1026 : Loss =  0.112335265  Acc:  0.96351665 Val_loss =  0.3947945 Val_acc =  0.8836\n",
            "Iteration  1027 : Loss =  0.11286319  Acc:  0.96351665 Val_loss =  0.39588827 Val_acc =  0.8836\n",
            "Iteration  1028 : Loss =  0.11376798  Acc:  0.9622167 Val_loss =  0.3969441 Val_acc =  0.8835\n",
            "Iteration  1029 : Loss =  0.114458114  Acc:  0.9623167 Val_loss =  0.39822525 Val_acc =  0.8822\n",
            "Iteration  1030 : Loss =  0.11477308  Acc:  0.96133333 Val_loss =  0.3982078 Val_acc =  0.8823\n",
            "Iteration  1031 : Loss =  0.11439372  Acc:  0.9623167 Val_loss =  0.3984746 Val_acc =  0.8829\n",
            "Iteration  1032 : Loss =  0.1137953  Acc:  0.96205 Val_loss =  0.39749694 Val_acc =  0.8831\n",
            "Iteration  1033 : Loss =  0.11289577  Acc:  0.9634167 Val_loss =  0.3976287 Val_acc =  0.8827\n",
            "Iteration  1034 : Loss =  0.11226371  Acc:  0.96328336 Val_loss =  0.39608854 Val_acc =  0.8838\n",
            "Iteration  1035 : Loss =  0.111546926  Acc:  0.9642 Val_loss =  0.39641482 Val_acc =  0.8829\n",
            "Iteration  1036 : Loss =  0.11111413  Acc:  0.9640833 Val_loss =  0.39603356 Val_acc =  0.8845\n",
            "Iteration  1037 : Loss =  0.11081974  Acc:  0.96423334 Val_loss =  0.39589646 Val_acc =  0.8832\n",
            "Iteration  1038 : Loss =  0.11074267  Acc:  0.96455 Val_loss =  0.39680234 Val_acc =  0.883\n",
            "Iteration  1039 : Loss =  0.11084866  Acc:  0.96383333 Val_loss =  0.3961293 Val_acc =  0.8844\n",
            "Iteration  1040 : Loss =  0.11095892  Acc:  0.9647167 Val_loss =  0.39776373 Val_acc =  0.8834\n",
            "Iteration  1041 : Loss =  0.111020155  Acc:  0.96381664 Val_loss =  0.3967094 Val_acc =  0.8836\n",
            "Iteration  1042 : Loss =  0.11100083  Acc:  0.96485 Val_loss =  0.39799845 Val_acc =  0.8831\n",
            "Iteration  1043 : Loss =  0.11113523  Acc:  0.9639 Val_loss =  0.39768168 Val_acc =  0.8833\n",
            "Iteration  1044 : Loss =  0.11137084  Acc:  0.9640333 Val_loss =  0.39858553 Val_acc =  0.8825\n",
            "Iteration  1045 : Loss =  0.11194202  Acc:  0.96315 Val_loss =  0.3994663 Val_acc =  0.8834\n",
            "Iteration  1046 : Loss =  0.11241928  Acc:  0.9632 Val_loss =  0.39974368 Val_acc =  0.8824\n",
            "Iteration  1047 : Loss =  0.11324905  Acc:  0.96211666 Val_loss =  0.4015499 Val_acc =  0.883\n",
            "Iteration  1048 : Loss =  0.11347376  Acc:  0.96283334 Val_loss =  0.40129688 Val_acc =  0.8824\n",
            "Iteration  1049 : Loss =  0.11430949  Acc:  0.96148336 Val_loss =  0.4026784 Val_acc =  0.8828\n",
            "Iteration  1050 : Loss =  0.113868974  Acc:  0.96265 Val_loss =  0.40250152 Val_acc =  0.8823\n",
            "Iteration  1051 : Loss =  0.114347346  Acc:  0.9615833 Val_loss =  0.40301132 Val_acc =  0.8827\n",
            "Iteration  1052 : Loss =  0.11305834  Acc:  0.9632667 Val_loss =  0.40241042 Val_acc =  0.8822\n",
            "Iteration  1053 : Loss =  0.112398684  Acc:  0.96265 Val_loss =  0.40112364 Val_acc =  0.8825\n",
            "Iteration  1054 : Loss =  0.11089826  Acc:  0.96426666 Val_loss =  0.4008666 Val_acc =  0.8827\n",
            "Iteration  1055 : Loss =  0.109958366  Acc:  0.96425 Val_loss =  0.3993207 Val_acc =  0.8831\n",
            "Iteration  1056 : Loss =  0.10941014  Acc:  0.965 Val_loss =  0.3994379 Val_acc =  0.8832\n",
            "Iteration  1057 : Loss =  0.10926772  Acc:  0.96463335 Val_loss =  0.39941898 Val_acc =  0.8842\n",
            "Iteration  1058 : Loss =  0.109252565  Acc:  0.96455 Val_loss =  0.3999531 Val_acc =  0.8837\n",
            "Iteration  1059 : Loss =  0.109317675  Acc:  0.9647667 Val_loss =  0.39983454 Val_acc =  0.8836\n",
            "Iteration  1060 : Loss =  0.109349385  Acc:  0.9649 Val_loss =  0.40075204 Val_acc =  0.8832\n",
            "Iteration  1061 : Loss =  0.10952235  Acc:  0.96435 Val_loss =  0.4003502 Val_acc =  0.8836\n",
            "Iteration  1062 : Loss =  0.10987733  Acc:  0.96458334 Val_loss =  0.4023426 Val_acc =  0.8834\n",
            "Iteration  1063 : Loss =  0.11025655  Acc:  0.96383333 Val_loss =  0.40093338 Val_acc =  0.8834\n",
            "Iteration  1064 : Loss =  0.11069938  Acc:  0.96393335 Val_loss =  0.40357256 Val_acc =  0.8836\n",
            "Iteration  1065 : Loss =  0.11073901  Acc:  0.9634 Val_loss =  0.40201122 Val_acc =  0.883\n",
            "Iteration  1066 : Loss =  0.11096229  Acc:  0.96345 Val_loss =  0.40431273 Val_acc =  0.8839\n",
            "Iteration  1067 : Loss =  0.1107008  Acc:  0.9637 Val_loss =  0.40287516 Val_acc =  0.883\n",
            "Iteration  1068 : Loss =  0.111203514  Acc:  0.96328336 Val_loss =  0.40437898 Val_acc =  0.8837\n",
            "Iteration  1069 : Loss =  0.11091241  Acc:  0.9637833 Val_loss =  0.40444744 Val_acc =  0.8826\n",
            "Iteration  1070 : Loss =  0.1116757  Acc:  0.9626667 Val_loss =  0.40493053 Val_acc =  0.8822\n",
            "Iteration  1071 : Loss =  0.11154103  Acc:  0.9633667 Val_loss =  0.40614158 Val_acc =  0.882\n",
            "Iteration  1072 : Loss =  0.11244852  Acc:  0.96185 Val_loss =  0.40577194 Val_acc =  0.8813\n",
            "Iteration  1073 : Loss =  0.113167435  Acc:  0.9627333 Val_loss =  0.40854785 Val_acc =  0.882\n",
            "Iteration  1074 : Loss =  0.114845105  Acc:  0.96063334 Val_loss =  0.4083639 Val_acc =  0.8807\n",
            "Iteration  1075 : Loss =  0.11593908  Acc:  0.96141666 Val_loss =  0.41186577 Val_acc =  0.8805\n",
            "Iteration  1076 : Loss =  0.11705823  Acc:  0.95963335 Val_loss =  0.4111156 Val_acc =  0.8806\n",
            "Iteration  1077 : Loss =  0.114622466  Acc:  0.96241665 Val_loss =  0.4110086 Val_acc =  0.8812\n",
            "Iteration  1078 : Loss =  0.111425534  Acc:  0.9626833 Val_loss =  0.40534022 Val_acc =  0.8827\n",
            "Iteration  1079 : Loss =  0.10777074  Acc:  0.9654667 Val_loss =  0.40345016 Val_acc =  0.8839\n",
            "Iteration  1080 : Loss =  0.10719742  Acc:  0.96525 Val_loss =  0.402462 Val_acc =  0.8826\n",
            "Iteration  1081 : Loss =  0.10956814  Acc:  0.96351665 Val_loss =  0.4061472 Val_acc =  0.8828\n",
            "Iteration  1082 : Loss =  0.1125205  Acc:  0.9623333 Val_loss =  0.40984824 Val_acc =  0.8815\n",
            "Iteration  1083 : Loss =  0.11484082  Acc:  0.9604 Val_loss =  0.41150254 Val_acc =  0.8798\n",
            "Iteration  1084 : Loss =  0.114912935  Acc:  0.9612167 Val_loss =  0.41298094 Val_acc =  0.8814\n",
            "Iteration  1085 : Loss =  0.11365345  Acc:  0.96113336 Val_loss =  0.41029406 Val_acc =  0.8802\n",
            "Iteration  1086 : Loss =  0.11047802  Acc:  0.9637333 Val_loss =  0.40883535 Val_acc =  0.8815\n",
            "Iteration  1087 : Loss =  0.10820605  Acc:  0.9644833 Val_loss =  0.40505674 Val_acc =  0.8828\n",
            "Iteration  1088 : Loss =  0.10708513  Acc:  0.9654833 Val_loss =  0.4041895 Val_acc =  0.8827\n",
            "Iteration  1089 : Loss =  0.1079138  Acc:  0.96486664 Val_loss =  0.4058361 Val_acc =  0.8834\n",
            "Iteration  1090 : Loss =  0.1099736  Acc:  0.9630167 Val_loss =  0.40694517 Val_acc =  0.8827\n",
            "Iteration  1091 : Loss =  0.11112878  Acc:  0.9638 Val_loss =  0.41153204 Val_acc =  0.8825\n",
            "Iteration  1092 : Loss =  0.112099536  Acc:  0.9619833 Val_loss =  0.40974703 Val_acc =  0.8827\n",
            "Iteration  1093 : Loss =  0.1101324  Acc:  0.9644 Val_loss =  0.41119254 Val_acc =  0.8825\n",
            "Iteration  1094 : Loss =  0.10767613  Acc:  0.9640333 Val_loss =  0.40534887 Val_acc =  0.8837\n",
            "Iteration  1095 : Loss =  0.105488755  Acc:  0.9668 Val_loss =  0.4054571 Val_acc =  0.8831\n",
            "Iteration  1096 : Loss =  0.10542948  Acc:  0.9664 Val_loss =  0.4049879 Val_acc =  0.8831\n",
            "Iteration  1097 : Loss =  0.106839366  Acc:  0.96458334 Val_loss =  0.40641642 Val_acc =  0.8834\n",
            "Iteration  1098 : Loss =  0.10813021  Acc:  0.9651667 Val_loss =  0.40970048 Val_acc =  0.8828\n",
            "Iteration  1099 : Loss =  0.10851369  Acc:  0.96385 Val_loss =  0.40824625 Val_acc =  0.882\n",
            "Iteration  1100 : Loss =  0.10733821  Acc:  0.9655667 Val_loss =  0.40962246 Val_acc =  0.8819\n",
            "Iteration  1101 : Loss =  0.105685316  Acc:  0.96535 Val_loss =  0.40598583 Val_acc =  0.8834\n",
            "Iteration  1102 : Loss =  0.10429386  Acc:  0.96723336 Val_loss =  0.40597662 Val_acc =  0.8828\n",
            "Iteration  1103 : Loss =  0.10398203  Acc:  0.96708333 Val_loss =  0.4055029 Val_acc =  0.883\n",
            "Iteration  1104 : Loss =  0.10463411  Acc:  0.96625 Val_loss =  0.4058545 Val_acc =  0.8834\n",
            "Iteration  1105 : Loss =  0.105535045  Acc:  0.96636665 Val_loss =  0.40886787 Val_acc =  0.8826\n",
            "Iteration  1106 : Loss =  0.106203  Acc:  0.9647667 Val_loss =  0.40773192 Val_acc =  0.8824\n",
            "Iteration  1107 : Loss =  0.10598842  Acc:  0.96606666 Val_loss =  0.40994444 Val_acc =  0.8828\n",
            "Iteration  1108 : Loss =  0.10532827  Acc:  0.96501666 Val_loss =  0.40734962 Val_acc =  0.8828\n",
            "Iteration  1109 : Loss =  0.10426836  Acc:  0.96701664 Val_loss =  0.40811825 Val_acc =  0.8838\n",
            "Iteration  1110 : Loss =  0.1034922  Acc:  0.9669167 Val_loss =  0.40625435 Val_acc =  0.8828\n",
            "Iteration  1111 : Loss =  0.1033152  Acc:  0.96758336 Val_loss =  0.4067586 Val_acc =  0.8834\n",
            "Iteration  1112 : Loss =  0.10376187  Acc:  0.9677167 Val_loss =  0.40779284 Val_acc =  0.8825\n",
            "Iteration  1113 : Loss =  0.104632124  Acc:  0.9659167 Val_loss =  0.4085062 Val_acc =  0.883\n",
            "Iteration  1114 : Loss =  0.105513826  Acc:  0.9661833 Val_loss =  0.41033784 Val_acc =  0.8819\n",
            "Iteration  1115 : Loss =  0.10686986  Acc:  0.96445 Val_loss =  0.41176388 Val_acc =  0.8828\n",
            "Iteration  1116 : Loss =  0.10817038  Acc:  0.9644333 Val_loss =  0.4131707 Val_acc =  0.8812\n",
            "Iteration  1117 : Loss =  0.11138408  Acc:  0.9619167 Val_loss =  0.41763657 Val_acc =  0.8829\n",
            "Iteration  1118 : Loss =  0.11353112  Acc:  0.9616 Val_loss =  0.41796532 Val_acc =  0.879\n",
            "Iteration  1119 : Loss =  0.12016911  Acc:  0.9576 Val_loss =  0.42816085 Val_acc =  0.8812\n",
            "Iteration  1120 : Loss =  0.12121238  Acc:  0.95683336 Val_loss =  0.42512667 Val_acc =  0.8779\n",
            "Iteration  1121 : Loss =  0.12596078  Acc:  0.95456666 Val_loss =  0.43427998 Val_acc =  0.8819\n",
            "Iteration  1122 : Loss =  0.12119466  Acc:  0.9571 Val_loss =  0.42505133 Val_acc =  0.8794\n",
            "Iteration  1123 : Loss =  0.11304663  Acc:  0.9612667 Val_loss =  0.41930717 Val_acc =  0.8846\n",
            "Iteration  1124 : Loss =  0.1060168  Acc:  0.9649 Val_loss =  0.41094732 Val_acc =  0.8831\n",
            "Iteration  1125 : Loss =  0.10539025  Acc:  0.9657 Val_loss =  0.41061768 Val_acc =  0.8827\n",
            "Iteration  1126 : Loss =  0.11276627  Acc:  0.9611833 Val_loss =  0.420085 Val_acc =  0.8826\n",
            "Iteration  1127 : Loss =  0.11539713  Acc:  0.9601167 Val_loss =  0.4213965 Val_acc =  0.8789\n",
            "Iteration  1128 : Loss =  0.11377374  Acc:  0.96038336 Val_loss =  0.422525 Val_acc =  0.8839\n",
            "Iteration  1129 : Loss =  0.10616439  Acc:  0.96463335 Val_loss =  0.41241837 Val_acc =  0.882\n",
            "Iteration  1130 : Loss =  0.102142476  Acc:  0.9678 Val_loss =  0.4107181 Val_acc =  0.8827\n",
            "Iteration  1131 : Loss =  0.1041236  Acc:  0.96653336 Val_loss =  0.41227442 Val_acc =  0.8837\n",
            "Iteration  1132 : Loss =  0.10770534  Acc:  0.964 Val_loss =  0.41549507 Val_acc =  0.8813\n",
            "Iteration  1133 : Loss =  0.10986536  Acc:  0.9623167 Val_loss =  0.42002138 Val_acc =  0.8847\n",
            "Iteration  1134 : Loss =  0.10703851  Acc:  0.9643833 Val_loss =  0.4145579 Val_acc =  0.881\n",
            "Iteration  1135 : Loss =  0.103952006  Acc:  0.9661833 Val_loss =  0.41419637 Val_acc =  0.8837\n",
            "Iteration  1136 : Loss =  0.10218654  Acc:  0.96711665 Val_loss =  0.41029406 Val_acc =  0.8842\n",
            "Iteration  1137 : Loss =  0.10233746  Acc:  0.96785 Val_loss =  0.41173056 Val_acc =  0.8834\n",
            "Iteration  1138 : Loss =  0.10352865  Acc:  0.96676666 Val_loss =  0.41352984 Val_acc =  0.8827\n",
            "Iteration  1139 : Loss =  0.10421029  Acc:  0.96576667 Val_loss =  0.41331434 Val_acc =  0.8826\n",
            "Iteration  1140 : Loss =  0.10409685  Acc:  0.96603334 Val_loss =  0.41611192 Val_acc =  0.8838\n",
            "Iteration  1141 : Loss =  0.10291815  Acc:  0.96638334 Val_loss =  0.4124419 Val_acc =  0.8828\n",
            "Iteration  1142 : Loss =  0.1016669  Acc:  0.9678 Val_loss =  0.41325414 Val_acc =  0.8834\n",
            "Iteration  1143 : Loss =  0.10095317  Acc:  0.96813333 Val_loss =  0.41153285 Val_acc =  0.8834\n",
            "Iteration  1144 : Loss =  0.10096937  Acc:  0.96786666 Val_loss =  0.4116545 Val_acc =  0.8836\n",
            "Iteration  1145 : Loss =  0.10144461  Acc:  0.96783334 Val_loss =  0.4138878 Val_acc =  0.8829\n",
            "Iteration  1146 : Loss =  0.10191634  Acc:  0.96678334 Val_loss =  0.41218427 Val_acc =  0.8831\n",
            "Iteration  1147 : Loss =  0.10164022  Acc:  0.96756667 Val_loss =  0.41481417 Val_acc =  0.8836\n",
            "Iteration  1148 : Loss =  0.100817725  Acc:  0.96776664 Val_loss =  0.41210905 Val_acc =  0.8831\n",
            "Iteration  1149 : Loss =  0.09995942  Acc:  0.96865 Val_loss =  0.4131791 Val_acc =  0.8834\n",
            "Iteration  1150 : Loss =  0.0996111  Acc:  0.9687667 Val_loss =  0.41273198 Val_acc =  0.8819\n",
            "Iteration  1151 : Loss =  0.09993208  Acc:  0.96855 Val_loss =  0.41225567 Val_acc =  0.8829\n",
            "Iteration  1152 : Loss =  0.10042718  Acc:  0.96826667 Val_loss =  0.41487753 Val_acc =  0.8825\n",
            "Iteration  1153 : Loss =  0.10062261  Acc:  0.96751666 Val_loss =  0.4131127 Val_acc =  0.8836\n",
            "Iteration  1154 : Loss =  0.10028932  Acc:  0.9684167 Val_loss =  0.4154185 Val_acc =  0.8823\n",
            "Iteration  1155 : Loss =  0.09970874  Acc:  0.96856666 Val_loss =  0.41303018 Val_acc =  0.8829\n",
            "Iteration  1156 : Loss =  0.09917217  Acc:  0.969 Val_loss =  0.41403946 Val_acc =  0.8825\n",
            "Iteration  1157 : Loss =  0.09898112  Acc:  0.9690167 Val_loss =  0.41345185 Val_acc =  0.8834\n",
            "Iteration  1158 : Loss =  0.099042304  Acc:  0.9690667 Val_loss =  0.41353 Val_acc =  0.8831\n",
            "Iteration  1159 : Loss =  0.09917531  Acc:  0.9690667 Val_loss =  0.41493312 Val_acc =  0.8825\n",
            "Iteration  1160 : Loss =  0.099211976  Acc:  0.96865 Val_loss =  0.4139002 Val_acc =  0.8827\n",
            "Iteration  1161 : Loss =  0.09914243  Acc:  0.9687833 Val_loss =  0.41524222 Val_acc =  0.8832\n",
            "Iteration  1162 : Loss =  0.09899608  Acc:  0.96923333 Val_loss =  0.4144466 Val_acc =  0.8829\n",
            "Iteration  1163 : Loss =  0.098875016  Acc:  0.9690833 Val_loss =  0.4153551 Val_acc =  0.8832\n",
            "Iteration  1164 : Loss =  0.09870612  Acc:  0.96968335 Val_loss =  0.4150585 Val_acc =  0.883\n",
            "Iteration  1165 : Loss =  0.09862032  Acc:  0.96903336 Val_loss =  0.41498607 Val_acc =  0.884\n",
            "Iteration  1166 : Loss =  0.09848389  Acc:  0.96973336 Val_loss =  0.41576344 Val_acc =  0.8818\n",
            "Iteration  1167 : Loss =  0.09844766  Acc:  0.96868336 Val_loss =  0.41511616 Val_acc =  0.8824\n",
            "Iteration  1168 : Loss =  0.09856896  Acc:  0.96936667 Val_loss =  0.4165407 Val_acc =  0.882\n",
            "Iteration  1169 : Loss =  0.09898847  Acc:  0.9682 Val_loss =  0.4160866 Val_acc =  0.8831\n",
            "Iteration  1170 : Loss =  0.099748366  Acc:  0.9685 Val_loss =  0.41853243 Val_acc =  0.8826\n",
            "Iteration  1171 : Loss =  0.10103888  Acc:  0.96708333 Val_loss =  0.4184099 Val_acc =  0.882\n",
            "Iteration  1172 : Loss =  0.102592856  Acc:  0.9669333 Val_loss =  0.42208472 Val_acc =  0.8817\n",
            "Iteration  1173 : Loss =  0.10496708  Acc:  0.96456665 Val_loss =  0.4229171 Val_acc =  0.8799\n",
            "Iteration  1174 : Loss =  0.10714502  Acc:  0.96433336 Val_loss =  0.42801005 Val_acc =  0.8799\n",
            "Iteration  1175 : Loss =  0.11008062  Acc:  0.9614 Val_loss =  0.428595 Val_acc =  0.8791\n",
            "Iteration  1176 : Loss =  0.11100532  Acc:  0.96211666 Val_loss =  0.43226346 Val_acc =  0.8793\n",
            "Iteration  1177 : Loss =  0.112204894  Acc:  0.96021664 Val_loss =  0.43161058 Val_acc =  0.8785\n",
            "Iteration  1178 : Loss =  0.10753255  Acc:  0.9637167 Val_loss =  0.428273 Val_acc =  0.8796\n",
            "Iteration  1179 : Loss =  0.104768194  Acc:  0.9648333 Val_loss =  0.4251376 Val_acc =  0.8813\n",
            "Iteration  1180 : Loss =  0.101102434  Acc:  0.96708333 Val_loss =  0.4194999 Val_acc =  0.8827\n",
            "Iteration  1181 : Loss =  0.10122829  Acc:  0.9675 Val_loss =  0.4234171 Val_acc =  0.8832\n",
            "Iteration  1182 : Loss =  0.10414977  Acc:  0.96468335 Val_loss =  0.42302978 Val_acc =  0.8811\n",
            "Iteration  1183 : Loss =  0.1062172  Acc:  0.9651 Val_loss =  0.43054664 Val_acc =  0.88\n",
            "Iteration  1184 : Loss =  0.10649921  Acc:  0.96361667 Val_loss =  0.42701602 Val_acc =  0.8797\n",
            "Iteration  1185 : Loss =  0.1032787  Acc:  0.9661833 Val_loss =  0.42571303 Val_acc =  0.8807\n",
            "Iteration  1186 : Loss =  0.10031797  Acc:  0.9672 Val_loss =  0.42183223 Val_acc =  0.8837\n",
            "Iteration  1187 : Loss =  0.09928439  Acc:  0.9676667 Val_loss =  0.41959435 Val_acc =  0.8816\n",
            "Iteration  1188 : Loss =  0.10070502  Acc:  0.96705 Val_loss =  0.4253541 Val_acc =  0.8833\n",
            "Iteration  1189 : Loss =  0.10240682  Acc:  0.96543336 Val_loss =  0.42275387 Val_acc =  0.8817\n",
            "Iteration  1190 : Loss =  0.10148049  Acc:  0.96718335 Val_loss =  0.42601874 Val_acc =  0.882\n",
            "Iteration  1191 : Loss =  0.099987105  Acc:  0.96711665 Val_loss =  0.42096138 Val_acc =  0.882\n",
            "Iteration  1192 : Loss =  0.097781084  Acc:  0.9690167 Val_loss =  0.4204875 Val_acc =  0.8833\n",
            "Iteration  1193 : Loss =  0.09681912  Acc:  0.96961665 Val_loss =  0.4204608 Val_acc =  0.8833\n",
            "Iteration  1194 : Loss =  0.097258806  Acc:  0.96905 Val_loss =  0.41936034 Val_acc =  0.8829\n",
            "Iteration  1195 : Loss =  0.0981322  Acc:  0.9687667 Val_loss =  0.4233257 Val_acc =  0.8832\n",
            "Iteration  1196 : Loss =  0.09890423  Acc:  0.96788335 Val_loss =  0.42089823 Val_acc =  0.8833\n",
            "Iteration  1197 : Loss =  0.098456204  Acc:  0.96865 Val_loss =  0.42365858 Val_acc =  0.8836\n",
            "Iteration  1198 : Loss =  0.09784223  Acc:  0.9690167 Val_loss =  0.4216841 Val_acc =  0.8833\n",
            "Iteration  1199 : Loss =  0.09670862  Acc:  0.96933335 Val_loss =  0.42169482 Val_acc =  0.8827\n",
            "\n",
            "The final test accuracy of the Fashion MNIST dataset is 0.883400022983551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5vDaxkAUmuW",
        "colab_type": "text"
      },
      "source": [
        "### Plot of Training/Validation Accuracy and Training/Validation Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxR4-UrqUs6X",
        "colab_type": "code",
        "outputId": "f1fb295d-969a-4426-b60b-fa02ca87f315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(test_loss, label=\"Val Loss\", color='blue')\n",
        "plt.plot(test_acc, label=\"Val Acc\", color='orange')\n",
        "plt.plot(training_loss, label=\"Train Loss\", color='green')\n",
        "plt.plot(training_acc, label=\"Train Acc\", color='red')\n",
        "plt.title(\"Number of Iterations vs Training/Validation Loss and Accuracy\")\n",
        "plt.xlabel(\"Number of Iterations\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# Print the test accuracy \n",
        "print()\n",
        "print(\"The final test accuracy of the Fashion MNIST dataset is {}\".format(final_test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The final test accuracy of the Fashion MNIST dataset is 0.883400022983551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7gkozKp1kqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}